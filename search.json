[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI総合演習",
    "section": "",
    "text": "ようこそ\nこの資料は徳島大学 教養教育院・教養科目群・自然と技術の「AI総合演習」（INTT1520JLAS01）についての補助教材です。講義で扱った内容について、Quartoによるブック形式で提供しています。内容は、講義名のとおり、AI（人工知能）に含まれる機械学習、深層学習について、Pythonプログラミングによる演習形式で行います。"
  },
  {
    "objectID": "index.html#演習プログラムの実行",
    "href": "index.html#演習プログラムの実行",
    "title": "AI総合演習",
    "section": "演習プログラムの実行",
    "text": "演習プログラムの実行\n自身のコンピュータ上に用意した、ローカルJupyter環境あるいはリモートJupyter環境での利用を想定します。\nリモート環境としてGoogle Colaboratory (Colab)が利用できます。プログラムの実行にはGoogleのアカウント登録が必要です。Colabについては付録をご覧ください。\n\n\n\nOpen In Colab\n\n\n上記のボタンをクリックするとColabが開きます。"
  },
  {
    "objectID": "index.html#講義計画",
    "href": "index.html#講義計画",
    "title": "AI総合演習",
    "section": "講義計画",
    "text": "講義計画\n講義の計画は以下の通りです。2023年10月2日現在、後期の講義が進行中であり、内容は順次追加されていきます。\n\nガイダンス\nプログラミング入門\n機械学習の背景・数理\n機械学習モデルの設計と評価\n機械学習の手法\n機械学習モデルの解釈・説明性\n演習1\n深層学習の基礎\n実社会での応用\n深層生成モデル\n演習2\n課題解決型演習1\n課題解決型演習2\n課題解決型演習3\n課題解決型演習の発表と振り返り\n\nこのページの元となるファイルはGitHubリポジトリでも公開しています。そこでは講義中に投影したスライド資料（一部、公開のために調整したものもあり）も含まれます。内容の誤りや誤字脱字、不明点や改善のためのコメント等はGitHub issuesあるいはpull requestを通して行うことが可能です。資料の品質向上のために協力いただけますと幸いです。"
  },
  {
    "objectID": "index.html#ライセンス",
    "href": "index.html#ライセンス",
    "title": "AI総合演習",
    "section": "ライセンス",
    "text": "ライセンス\n資料のうち、プログラム部分以外（文章や画像）はクリエイティブ・コモンズ 表示 - 非営利 - 改変禁止 4.0 国際 (CC BY-NC-ND 4.0)、プログラム部分はMITライセンスに従い、利用できます。"
  },
  {
    "objectID": "week01/guidance.html#参考資料url",
    "href": "week01/guidance.html#参考資料url",
    "title": "ガイダンス",
    "section": "参考資料・URL",
    "text": "参考資料・URL\n\n(小林一郎 2008)\n(谷口忠大 2020)\n(赤穂昭太郎 ほか 2023)\n(岡野原大輔 2022b)\n(岡野原大輔 2022a)\n(岡谷貴之 2022)\n(AI白書編集委員会 2022)\n\n\n\n\n\nAI白書編集委員会. 2022. AI白書2022. KADOKAWA.\n\n\n小林一郎. 2008. 人工知能の基礎. Computer science library ; 13. サイエンス社. http://id.ndl.go.jp/bib/000009788211.\n\n\n岡谷貴之. 2022. 深層学習 = Deep Learning. 改訂第2版 版. 機械学習プロフェッショナルシリーズ. 講談社. http://id.ndl.go.jp/bib/031901202.\n\n\n岡野原大輔. 2022a. ディープラーニングを支える技術. Tech×Books plus 2. 技術評論社. http://id.ndl.go.jp/bib/032086743.\n\n\n———. 2022b. ディープラーニングを支える技術 : 「正解」を導くメカニズム〈技術基礎〉. Tech×Books plus. 技術評論社. http://id.ndl.go.jp/bib/031881422.\n\n\n谷口忠大. 2020. イラストで学ぶ人工知能概論 = An Illustrated Guide to Artificial Intelligence. 改訂第2版 版. 講談社. http://id.ndl.go.jp/bib/030810109.\n\n\n赤穂昭太郎, 今泉允聡, 内田誠一, 清智也, 高野渉, 辻真吾, 原尚幸, ほか. 2023. 応用基礎としてのデータサイエンス : AI×データ活用の実践. 編集者： 北川源四郎 と 竹村彰通. データサイエンス入門シリーズ. 講談社. http://id.ndl.go.jp/bib/032637501."
  },
  {
    "objectID": "week02/index.html#この章の参考資料url",
    "href": "week02/index.html#この章の参考資料url",
    "title": "プログラミング入門",
    "section": "この章の参考資料・URL",
    "text": "この章の参考資料・URL\n\n(池内孝啓 と 片柳薫子 2020)\n(有賀友紀 と 大橋俊介 2021)\n(Grus と 菊池彰 2020)\n(Sweigart と 相川愛三 2023)\n(八谷大岳 2020)\nhttps://docs.python.org/3/\nhttps://www.python.jp/train/index.html\n\n\n\n\n\nGrus, Joel, と 菊池彰. 2020. ゼロからはじめるデータサイエンス : Pythonで学ぶ基本と実践. 翻訳者： 菊池彰. 第2版 版. オライリー・ジャパン. http://id.ndl.go.jp/bib/030372878.\n\n\nSweigart, Al, と 相川愛三. 2023. 退屈なことはPythonにやらせよう : ノンプログラマーにもできる自動化処理プログラミング. 翻訳者： 相川愛三. 第2版 版. オライリー・ジャパン.\n\n\n八谷大岳. 2020. ゼロからつくるPython機械学習プログラミング入門 = Introduction to Machine Learning from Scratch with Python. 機械学習スタートアップシリーズ. 講談社. http://id.ndl.go.jp/bib/030584765.\n\n\n有賀友紀, と 大橋俊介. 2021. RとPythonで学ぶ実践的データサイエンス&機械学習. 増補改訂版. 技術評論社. http://id.ndl.go.jp/bib/031401828.\n\n\n池内孝啓, と 片柳薫子. 2020. PythonユーザのためのJupyter〈実践〉入門. 改訂版. 技術評論社. http://id.ndl.go.jp/bib/030570561."
  },
  {
    "objectID": "week02/0201_pandas.html",
    "href": "week02/0201_pandas.html",
    "title": "1  pandas基礎",
    "section": "",
    "text": "pandasは、Pythonでさまざまなデータ分析を効率的に行うことができるライブラリです。ここではpandasの基本的な使い方を学びます。\nまずはpandasをインポートします。もしpandasがインストールされていない場合、pip install pandasでインストールしてください。\n\n# pandasの読み込み ... pdとして参照できるようにします\nimport pandas as pd\n\n\n# seies\nsr = pd.Series([1, 2, 3, 4, 5])\nsr\n\n0    1\n1    2\n2    3\n3    4\n4    5\ndtype: int64\n\n\n\nimport numpy as np\n\ndata = np.array(\n    [\n        [63.5, 100, 64, 110, 85],\n        [6, 3.5, 5.4, 6.5, 60],\n        [\"レッサーパンダ\", \"ホオジロカンムリヅル\", \"コツメカワウソ\", \"カナダガン\", \"チンパンジー\"],\n    ]\n).T\n\n\n# data frame (df)\ndf = pd.DataFrame(data, columns=[\"body_length_cm\", \"weight_kg\", \"name\"])\n\ndf\n\n\n\n\n\n\n\n\nbody_length_cm\nweight_kg\nname\n\n\n\n\n0\n63.5\n6\nレッサーパンダ\n\n\n1\n100\n3.5\nホオジロカンムリヅル\n\n\n2\n64\n5.4\nコツメカワウソ\n\n\n3\n110\n6.5\nカナダガン\n\n\n4\n85\n60\nチンパンジー"
  },
  {
    "objectID": "week02/0202_matplotlib.html",
    "href": "week02/0202_matplotlib.html",
    "title": "2  データの可視化",
    "section": "",
    "text": "```{python}\n# japanize_matplotlibとpolarsのインストール\n!pip install -qq japanize_matplotlib \n!pip install -qq polars pyarrow\n```\n\n\n# 利用するライブラリを読み込む\n# as で別名をつけることができる（長いライブラリ名の省略）\nimport japanize_matplotlib  # matplotlibの日本語化\nimport matplotlib.pyplot as plt  # データ可視化\nimport numpy as np  # 数値計算用\nimport pandas as pd  # 表形式データの操作\nimport polars as pl  # pandas と同じく表形式データの操作\n\nデータフレームの操作にはpolarsとpandasの2つのライブラリを使用すると便利です。\n\n# データ（csvファイル）の読み込み\n# ファイルがある場所（パス）を指定する\n# ウェブ上のデータを読み込む場合はURLを指定する\ndf = pl.read_csv(\n    \"https://raw.githubusercontent.com/uribo/cue2022aw_r104/main/data-raw/shikoku_kome_sisyutu2019to2021/20221121T014749Z-f75ff/shikoku_kome_sisyutu2019to2021.csv\"\n)\n\n\n# 読み込んだデータの確認\n# shape: データの行数と列数を確認する\ndf.shape\n\n(864, 6)\n\n\n\n# 型の確認\ndf.dtypes\n\n[Int64, Utf8, Int64, Utf8, Utf8, Int64]\n\n\n\n2.0.1 データの可視化\nコードセルでdfと入力して実行すると、データフレームの中身が表示されます。 一方、行数の多いデータを出力すると画面が埋まってしまうので、一部だけを確認したいこともあります。 その場合、headやtailを使います。これにより、データフレームの先頭行や末尾行を表示することができます。括弧内の値は表示する行数です。\n\n# 先頭3行を表示する\ndf.head(3)\n\n\nshape: (3, 6)\n\n\n\nym\n品目分類\n市区町村コード\n市\n項目\nvalue\n\n\ni64\nstr\ni64\nstr\nstr\ni64\n\n\n\n\n201901\n\"アイスクリーム・シャーベット…\n36201\n\"徳島市\"\n\"支出金額_複数単位\"\n367\n\n\n201901\n\"アイスクリーム・シャーベット…\n36201\n\"徳島市\"\n\"購入頻度_100世帯当たり\"\n123\n\n\n201901\n\"殺虫・防虫剤\"\n36201\n\"徳島市\"\n\"支出金額_複数単位\"\n7\n\n\n\n\n\n\n\n# 末尾2行を表示する\ndf.tail(2)\n\n\nshape: (2, 6)\n\n\n\nym\n品目分類\n市区町村コード\n市\n項目\nvalue\n\n\ni64\nstr\ni64\nstr\nstr\ni64\n\n\n\n\n202112\n\"米\"\n39201\n\"高知市\"\n\"支出金額_複数単位\"\n1557\n\n\n202112\n\"米\"\n39201\n\"高知市\"\n\"購入頻度_100世帯当たり\"\n73\n\n\n\n\n\n\n続いて、簡単なグラフを作成してみましょう。matplotlibにはPythonでグラフを作成する際に便利な関数をまとめたpyplotというモジュールがあります。pyplotをpltという名前でインポートしておきます。\n\n# import matplotlib.pyplot as plt # 最初のコードセルで実行済み\n\npltには、plotやscatterなどの関数があります。plotは折れ線グラフを、scatterは散布図を作成します。今回は棒グラフを作成したいのでbarが対応します。\nこれらの関数の引数には、グラフに描画するx軸の値とy軸の値（変数）を指定します。\nmatplotlibがサポートするグラフの種類や詳しい関数の利用方法は公式ドキュメントを参照してください。\n\n# 棒グラフを作成\n# x軸には品目分類、y軸にはvalueを指定する\nplt.bar(df[\"品目分類\"], df[\"value\"])\n# plt.show() # グラフを表示する\n\n&lt;BarContainer object of 864 artists&gt;\n\n\n\n\n\n\n\n\n\n別のグラフを作成してみましょう。\npolarsライブラリのデータ操作関数を使い、一部のデータを抽出してグラフを作成してみます。\n\ndf2 = df.filter((pl.col(\"項目\") == \"支出金額_複数単位\") & (pl.col(\"市区町村コード\") == 36201))\ndf2 = df2.select(\"ym\", \"品目分類\", \"value\")\ndf2.head(4)\n\n\nshape: (4, 3)\n\n\n\nym\n品目分類\nvalue\n\n\ni64\nstr\ni64\n\n\n\n\n201901\n\"アイスクリーム・シャーベット…\n367\n\n\n201901\n\"殺虫・防虫剤\"\n7\n\n\n201901\n\"米\"\n1138\n\n\n201902\n\"アイスクリーム・シャーベット…\n244\n\n\n\n\n\n\n上記のコードは以下のコードの実行結果と同じです。 polarsでは、.を使って処理内容をつなげて記述することができます。こうすることで変数への代入の手間がなくなるとともにコードを簡潔に記述できる利点があります。\n\ndf2 = df.filter((df[\"項目\"] == \"支出金額_複数単位\") & (df[\"市区町村コード\"] == 36201)).select(\n    \"ym\", \"品目分類\", \"value\"\n)\ndf2.head(4)\n\n\nshape: (4, 3)\n\n\n\nym\n品目分類\nvalue\n\n\ni64\nstr\ni64\n\n\n\n\n201901\n\"アイスクリーム・シャーベット…\n367\n\n\n201901\n\"殺虫・防虫剤\"\n7\n\n\n201901\n\"米\"\n1138\n\n\n201902\n\"アイスクリーム・シャーベット…\n244\n\n\n\n\n\n\nデータの加工が済んだのでグラフを作成します。品目分類別に折れ線グラフを作成したいので、groupby関数を使います。groupby関数は、指定した列の値ごとにデータをグループ化します。今回はitem列の値ごとにグループ化します。\n\n# pandasデータフレームに変換\ndf_pnd = df2.to_pandas()\n# 年月の表現を変換 (i64クラスの年月をdatetime形式に変換)\ndf_pnd[\"ym\"] = pd.to_datetime(df_pnd[\"ym\"], format=\"%Y%m\")\n\n# 品目ごとにデータフレームをグループ化\ngrouped = df_pnd.groupby(\"品目分類\")\n\n# グループごとにプロット\nfor item, group in grouped:\n    plt.plot(group[\"ym\"], group[\"value\"], label=item)\n\nplt.xlabel(\"年月\")\nplt.ylabel(\"金額\")\nplt.title(\"徳島市における品目分類別の支出金額の推移\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n2.0.1.1 棒グラフの塗り分け\n\n# グループごとに値を集計\ngrouped_df = df2.groupby(\"品目分類\").agg(total_value=pl.col(\"value\").sum())\n# グループ名と集計された値を取得\ngroups, aggregated_values = grouped_df.get_columns()\n\n# カラーマップを作成\ncolors = plt.cm.viridis(np.linspace(0, 1, len(groups)))\n\n# 棒グラフを描画し、グループごとに塗り分け\nplt.bar(groups, aggregated_values, color=colors)\n\nplt.xlabel(\"品目\")\nplt.ylabel(\"総支出金額\")\nplt.title(\"徳島市における品目別支出金額総計\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n2.0.2 インタラクティブに操作可能な図の作成\nplotlyライブラリを使うとインタラクティブに操作できるグラフを作成できます。\n\nimport plotly.graph_objects as go\n\n\n# 棒グラフを作成\nfig = go.Figure(data=[go.Bar(x=groups, y=aggregated_values)])\n\n\n# グラフのタイトルと軸ラベルを設定\nfig.update_layout(\n    title=\"徳島市における品目別支出金額総計\",\n    xaxis_title=\"品目\",\n    yaxis_title=\"総支出金額\",\n)\n\n# グラフを表示\nfig.show()"
  },
  {
    "objectID": "week03/index.html#この章の参考資料url",
    "href": "week03/index.html#この章の参考資料url",
    "title": "機械学習の背景・数理",
    "section": "この章の参考資料・URL",
    "text": "この章の参考資料・URL\n\n(石川聡彦 2018)\n(椎名洋 ほか 2019)\n(八谷大岳 2020)\n(吉田拓真 と 尾原颯 2018)\n\n\n\n\n\n八谷大岳. 2020. ゼロからつくるPython機械学習プログラミング入門 = Introduction to Machine Learning from Scratch with Python. 機械学習スタートアップシリーズ. 講談社. http://id.ndl.go.jp/bib/030584765.\n\n\n吉田拓真, と 尾原颯. 2018. 現場で使える!NumPyデータ処理入門 : 機械学習・データサイエンスで役立つ高速処理手法. 翔泳社. http://id.ndl.go.jp/bib/029316312.\n\n\n椎名洋, 姫野哲人, 保科架風, と 清水昌平. 2019. データサイエンスのための数学 = Mathematics for Data Science. 編集者： 清水昌平. データサイエンス入門シリーズ. 講談社. http://id.ndl.go.jp/bib/029903862.\n\n\n石川聡彦. 2018. 人工知能プログラミングのための数学がわかる本 = MATHEMATICS FOR AI PROGRAMMING. KADOKAWA. http://id.ndl.go.jp/bib/028819866."
  },
  {
    "objectID": "week03/03_mathematics.html#利用するライブラリの読み込み",
    "href": "week03/03_mathematics.html#利用するライブラリの読み込み",
    "title": "3  機械学習で用いる数学の基礎",
    "section": "3.1 利用するライブラリの読み込み",
    "text": "3.1 利用するライブラリの読み込み\nPython標準ライブラリおよびJupyterHub環境（hub-scipy-notebookを想定）で導入済みのライブラリを使用します。ここで扱う数学的な処理は標準ライブラリのmathと数値計算、多次元配列を扱うライブラリであるNumPyです。どちらも同様の機能を提供します。\n\n```{python}\n# japanize_matplotlibのインストール\n!pip install -qq japanize_matplotlib\n```\n\n\n# 利用するライブラリを読み込む\n# as で別名をつけることができる（長いライブラリ名を省略）\nimport math # 数学演算のための標準ライブラリ\nimport matplotlib.pyplot as plt # グラフ作成用\nimport japanize_matplotlib # matplotlibの日本語化\nimport numpy as np # 数値計算用"
  },
  {
    "objectID": "week03/03_mathematics.html#変数定数関数",
    "href": "week03/03_mathematics.html#変数定数関数",
    "title": "3  機械学習で用いる数学の基礎",
    "section": "3.2 変数・定数・関数",
    "text": "3.2 変数・定数・関数\n\n\n\n変数・定数・関数の関係\n\n\n\n変数: さまざまな値を取り得る値\n定数: 決められた値\n関数: 入力と出力の関係性を表す数式。入力\\(x\\)が決まると出力\\(y\\)の値も決まる、一対一の関係。\n\n消費税率10%の店で100円のトマトを3個買うとき…\n\nTOMATO_PRICE = 100\nTAX = 10\nn = 3\n\nprint(\"金額: \", (TOMATO_PRICE + (TOMATO_PRICE/TAX)) * n, \"円\")\n\n金額:  330.0 円\n\n\n同じ店で同じトマトを5個買うとき…\n\nn = 5\n\nprint(\"金額: \", (TOMATO_PRICE + (TOMATO_PRICE/TAX)) * n, \"円\")\n\n金額:  550.0 円\n\n\n変わらないのはトマトの価格と消費税率（TOMATO_PRICEとTAX）… 定数1\n変わったのはトマトの個数 … 変数\nこれらの定数と変数の関係を関数にまとめてみましょう。 Pythonでは関数の定義をdef文を使って行います。\n\n# 1個100円で消費税率10%の商品に対して、\n# 個数に応じていくらになるかを求める関数\ndef fn_tomato(n = 1):\n  PRICE = 100\n  TAX = 0.1\n  price_in_tax = PRICE + PRICE * TAX\n  # returnで関数の戻り値を指定\n  # return 文を省略すると関数は None を返す\n  return price_in_tax * n\n\n\nprint(\"金額: \", fn_tomato(3), \"円\") # 3個\nprint(\"金額: \", fn_tomato(5), \"円\") # 5個\n\n金額:  330.0 円\n金額:  550.0 円\n\n\n関数を用いることで、定数の記述を省略し、変数だけを変更した計算が容易に行えるようになりました。このように関数では、関連のある一連の処理をまとめて再利用することができて便利です。また、コードに問題が生じたときも関数の部分に注目して修正を行える利点があります。"
  },
  {
    "objectID": "week03/03_mathematics.html#一次関数",
    "href": "week03/03_mathematics.html#一次関数",
    "title": "3  機械学習で用いる数学の基礎",
    "section": "3.3 一次関数",
    "text": "3.3 一次関数\n一次関数\\(y=ax+b\\)について考えます。この一次関数では2つの定数\\(a\\)と\\(b\\)が存在し、それぞれ傾きと切片として機能します。\nここで、定数が異なる一次関数がどのようにグラフ上で変化するか確認しましょう。変数である\\(x\\)にはNumPyのlinspace()関数により生成した隣接する項との差が一定の等差数列を与えます。\n\n# 2つの一次関数によるグラフを作成する\nx = np.linspace(0, 10, 5) # 0から10までの公差が2.5の等差数列を生成\n\n# 定数が異なる一次関数に変数を当てはめてyの値を得る\ny1 = 3 * x + 5\ny2 = 2.4 * x + 2\n\n# xとyの関係をグラフに描画\nplt.grid(True)\nplt.plot(x, y1, label=\"式 y = 3x + 5\", color=\"#5097F8\")\nplt.plot(x, y2, label=\"式 y = 2.4x + 2\", color=\"#fc5998\")\nplt.legend(loc=\"upper left\")\nplt.show()\n\n\n\n\n\n\n\n\n一次関数のグラフは直線を描きます。異なる定数からなる一次関数のグラフは、異なる直線になっていることがわかります。\n機械学習・深層学習では、このような関数の出力に影響を及ぼす「重み」を変数として学習し、未知のデータに対しては定数として「重み」を利用します。特に深層学習では複数の中間層が「重み」をもつことになり、より柔軟な表現力を得ることがモデルの性能向上に貢献しています。"
  },
  {
    "objectID": "week03/03_mathematics.html#平方根",
    "href": "week03/03_mathematics.html#平方根",
    "title": "3  機械学習で用いる数学の基礎",
    "section": "3.4 平方根",
    "text": "3.4 平方根\n平方根はmath、NumPyで利用方法が異なります。 それぞれのライブラリでの平方根の使い方をみましょう。\n\\(\\sqrt{2}\\)\n\nmath.sqrt(2)\n\nnp.sqrt(2)\n\n1.4142135623730951\n\n\n\\(\\sqrt{3}\\)\n\nmath.sqrt(3)\n\nnp.sqrt(3)\n\n1.7320508075688772\n\n\n\n# 問題: 「富士山麓オウム鳴く」となる整数は？\nx = x\nnp.sqrt(x)"
  },
  {
    "objectID": "week03/03_mathematics.html#累乗",
    "href": "week03/03_mathematics.html#累乗",
    "title": "3  機械学習で用いる数学の基礎",
    "section": "3.5 累乗",
    "text": "3.5 累乗\n\n**演算子\nmathライブラリのpow()関数\nnumpyライブラリのpower()関数\n\n\nbase = 3 # 底\nexponent = 2 # 指数\n\n\nbase**exponent\n\n9\n\n\n\nmath.pow(base, exponent)\n\n9.0\n\n\n\nnp.power(base, exponent)\n\n9"
  },
  {
    "objectID": "week03/03_mathematics.html#指数関数と対数関数",
    "href": "week03/03_mathematics.html#指数関数と対数関数",
    "title": "3  機械学習で用いる数学の基礎",
    "section": "3.6 指数関数と対数関数",
    "text": "3.6 指数関数と対数関数\n\n3.6.1 指数関数\n指数関数… 指数を変数とした関数\n\n# 引数として与えられた数値の指数関数の値を返す\n# e^2\n# eは自然対数の底。およそ2.71828\nnp.exp(2)\n\n# np.power()で同じ値が得られることを確認\n# np.power(2.71828, 2)\n\n7.38905609893065\n\n\n\n# 0から10まで0.1刻みの配列を生成\nx = np.arange(0, 10, 0.1)\n\ny = np.exp(x)\n\nplt.suptitle(\"指数関数のグラフ\")\nplt.plot(x, y, color = \"#5097F8\")\nplt.grid(True)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3.6.2 対数関数\n\nx = np.exp(2)\nnp.log(x)\n\n2.0\n\n\n\n# 0.1から10まで0.1刻みの配列を生成\nx = np.arange(0.1, 10, 0.1)\n\ny = np.log(x)\n\nplt.suptitle(\"対数関数のグラフ\")\nplt.plot(x, y, color = \"#5097F8\")\nplt.grid(True)\n\nplt.show()\n\n\n\n\n\n\n\n\n常用対数\n\nnp.log10(10)\n\n1.0"
  },
  {
    "objectID": "week03/03_mathematics.html#ベクトル",
    "href": "week03/03_mathematics.html#ベクトル",
    "title": "3  機械学習で用いる数学の基礎",
    "section": "3.7 ベクトル",
    "text": "3.7 ベクトル\nリストは行ベクトルとして機能します。ただしベクトル同士の演算を行うことは難しいです。\n\nvector = [1, 2, 3, 4, 5]\nprint(vector)\n\n[1, 2, 3, 4, 5]\n\n\nそのためベクトル演算を実現するにはNumPyのarray()関数を利用する方法が簡単です。なおのarray()関数では厳密に行ベクトルまたは列ベクトルを区別せずに1次元配列を表現するために使用されます。\n\nvector = np.array([1, 2, 3, 4, 5])\n\n\n# ベクトルの和\nvector + vector\n\narray([ 2,  4,  6,  8, 10])\n\n\n\n# ベクトルの差\nvector - np.array([0, 2, 4, 1, 1])\n\narray([ 1,  0, -1,  3,  4])\n\n\n\n# リストによるベクトルでは+演算子を使ったベクトルの和の計算が行えない\n[1, 2, 3, 4, 5]+[1, 2, 3, 4, 5]\n\n[1, 2, 3, 4, 5, 1, 2, 3, 4, 5]\n\n\nベクトルの和は同じ成分の数からなるベクトル同士でないと計算できません。\n\nvector2 = np.array([6, 7])\n\nvector + vector2\n\nValueError: operands could not be broadcast together with shapes (5,) (2,) \n\n\nNumPyでベクトルの要素数を知る方法は、numpy.arrayオブジェクトのsize属性を使用する、len()関数を用いるなどがあります。\n\n# numpy.arrayの要素数を確認\nvector.size\n\nnp.size(vector)\nnp.size(vector2)\n\nlen(vector)\n\n5\n\n\n\n3.7.1 ベクトルの内積: 同一次元のベクトル同士のかけ算\n\n# ベクトル u と v を定義\nu = np.array([1, 2, 3])\nv = np.array([4, 5, 6])\n\n# NumPyのdot関数を使って内積を計算\ninner_product = np.dot(u, v)\n\nprint(\"内積:\", inner_product)\n\n内積: 32\n\n\n\n# 検算\n(1 * 4) + (2 * 5) + (3 * 6)\n\n32\n\n\n内積は次元の等しいベクトル同士でないと求められません。\n\nnp.dot(u, np.array([3, 2]))\n\nValueError: shapes (3,) and (2,) not aligned: 3 (dim 0) != 2 (dim 0)"
  },
  {
    "objectID": "week03/03_mathematics.html#行列",
    "href": "week03/03_mathematics.html#行列",
    "title": "3  機械学習で用いる数学の基礎",
    "section": "3.8 行列",
    "text": "3.8 行列\n行列の表現もNumPyを使うと演算（行列の加算、減算、乗算、転置、逆行列）が簡単に行えるようになります。\n\nmatrix = np.array([\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n])\n\nmatrix\n\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\n\n\n3.8.1 行列の転置\n\n# 行列の縦と横を入れ替える\nmatrix.T\n\narray([[1, 4, 7],\n       [2, 5, 8],\n       [3, 6, 9]])"
  },
  {
    "objectID": "week03/03_mathematics.html#footnotes",
    "href": "week03/03_mathematics.html#footnotes",
    "title": "3  機械学習で用いる数学の基礎",
    "section": "",
    "text": "Pythonでは変数と定数を明確に区分する仕組みはありません。Pythonコミュニティにおけるコードの記述ルールをまとめたコーディング規約であるPEP 8では定数の名前は大文字で書き、単語をアンダースコアで区切ることが明記されています。↩︎"
  },
  {
    "objectID": "week04/index.html#この章の参考資料url",
    "href": "week04/index.html#この章の参考資料url",
    "title": "機械学習モデルの設計と評価",
    "section": "この章の参考資料・URL",
    "text": "この章の参考資料・URL\n\n(岡野原大輔 2022)\n(門脇大輔 ほか 2019)\n(八谷大岳 2020)\n(Géron, 下田倫大, と 長尾高弘 2020)\n(Alice Zheng と ホクソエム 2019)\n\n\n\n\n\nAlice Zheng, Amanda Casari, と ホクソエム. 2019. 機械学習のための特徴量エンジニアリング : その原理とPythonによる実践. 翻訳者： ホクソエム. オライリー・ジャパン. http://id.ndl.go.jp/bib/029512290.\n\n\nGéron, Aurélien, 下田倫大, と 長尾高弘. 2020. scikit-learn、Keras、TensorFlowによる実践機械学習. 翻訳者： 下田倫大 と 長尾高弘. 第2版 版. オライリー・ジャパン. http://id.ndl.go.jp/bib/030701507.\n\n\n八谷大岳. 2020. ゼロからつくるPython機械学習プログラミング入門 = Introduction to Machine Learning from Scratch with Python. 機械学習スタートアップシリーズ. 講談社. http://id.ndl.go.jp/bib/030584765.\n\n\n岡野原大輔. 2022. ディープラーニングを支える技術 : 「正解」を導くメカニズム〈技術基礎〉. Tech×Books plus. 技術評論社. http://id.ndl.go.jp/bib/031881422.\n\n\n門脇大輔, 阪田隆司, 保坂桂佑, と 平松雄司. 2019. Kaggleで勝つデータ分析の技術. 技術評論社. http://id.ndl.go.jp/bib/029976550."
  },
  {
    "objectID": "week04/0401_penguins.html#ペンギンデータ",
    "href": "week04/0401_penguins.html#ペンギンデータ",
    "title": "4  南極大陸に生育するペンギンの体長データ",
    "section": "4.1 ペンギンデータ",
    "text": "4.1 ペンギンデータ\n機械学習による分類問題の例として、南極大陸に生育するペンギンの体長についての観測データを利用します。このデータを使ってペンギンの種類を分類するモデルを構築します。\n\n4.1.1 データの読み込み\nペンギンデータはseabornライブラリに含まれています。データを利用するにはload_dataset()関数で読み込みます。\n\n# ペンギンデータの読み込み\npenguins = sns.load_dataset(\"penguins\")\n\n# pandasのDataFrameとして格納されている\n# type(penguins)\n\n\n\n4.1.2 データの確認\nペンギンデータの中身を確認します。データについて理解を深めるためのいくつかの処理を行ってみましょう。まずはデータフレームオブジェクトの属性の一つ、shapeを使うと、行数と列数を確認できます。\n\npenguins.shape\n\n# 以下のコードの実行はエラーとなる\n# 理由はshapeがデータフレームの属性であるのに対して、\n# shape()はメソッド（関数）であるため\n# penguins.shepe()\n\n(344, 7)\n\n\n以下の例では、いくつかのpandasのデータフレーム関数を使って、データの概要を確認しています。これらは関数として実行するため、末尾に括弧を付けています。\n\n# データフレームの概要\n# 各列（変数）の名前（Column）、欠損値の件数（Non-Null Count）、データ型(Dtype)を表示\npenguins.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 7 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    object \n 1   island             344 non-null    object \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    object \ndtypes: float64(4), object(3)\nmemory usage: 18.9+ KB\n\n\nいくつの変数に欠損値があるようです。欠損値の有無は機械学習モデルを実行する際に影響を及ぼす可能性があります。ここでは欠損値への処理を行いませんが、ペンギンデータに欠損値が含まれるということを覚えておきましょう。\nつづいて具体的なデータの中身を確認します。head()関数を使うと、データの先頭から引数で指定した行数を表示できます。tail()関数はデータの末尾から指定した行数を表示するのに利用します。\n\n# 先頭5行の表示\npenguins.head(5)\n\n# 末尾3行の表示\n# penguins.tail(3)\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n\n\n\n\n\nここでペンギンデータの変数をまとめておきましょう。このデータには次の変数が含まれます。\n\nspecies: ペンギンの種名。次の3種が含まれます。Adelie（アデリーペンギン）、Chinstrap（ヒゲペンギン）、Gentoo（ジェンツーペンギン）。\nisland: ペンギンが生息する島の名前。次の3つの島が含まれます。Biscoe、Dream、Torgersen。\nbill_length_mm: くちばしの長さ（単位: mm）。\nbill_depth_mm: くちばしの深さ（単位: mm）。くちばしの高さとも言えます。\nflipper_length_mm: 翼の長さ（単位: mm）。ペンギンの場合はひれの長さとも言えます。\nbody_mass_g: 体重（単位: g）。\nsex: 性別。Male（オス）、Female（メス）の2種類。\n\n分類対象として扱うペンギンの種名について、詳しく調べます。次のコードでそれぞれの種名のデータが何件含まれるかを確認します。\n\nprint(\"ペンギンの種類: \", penguins[\"species\"].unique())\n\npenguins.groupby(\"species\").size().reset_index(name=\"count\")\n\nペンギンの種類:  ['Adelie' 'Chinstrap' 'Gentoo']\n\n\n\n\n\n\n\n\n\nspecies\ncount\n\n\n\n\n0\nAdelie\n152\n\n\n1\nChinstrap\n68\n\n\n2\nGentoo\n124\n\n\n\n\n\n\n\n\n\n\n\n\nアデリーペンギン\n\n\n\n\n\nヒゲペンギン\n\n\n\n\n\nジェンツーペンギン\n\n\n\n\n\n\n4.1.3 統計量\npandasのdescribe()関数を使うと、平均値や標準偏差、最小値・最大値といった複数の統計量を一度に確認できます。\n\n# ペンギンデータの統計量の表示\npenguins.describe()\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\ncount\n342.000000\n342.000000\n342.000000\n342.000000\n\n\nmean\n43.921930\n17.151170\n200.915205\n4201.754386\n\n\nstd\n5.459584\n1.974793\n14.061714\n801.954536\n\n\nmin\n32.100000\n13.100000\n172.000000\n2700.000000\n\n\n25%\n39.225000\n15.600000\n190.000000\n3550.000000\n\n\n50%\n44.450000\n17.300000\n197.000000\n4050.000000\n\n\n75%\n48.500000\n18.700000\n213.000000\n4750.000000\n\n\nmax\n59.600000\n21.500000\n231.000000\n6300.000000\n\n\n\n\n\n\n\ndescribe()関数が対象とするのはデータフレームの数値データです。speciesやislandのような文字列データは対象となりません。理由は文字列データは平均値（mean）や標準偏差（std）などの統計量に意味がないためです1。しかし最頻値（top）や最頻値の出現回数（freq）は計算できます。describe()関数のデフォルトでは数値データのみが対象となりますが、引数でinclude=\"all\"を指定することで文字列データも対象となります。\n\n# 文字列の変数も含めた統計量の表示\npenguins.describe(include=\"all\")\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\ncount\n344\n344\n342.000000\n342.000000\n342.000000\n342.000000\n333\n\n\nunique\n3\n3\nNaN\nNaN\nNaN\nNaN\n2\n\n\ntop\nAdelie\nBiscoe\nNaN\nNaN\nNaN\nNaN\nMale\n\n\nfreq\n152\n168\nNaN\nNaN\nNaN\nNaN\n168\n\n\nmean\nNaN\nNaN\n43.921930\n17.151170\n200.915205\n4201.754386\nNaN\n\n\nstd\nNaN\nNaN\n5.459584\n1.974793\n14.061714\n801.954536\nNaN\n\n\nmin\nNaN\nNaN\n32.100000\n13.100000\n172.000000\n2700.000000\nNaN\n\n\n25%\nNaN\nNaN\n39.225000\n15.600000\n190.000000\n3550.000000\nNaN\n\n\n50%\nNaN\nNaN\n44.450000\n17.300000\n197.000000\n4050.000000\nNaN\n\n\n75%\nNaN\nNaN\n48.500000\n18.700000\n213.000000\n4750.000000\nNaN\n\n\nmax\nNaN\nNaN\n59.600000\n21.500000\n231.000000\n6300.000000\nNaN\n\n\n\n\n\n\n\n\n4.1.3.1 相関係数\n次にペンギンデータの変数間の相関関係をcorr()関数で確認します。相関係数は数値データに対してのみ計算されるため、カテゴリ変数であるspeciesやislandは除外しておきます。さらに、相関係数は欠損値がある場合、適切な値を計算できないため、欠損値を含む行を削除しておきます。\n\n# 欠損値を含む行を削除\npenguins_mod = penguins.dropna()\n\n# カテゴリ変数を含まないデータフレームを作成\npenguins_numeric = penguins_mod.drop(columns=[\"species\", \"island\", \"sex\"])\n\n# 相関係数の計算\ncorrelation_matrix = penguins_numeric.corr()\n\n# 相関係数の表示\ncorrelation_matrix\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\nbill_length_mm\n1.000000\n-0.228626\n0.653096\n0.589451\n\n\nbill_depth_mm\n-0.228626\n1.000000\n-0.577792\n-0.472016\n\n\nflipper_length_mm\n0.653096\n-0.577792\n1.000000\n0.872979\n\n\nbody_mass_g\n0.589451\n-0.472016\n0.872979\n1.000000\n\n\n\n\n\n\n\n相関係数は0から1の間の値となり、絶対値が大きいほど強い相関関係があることを示します。そのため自分自身との相関係数は1となります。符号が正のとき、2つの変数は正の相関関係にあります。つまり、片方の変数が増加すると、もう片方の変数も増加する傾向にあることを示します。一方符号が負のとき、2つの変数は負の相関関係にあります。これは片方の変数が増加すると、もう片方の変数は減少する傾向にあることを意味します。\n\n\n\n4.1.4 データの可視化\nさらにデータへの理解を深めるために、グラフ上にペンギンデータを表現してみます。データ可視化によって、細かな傾向や全体的な特徴を発見することにつながる可能性があります。\n\n4.1.4.1 散布図\n\n# seabornを使った散布図の作成\nsns.scatterplot(\n    data=penguins,\n    x=\"bill_length_mm\",\n    y=\"bill_depth_mm\",\n    # 塗り分けの変数の指定\n    hue=\"species\",\n)\nplt.show()\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n\n\n\n\n\n\n\n複数の変数の組み合わせについて散布図を作成するには、seabornのpairplot()関数を利用します。\n\nsns.pairplot(penguins, hue=\"species\")\nplt.show()\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n\n\n\n\n\n\n\npairplot()関数はデータフレームの数値データの組み合わせについて散布図を作成します。body_mass_gとbody_mass_gのように、自分自身との組み合わせについてはヒストグラムを描画します。pairplot()の出力のような変数の組み合わせに対応する散布図とヒストグラムなどをひとまとめにしたものを散布図行列と呼びます。\n\n\n4.1.4.2 箱ひげ図\nペンギンデータの数値変数の分布を箱ひげ図を用いて確認します。箱ひげ図はseabornのboxplot()関数を使って作成します。\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\nsns.boxplot(x=\"species\", y=\"bill_length_mm\", data=penguins, ax=axes[0])\nsns.boxplot(x=\"species\", y=\"bill_depth_mm\", data=penguins, ax=axes[1])\nplt.show()\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nノート\n\n\n\n\n同様にflipper_length_mmとbody_mass_gについても箱ひげ図を作成しましょう。\nここで示した散布図、箱ひげ図のほかのデータ可視化方法を調べて実行してみましょう。"
  },
  {
    "objectID": "week04/0401_penguins.html#footnotes",
    "href": "week04/0401_penguins.html#footnotes",
    "title": "4  南極大陸に生育するペンギンの体長データ",
    "section": "",
    "text": "文字列データは数値データとは異なるため、算術平均を計算することはできません。そのため、平均値や標準偏差などの統計量の出力はNaN(Not a Number)として表示されます。一方で数値データは文字列データとは異なり、最頻値を計算することはできません。そのため、最頻値の出力はNaNとして表示されます。↩︎"
  },
  {
    "objectID": "week04/0402_classification.html#scikit-learnを使った機械学習モデルの構築",
    "href": "week04/0402_classification.html#scikit-learnを使った機械学習モデルの構築",
    "title": "5  ペンギンデータの分類に挑戦",
    "section": "5.1 scikit-learnを使った機械学習モデルの構築",
    "text": "5.1 scikit-learnを使った機械学習モデルの構築\nペンギンデータをロジスティック回帰モデルによって分類します。\n\n5.1.1 前処理\n具体的には欠損値への対応とラベルエンコーディングを実行します。\nロジスティック回帰など、いくつかの機械学習モデルは欠損値を含むデータを直接扱うことができません。そのため、欠損値を含む行を削除するか、欠損値を別の値に置き換える必要があります。今回はpandasのdropna()関数で欠損値を含む行を削除します。\n\n# 1/2 欠損値を含む行を削除\npenguins.dropna(inplace=True)  # 元のデータフレームを書き換える場合、inplace=Trueを指定する\n\n# 11行削除されていることを確認\npenguins.shape\n\n(333, 7)\n\n\n続いてラベルエンコーディングを適用します。ラベルエンコーディングとは、カテゴリ変数を数値に変換する処理です。例えば、species列の値Adelie、Chinstrap、Gentooをそれぞれ0, 1, 2に変換します。機械学習モデルは数値データを扱うため、カテゴリ変数を数値に変換する必要があります。ペンギンデータの場合、speciesの他にislandやsex列もカテゴリ変数なのでラベルエンコーディングを適用対象となります。ラベルエンコーディングはscikit-learnのLabelEncoderクラスを使って実行します。\n\n# 2/2 ラベルエンコーディング\n# LabelEncoderクラスのインスタンスを作成\nle = LabelEncoder()\n# 以下のコードの実行で、species列の値Adelie、Chinstrap、Gentooがそれぞれ0, 1, 2に変換される\npenguins[\"species\"] = le.fit_transform(penguins[\"species\"])\n# island列とsex列についても同様にラベルエンコーディングを実行\npenguins[\"island\"] = le.fit_transform(penguins[\"island\"])\npenguins[\"sex\"] = le.fit_transform(penguins[\"sex\"])\n\n\n# 前処理を行ったデータを表示\n# ラベルエンコーディングが適用された3列に注目\npenguins\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\n0\n2\n39.1\n18.7\n181.0\n3750.0\n1\n\n\n1\n0\n2\n39.5\n17.4\n186.0\n3800.0\n0\n\n\n2\n0\n2\n40.3\n18.0\n195.0\n3250.0\n0\n\n\n4\n0\n2\n36.7\n19.3\n193.0\n3450.0\n0\n\n\n5\n0\n2\n39.3\n20.6\n190.0\n3650.0\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n338\n2\n0\n47.2\n13.7\n214.0\n4925.0\n0\n\n\n340\n2\n0\n46.8\n14.3\n215.0\n4850.0\n0\n\n\n341\n2\n0\n50.4\n15.7\n222.0\n5750.0\n1\n\n\n342\n2\n0\n45.2\n14.8\n212.0\n5200.0\n0\n\n\n343\n2\n0\n49.9\n16.1\n213.0\n5400.0\n1\n\n\n\n\n333 rows × 7 columns\n\n\n\n\n\n\n元の値\nラベルエンコーディング後の値\n\n\n\n\nAdelie\n0\n\n\nChinstrap\n1\n\n\nGentoo\n2\n\n\n\n\n\n5.1.2 データ分割\n\n訓練データ: モデルの学習に用いるデータ。モデルはこのデータの特徴を学習し、テストデータに対する予測を行うためのパラメータを決定する。\nテストデータ: モデルの予測精度を評価するためのデータ。モデルの学習には用いられない。\n\n\n# 目的変数をspecies列、説明変数をspecies列以外の列とする\n# X... 説明変数のみからなるデータフレーム\n# y... 目的変数のみからなるデータフレーム\nX = penguins.drop(columns=\"species\")\ny = penguins[\"species\"]\n\n# 訓練データとテストデータに分割\n# test_sizeでテストデータの割合を指定する。ここでは全体の20%をテストデータとする（80%を訓練データとする）\n# random_stateは乱数のシードを指定する引数。この値を変更すると結果が変わるので注意\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=20230508\n)\n\n\n\n5.1.3 モデルの構築\nsklearn.linear_modelモジュールのLogisticRegressionを使ってロジスティック回帰モデルを構築します。\n\n# ロジスティック回帰モデルを適用し、モデルを訓練\n# max_iter引数でパラメータ探索のための試行回数を設定（デフォルトでは100）\nlr = LogisticRegression(max_iter=1000)\n# 訓練データを使ってモデルを訓練\nlr.fit(X_train, y_train)\n\nLogisticRegression(max_iter=1000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(max_iter=1000)\n\n\n\n\n5.1.4 モデルの予測\n構築したモデルに対して、テストデータを使って予測を行います。lr.predict(X_test)を実行すると、テストデータに対する予測結果が得られます。\n\n# テストデータを使って予測を行う\ny_pred = lr.predict(X_test)\n\n# 予測結果は配列で格納されている\ny_pred\n\narray([1, 1, 0, 2, 2, 1, 2, 2, 0, 1, 2, 2, 0, 1, 2, 2, 0, 1, 1, 0, 0, 0,\n       1, 1, 2, 0, 2, 2, 0, 2, 0, 1, 2, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n       1, 1, 0, 0, 2, 0, 2, 0, 0, 2, 2, 2, 2, 0, 1, 1, 1, 0, 0, 2, 0, 2,\n       2])\n\n\nロジスティック回帰モデルでは、各クラス（ここではペンギンの種名）に属する確率を出力することもできます。構築したモデルのインスタンスとpredict_proba()関数を実行すると、各データポイントに対するクラスの所属確率を配列として取得できます。\n\n# テストデータがどのクラスに属するかの確率を出力\nprobabilities = lr.predict_proba(X_test)\n\nペンギンの種名は3つのクラスに分類されるので、predict_proba()関数の出力は3列の配列となります。各データポイントに対して、それぞれのクラスに属する確率が出力されています。このうち、最も確率が高いクラスが予測結果となります。\n\n# テストデータの一件目のデータポイントに対するクラス別の所属確率\n# 表示を見やすくするために、小数点以下4桁まで表示するように設定\nnp.set_printoptions(suppress=True, formatter={\"float\": \"{:.4f}\".format})\nprint(probabilities[0])\n\n[0.0010 0.9986 0.0004]\n\n\n\n# 実際の予測結果\ny_pred[0]\n\n1"
  },
  {
    "objectID": "week04/0402_classification.html#モデルの評価",
    "href": "week04/0402_classification.html#モデルの評価",
    "title": "5  ペンギンデータの分類に挑戦",
    "section": "5.2 モデルの評価",
    "text": "5.2 モデルの評価\n\n# 真の値であるy_testと予測値のy_predを比較して2つの値がどの程度一致しているかを確認する\n# y_testをデータフレームに変換\ny_test_df = pd.DataFrame(y_test)\n# y_predをデータフレームに変換\ny_pred_df = pd.DataFrame(y_pred, columns=[\"predicted\"], index=y_test.index)\n\n# y_testとy_predのデータフレームを結合\ncomparison_df = pd.concat([y_test_df, y_pred_df], axis=1)\n\n# speciesとpredictedが一致しているかどうかを確認\ncomparison_df[\"correct\"] = comparison_df[\"species\"] == comparison_df[\"predicted\"]\n\n# TrueとFalseの数をカウント\ncomparison_df.groupby(\"correct\").size().reset_index(name=\"count\")\n\n\n\n\n\n\n\n\ncorrect\ncount\n\n\n\n\n0\nFalse\n1\n\n\n1\nTrue\n66\n\n\n\n\n\n\n\n\n\n一致した件数は 66/67件です。これは、モデルが正しく予測できたデータの割合が約98%であることを意味します。\n\n\nscikit-learnには、モデルの評価を行うための様々な関数が用意されています。ここでは、sklearn.metricsモジュールの関数を使ってモデルの評価を行います。\naccuracy_score()関数は、正解率を計算する関数です。正解率は、正しく予測できたデータの割合を表します。この値は先ほど確認した正解率と一致します。\n\n# モデルを評価\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\n\nAccuracy: 0.9850746268656716\n\n\n種名ごとの精度を確認するためにclassification_report()関数を使います。\n\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       1.00      0.97      0.98        29\n           1       0.93      1.00      0.97        14\n           2       1.00      1.00      1.00        24\n\n    accuracy                           0.99        67\n   macro avg       0.98      0.99      0.98        67\nweighted avg       0.99      0.99      0.99        67\n\n\n\n各クラスごとに、 precision、recall、f1-score、supportの4つの値が表示されます。それぞれの値の意味は以下の通りです。\n\nprecision: 適合率。正と予測したデータのうち、実際に正であるものの割合。\nrecall: 再現率。実際に正であるもののうち、正であると予測されたものの割合。\nf1-score: 適合率と再現率の調和平均により得られた値。適合率と再現率のバランスを考慮した評価指標。この値が高いほど、適合率と再現率の両方が高いことを示す。\nsupport: 分類されたデータ数。\n\nここで出力される評価指標についてはaccuracy_score()関数のように個別の関数を使った算出も可能です。具体的にはprecisionはprecision_score()、recallはrecall_score()、f1-scoreはf1_score()関数を利用して求めます。\n\nprint(\"Precision:\", precision_score(y_test, y_pred, average=None))\nprint(\"Recall:\", recall_score(y_test, y_pred, average=None))\nprint(\"F1-Score:\", f1_score(y_test, y_pred, average=None))\n\n\n5.2.1 混合行列\n最後に、分類モデルの性能評価に使われる混合行列を確認しましょう。混合行列は、モデルの予測結果と実際のクラスの関係を4つの要素に分類した行列です。以下の4つの要素に分類されます。\n\nTrue Positive（TP）：正のクラスを正と予測し、実際に正である場合。\nFalse Positive（FP）：正のクラスを正と予測したが、実際には負である場合。\nTrue Negative（TN）：負のクラスを負と予測し、実際に負である場合。\nFalse Negative（FN）：負のクラスを負と予測したが、実際には正である場合。\n\nscikit-learnでは、confusion_matrix()関数を使って混合行列を作成できます。ここではさらに、ConfusionMatrixDisplay()関数を使って混合行列を可視化する例を示します。\n\ncm = confusion_matrix(y_test, y_pred)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=lr.classes_)\n\ndisp.plot()\nplt.show()\n\n\n\n\n\n\n\n\n混合行列を見ることで、どのクラスが誤分類されやすいかを確認することができます。また、適合率や再現率の値がどのように計算されているかを理解することができます。\n具体的には以下のように計算されます。\n\n適合率: TP / (TP + FP)\n再現率: TP / (TP + FN)\n\n今回構築したロジスティック回帰モデルでの適合率を計算してみましょう。ここでは、ペンギンデータの3種のラベルそれぞれの適合率を以下のように計算します。\n\nfor i in range(3):  # ラベルは0, 1, 2の3種類\n    # True Positiveの数\n    TP = np.sum((y_pred == i) & (np.array(y_test) == i))\n\n    # False Positiveの数\n    FP = np.sum((y_pred == i) & (np.array(y_test) != i))\n\n    # Precisionの計算\n    precision = TP / (TP + FP)\n\n    print(f\"Precision for label {i}: {precision}\")\n\nPrecision for label 0: 1.0\nPrecision for label 1: 0.9333333333333333\nPrecision for label 2: 1.0\n\n\nこのラベルごとの適合率を平均した値が、モデルの適合率となります。precision_score()関数はデフォルトでラベルごとの適合率を計算しますが、average引数を指定することで、ラベルごとの適合率の平均値を求めることができます。\n\nprint(\"Average Precision:\", precision_score(y_test, y_pred, average=\"macro\"))\n\nAverage Precision: 0.9777777777777779\n\n\n\n\n\n\n\n\nノート\n\n\n\n\n数値変数の標準化、ラベルエンコーディングの代わりにダミー変数化を行ったデータでモデルの学習を行いましょう。"
  },
  {
    "objectID": "week05/index.html#この章の参考資料url",
    "href": "week05/index.html#この章の参考資料url",
    "title": "機械学習の手法",
    "section": "この章の参考資料・URL",
    "text": "この章の参考資料・URL\n\n(竹内一郎 と 烏山昌幸 2015)\n(森下光之助 2021)\n\n\n\n\n\n森下光之助. 2021. 機械学習を解釈する技術 = Techniques for Interpreting Machine learning : 予測力と説明力を両立する実践テクニック. 技術評論社. http://id.ndl.go.jp/bib/031576666.\n\n\n竹内一郎, と 烏山昌幸. 2015. サポートベクトルマシン = Support Vector Machine. 機械学習プロフェッショナルシリーズ. 講談社. http://id.ndl.go.jp/bib/026619227."
  },
  {
    "objectID": "week05/0501_regression.html#前処理特徴量エンジニアリング",
    "href": "week05/0501_regression.html#前処理特徴量エンジニアリング",
    "title": "6  線形回帰モデルによるペンギンの体重の予測",
    "section": "6.1 前処理・特徴量エンジニアリング",
    "text": "6.1 前処理・特徴量エンジニアリング\n一般的に欠損値を含むデータは、機械学習モデルに直接利用できません。そのため、欠損値の削除や欠損値の補完を行う必要があります。単純に欠損値を削除する方法がもっとも簡単な対策ですが、欠損値を削除すると、データの一部を失うことになるため、欠損値を含む行が多い場合やデータが少ない場合は注意が必要です。\n\n6.1.1 欠損値の削除\nここでは欠損値の削除で、モデルに与えるデータを用意します。欠損値の削除は、pandasのdropna()メソッドを用いて行います。\n\n# 前処理: 欠損値の削除\npenguins.dropna(inplace=True)\n# データの確認。欠損値の削除によっていくつかの行が削除される\npenguins.shape\n\n(333, 7)\n\n\n\n\n6.1.2 ラベルエンコーディングと標準化\n次にペンギンデータの変数に対する操作、特徴量エンジニアリングを行います。ここではラベルエンコーディングと標準化を行うことにしましょう。どちらの操作も、特徴量エンジニアリングの工程では変数の変換に分類されます。\nラベルエンコーディングは、カテゴリ変数を数値に変換する操作です。ロジスティック回帰モデルを扱った際にも適用しました。\n\n# 特徴量エンジニアリング: ラベルエンコーディングとスケーリング\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\npenguins[\"species\"] = le.fit_transform(penguins[\"species\"])\npenguins[\"island\"] = le.fit_transform(penguins[\"island\"])\npenguins[\"sex\"] = le.fit_transform(penguins[\"sex\"])\n\n標準化（スケーリング）は、変数の値を平均値0、標準偏差1に変換する操作です。標準化を行うことで、変数間のスケール（単位）を揃えることができます。例えば、ペンギンデータの体の部位はmmで記録されますが、くちばしの長さbill_length_mmはくちばしの太さbill_depth_mmよりも大きい値を取ります。また、体重はgで記録されます。このように単位が異なる変数をそのまま利用すると、値が大きな変数の影響が大きくなってしまいます。このような問題を解決するために、標準化を行います。\n\n# ペンギンデータのbill_length_mmについて平均値と標準偏差を求める\nprint(penguins[\"bill_length_mm\"].mean())\nprint(penguins[\"bill_length_mm\"].std())\n\n43.99279279279279\n5.468668342647559\n\n\n\n# bill_depth_mmについても平均値と標準偏差を求める\nprint(penguins[\"bill_depth_mm\"].mean())\nprint(penguins[\"bill_depth_mm\"].std())\n\n17.164864864864864\n1.9692354633199007\n\n\n\nfig, ax = plt.subplots(1, 2, figsize=(6.8, 3))\nsns.histplot(penguins, x=\"bill_length_mm\", ax=ax[0], color=\"#284b67\")\nsns.histplot(penguins, x=\"bill_depth_mm\", ax=ax[1], color=\"#1a2127\")\nplt.show()\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n\n\n\n\n\n\n\nscikit-learnでの標準化は、StandardScalerクラスを利用します。StandardScalerは、平均値と標準偏差を計算し、それらの値を利用して変数の値を変換します。標準化を行うことで、変数の値が平均値を中心に分布するようになります。\n\n# データの標準化\n# StandardScalerのインスタンスを用意\nscaler = StandardScaler()\n# ペンギンデータのbody_mass_g以外の数値変数をfit_transformメソッドにより標準化\npenguins[\n    [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\"]\n] = scaler.fit_transform(\n    penguins[[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\"]]\n)\n\nラベルエンコーディング、標準化を適用したデータを確認しましょう。\n\npenguins.head(n=3)\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\n0\n2\n-0.896042\n0.780732\n-1.426752\n3750.0\n1\n\n\n1\n0\n2\n-0.822788\n0.119584\n-1.069474\n3800.0\n0\n\n\n2\n0\n2\n-0.676280\n0.424729\n-0.426373\n3250.0\n0\n\n\n\n\n\n\n\n平均値0、標準偏差1に変換されていることが確認できます。\n\n# ペンギンデータのbill_length_mmについて平均値と標準偏差を求める\nprint(penguins[\"bill_length_mm\"].mean())\nprint(penguins[\"bill_length_mm\"].std())\n\n3.8407715446491904e-16\n1.0015048917468008\n\n\n\n# bill_depth_mmについても平均値と標準偏差を求める\nprint(penguins[\"bill_depth_mm\"].mean())\nprint(penguins[\"bill_depth_mm\"].std())\n\n6.40128590774865e-16\n1.0015048917468008\n\n\n標準化は変数の値を操作しますが、変数の分布は変化しません。そのため、標準化を行う前後でヒストグラムを比較すると、分布の形に違いがないことがわかります。\n\n\nコード\nfig, ax = plt.subplots(1, 2, figsize=(6.8, 3))\nsns.histplot(penguins, x=\"bill_length_mm\", ax=ax[0], color=\"#284b67\")\nsns.histplot(penguins, x=\"bill_depth_mm\", ax=ax[1], color=\"#1a2127\")\nplt.show()\n\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n\n\n\n\n\n\n\nここでは特徴量エンジニアリングの例として、ラベルエンコーディングと標準化を紹介しました。いずれも元の変数の値を別の値に置き換える操作となります。一方、特徴量エンジニアリングには既存のデータには存在しない、新しい変数を作成する工程も含まれます。例えば、身長と体重の変数を利用し、BMIを計算するなどがあります。またダミー変数化も既存の変数の情報をもとに変数を作成します。"
  },
  {
    "objectID": "week05/0501_regression.html#関数化による工程の整理",
    "href": "week05/0501_regression.html#関数化による工程の整理",
    "title": "6  線形回帰モデルによるペンギンの体重の予測",
    "section": "6.2 関数化による工程の整理",
    "text": "6.2 関数化による工程の整理\nここまでのデータの前処理・特徴量エンジニアリングの操作を整理してみましょう。具体的には一連のコードを関数として定義します。関数を作成することで、同じ処理を何度も繰り返す必要がなくなる利点があります。また、処理の流れの整理にも役立ちます。\n関数の定義は、以下のようにdef構文を利用します。defのあとに関数名を指定します。関数名は、任意の名前を付けることができますが、関数の処理内容がわかりやすい名前を付けるようにしましょう。ここではペンギンデータの前処理までを実行する関数としてpenguins_preprocess()という名前を付けます。\n\n# 関数定義の例\ndef penguins_preprocess(data):\n    # 処理内容の記述\n    return data\n\n関数名の後には、括弧の中で引数を指定します。引数は、関数の実行に必要な情報を渡すためのものです。ここでは入力データを指定するためにdataという引数を指定しています。関数の処理内容は、コロンのあとにインデントを利用して記述します。関数の処理が終わったら、return構文を利用して関数が返却する値（戻り値）を指定します。\n具体的なペンギンデータへの前処理・特徴量エンジニアリングの操作を関数化したものが以下になります。\n\ndef penguins_preprocess(data):\n    from sklearn.preprocessing import LabelEncoder, StandardScaler\n    \n    # 元のデータを保持するためデータをコピー\n    data_processed = data.copy()\n\n    # 前処理: 欠損値の削除\n    data_processed.dropna(inplace=True)\n\n    # 特徴量エンジニアリング: ラベルエンコーディング\n    le = LabelEncoder()\n    for col in [\"species\", \"island\", \"sex\"]:\n        data_processed[col] = le.fit_transform(data_processed[col])\n\n    # 特徴量エンジニアリング: データの標準化\n    scaler = StandardScaler()\n    data_processed[\n        [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\"]\n    ] = scaler.fit_transform(\n        data_processed[[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\"]]\n    )\n\n    return data_processed\n\n定義した関数はpenguins_preprocess()として機能します。もう一度、ペンギンデータを読み込み、penguins_preprocess()関数による前処理・特徴量エンジニアリングを実行してみましょう。\n\n# ペンギンデータの読み込み\npenguins = sns.load_dataset(\"penguins\")\n\n# 前処理と特徴量エンジニアリングの実行\npenguins_baked = penguins_preprocess(penguins)\n\n結果を確認します。関数で定義したラベルエンコーディング、標準化の処理が実行されていることがわかります。関数を利用することで複数の処理を一度に実行できるようになりました。\n\npenguins_baked.head(n=3)\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\n0\n2\n-0.896042\n0.780732\n-1.426752\n3750.0\n1\n\n\n1\n0\n2\n-0.822788\n0.119584\n-1.069474\n3800.0\n0\n\n\n2\n0\n2\n-0.676280\n0.424729\n-0.426373\n3250.0\n0"
  },
  {
    "objectID": "week05/0501_regression.html#データの分割",
    "href": "week05/0501_regression.html#データの分割",
    "title": "6  線形回帰モデルによるペンギンの体重の予測",
    "section": "6.3 データの分割",
    "text": "6.3 データの分割\nモデルの学習の前に、予測の対象となる変数（目的変数）とそれ以外の変数（説明変数）に分割します。今回、目的変数はbody_mass_g、説明変数はbody_mass_g以外の変数となります。\n\n# 説明変数と目的変数の設定\n# Xとしてbody_mass_g以外の変数を、yとしてbody_mass_gを設定\nX = penguins_baked.drop(columns=\"body_mass_g\")\ny = penguins_baked[\"body_mass_g\"]\n\n以上でデータの準備は完了です。次に、データをモデルの学習に用いる訓練データと、評価に利用するテストデータに分割します。今回は、訓練・テストデータに含まれるspeciesの割合が均等となるように分割します。このようなグループ間の割合を一定にする分割方法を層化抽出と呼びます。\nscikit-learnのtrain_test_split()関数を使用してデータ分割をします。引数により、テスト用データを20%（学習用データを80%）、species変数による層化抽出を行うように指定します。\n\n\n\nホールドアウト法\n\n\n\n# データ分割\n# test_size=0.2でテストデータの割合を20%に指定\n# random_stateは乱数のシードを指定。これにより、実行するたびに同じデータ分割を行うことができる\n# stratifyで層化抽出を行う。今回はspeciesを指定\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=20230515, stratify=X[\"species\"]\n)\n\n\npenguins_baked.groupby(\"species\").size()\n\nspecies\n0    146\n1     68\n2    119\ndtype: int64\n\n\n\nX_train.groupby(\"species\").size()\n\nspecies\n0    117\n1     54\n2     95\ndtype: int64\n\n\n\nX_test.groupby(\"species\").size()\n\nspecies\n0    29\n1    14\n2    24\ndtype: int64"
  },
  {
    "objectID": "week05/0501_regression.html#モデルの学習と予測",
    "href": "week05/0501_regression.html#モデルの学習と予測",
    "title": "6  線形回帰モデルによるペンギンの体重の予測",
    "section": "6.4 モデルの学習と予測",
    "text": "6.4 モデルの学習と予測\nScikit-learnでの線形回帰モデルの構築はsklearn.linear_modelモジュールのLinearRegressionクラスで行います。\n\n# インスタンスの生成\nlr = LinearRegression()\n# fitメソッドでモデルの学習を実行\n# 訓練データ（X_train, y_train）を引数に指定\nlr.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n# predictメソッドで予測を実行\n# 訓練データとテストデータをそれぞれを引数に指定し、予測値を得る\ny_pred_train = lr.predict(X_train)\ny_pred_test = lr.predict(X_test)\n\n# 予測値の確認\n# body_mass_gは標準化を適用していないため、もとのg単位での予測値となる\ny_pred_test\n\narray([4783.75905524, 5257.63304031, 4622.73374997, 5440.80373253,\n       3315.95001135, 3449.22324626, 4182.91106858, 3779.31387801,\n       5332.35879539, 5615.37691183, 4585.19536602, 4544.85274993,\n       3378.95494568, 4651.41753751, 3637.0685959 , 4669.99108179,\n       3941.43440199, 4058.0413563 , 3856.80769412, 3367.99787016,\n       4749.28198692, 5560.64069046, 4110.84101703, 4214.64059372,\n       3900.7371804 , 4580.99680682, 3912.27828029, 4710.12053689,\n       4220.79338127, 4133.35394733, 5469.58813153, 3886.77989401,\n       5408.29101883, 4445.499841  , 4016.26966443, 3832.29357079,\n       4726.61353707, 4085.79256734, 3591.65543457, 5495.44906674,\n       3255.90273903, 5472.53021551, 4842.32420278, 3380.23258003,\n       4197.5492215 , 4581.65490999, 4574.81831498, 3350.66554298,\n       4808.00360649, 3971.2999061 , 3597.35269633, 3601.3340526 ,\n       3132.39715024, 3644.53494959, 4144.78838762, 4184.99465485,\n       3948.11374387, 3508.36355321, 3265.37178874, 3950.86206815,\n       4745.25266621, 4098.12384673, 3737.99617275, 4627.01117925,\n       3558.17566828, 3949.27679296, 3392.07009348])\n\n\npredictメソッドにより、訓練データとテストデータそれぞれの予測値を得ることができました。次に、モデルの評価を行います。"
  },
  {
    "objectID": "week05/0501_regression.html#評価",
    "href": "week05/0501_regression.html#評価",
    "title": "6  線形回帰モデルによるペンギンの体重の予測",
    "section": "6.5 評価",
    "text": "6.5 評価\n回帰モデルの性能評価は実測値と予測値の間の誤差を用いた指標を利用します。主に使われる誤差の指標として次のものがあります。\n\n平均絶対誤差（Mean Absolute Error: MAE）: 実際の値と予測値との絶対値の差を平均したもの。MAEが小さいほど誤差は小さく、予測精度が高いと言える。一方で外れ値の影響を受けやすい。\n平均二乗誤差（Mean Squared Error: MSE）: 実際の値と予測値との差（予測誤差）を二乗して平均したもの。MSEが小さいほど誤差は小さく、予測精度が高いと言える。\n二乗平均平方根誤差（Root Mean Squared Error: RMSE）: MSEの平方根を求めたもの。MSEと同様、値が小さいほど誤差は小さく、予測精度が高いと言える。誤差のスケールが目的変数と同じスケールになるため、解釈が直感的になる。\n決定係数（R-squared, \\(R^2\\)）: モデルの当てはまりの良さを示す指標。1に近いほど予測精度が高いと言える。ただし、説明変数の数が増加し、モデルが複雑になると、\\(R^2\\)の値が大きくなりやすいという欠点がある。\n\nモデルや問題設定により、参照すべき適切な評価指標は異なります。例えば、MAEやMSEは外れ値の影響を受けやすいため、外れ値が多いデータではRMSEや決定係数を評価指標に用いるのが適切です。\n今回は、誤差の指標としてRMSEと決定係数を用います。RMSEは次の式で計算できます。\n\\[\nRMSE = \\sqrt{\\frac{1}{n}\\sum_{i=0}^{n-1} (y_{i} - \\hat{y}_{})^2}\n\\]\n\n\\(n\\): データの件数\n\\(y_{i}\\): \\(i\\)番目のデータの実測値\n\\(\\hat{y}_{i}\\): \\(i\\)番目のデータの予測値\n\nつまり、各観測値について実際の値と予測値の差を二乗し、その平均を取った後、平方根を求めることでRMSEが計算できます。\n決定係数0から1の値を取る指標です。1に近いほどモデルの予測が実測値に近い、つまりモデルの精度が高いことを示します。\n決定係数は次の式で計算されます。\n\\[\nR^2 = 1 - \\frac{RSS}{TSS}\n\\]\nここで \\(RSS\\) は残差平方和（Residual Sum of Squares）、\\(TSS\\) は全平方和（Total Sum of Squares）と呼ばれるもので、それぞれ次の式で計算できます。\n\\[\nRSS = \\sum_{i=1}^{n} (y_{i} - \\hat{y}_{i})^2\n\\]\n\\(RSS\\) は実際の値と予測値との差を2乗し、それらをすべて足し合わせたものです。これはモデルの予測値が実際の値からどの程度離れているかを示す指標となります。\n一方、\\(TSS\\) は予測値の代わりに実際の値の平均値 \\(\\bar{y}\\) を用います。モデルの予測値は関係なく、実際の値が全体の平均値からどの程度離れているかを示す指標となります。言い換えるとデータの全体的なバラつきを表すと考えることができます。\n決定係数に話しを戻しましょう。決定係数は \\(1-\\frac{RSS}{TSS}\\) で求められます。残差平方和（予測のズレ）が全平方和（データ全体のバラつき）に対してどれだけ小さいかを示す指標となります。これによりモデルの予測が実際のデータにどれだけ当てはまるかを評価することができます。\nこれらの指標は、scikit-learnのsklearn.metricsモジュールに含まれるmean_squared_error()関数とr2_score()関数を用いて計算することができます。\n\n# RMSEを求める\n# squared=Falseを指定することで、RMSEを計算することができる（デフォルトはMSE）\ntrain_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train, squared=False))\ntest_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test, squared=False))\n\n# 決定係数を求める\ntrain_r2 = r2_score(y_train, y_pred_train)\ntest_r2 = r2_score(y_test, y_pred_test)\n\n出力を表形式にして見やすくします。\n\npd.DataFrame(\n    data={\n        \"データの種類\": [\"訓練データ\", \"テストデータ\"],\n        \"RMSE\": [train_rmse, test_rmse],\n        \"R2\": [train_r2, test_r2],\n    }\n)\n\n\n\n\n\n\n\n\nデータの種類\nRMSE\nR2\n\n\n\n\n0\n訓練データ\n17.934423\n0.846080\n\n\n1\nテストデータ\n18.564218\n0.780487\n\n\n\n\n\n\n\n\n\n訓練データ、テストデータのRMSEはそれぞれ 17.934、18.564で、その差は0.63です。2つのデータセットの間での差はわずかで、訓練データとテストデータの両方でモデルの精度が同等であることを示しています。\n\n\nRMSEが訓練データとテストデータの両方で値が大きくなっている場合、モデルが訓練データに対して過学習していることを示しています。逆に、訓練データでのRMSEがテストデータのRMSEよりも極端に大きい場合、それはモデルが未学習である可能性を示します。\n\n\n\\(R^2\\)についても見てみましょう。訓練データ、テストデータそれぞれの値は0.846と0.78となりました。これはモデルが訓練データの分散の約0.846%を説明できているということを示します。同様にテストデータに対しても0.78%を説明することになります。\n\n\n\n# 実測値と予測値をプロット\nplt.scatter(y_test, y_pred_test, color=\"#284b67\")\nplt.xlabel(\"実際の値\")\nplt.ylabel(\"モデルの予測値\")\n\n# 実際の値と予測値が正確であれば、予測値は実測値と一致するため、\n# 対角線上（左下から右上への線）に点がプロットされる\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color=\"#fc5998\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n6.5.1 最小二乗法"
  },
  {
    "objectID": "week05/0502_svm.html#カーネルトリック",
    "href": "week05/0502_svm.html#カーネルトリック",
    "title": "7  サポートベクターマシン",
    "section": "7.1 カーネルトリック",
    "text": "7.1 カーネルトリック\nサポートベクターマシンが非線形の境界線を見つけられる背景には、カーネルトリックという特殊な機能を利用していることがあります。カーネルトリックとは、データを高次元空間に「投影」することで、直線で分けられない問題に対処する能力です。例えば、直線に分けられない2次元のデータを3次元に投影することで、直線で分けられるようになる場合があります。ただし、非線形変換によって高次元空間に投影されたデータに対して、元の特徴空間での直感的な理解とは一致しないことがある点は注意が必要です。\n\n7.1.1 カーネル関数とハイパーパラメータ\nカーネルトリックでは、データを高次元空間に投影する際にカーネル関数と呼ばれる関数を用います。カーネル関数は、2つのベクトル間の類似度を測るための関数であり、その計算は高次元空間での内積を計算することと等価です。サポートベクターマシンで利用されるカーネルトリックでは、以下のようなカーネル関数が用いられます。\n\n線形カーネル: ベクトルの内積を計算。これは基本的にはカーネルトリックを使用せずにSVMを適用するのと同等\n非線形カーネル\n\n多項式カーネル\nRBFカーネル（ガウシアンカーネル、Radial Basis Function kernel）\n\n\nRBFカーネルは、サポートベクターマシンによる非線形な境界線を用いた分類で広く利用されます。RBFカーネルは、以下のような式で表されます。\n\\[\nK(x, z) = exp(-\\gamma ||x - z||^2)\n\\]\n\n\\(x, z\\): 入力データ点。異なる2つのデータ点を表す。\n\\(||x - z||^2\\): \\(x\\) と \\(z\\) のユークリッド距離の二乗。\n\\(\\gamma\\): カーネルの形状を左右するハイパーパラメータ。正の実数として設定する。大きい値にすると、カーネルの幅が狭くなり、決定境界が複雑になる。小さい値にすると、カーネルの幅が広がるため、データ点が多く含まれるようになり、決定境界は単純になりやすい。\n\nこの関数では2つのデータ点が近いほど、カーネルの値が大きくなります。つまり、カーネル関数は2つのデータ点の類似度を表していると言えます。RBFカーネルは、2つのデータ点が近いほど、カーネルの値が大きくなるという性質を持ちます。そのため、データ点が密集している領域では、モデルはそれらの間で複雑な決定境界を形成しやすくなります。一方、データ点が疎になる領域では、決定境界は一般的に単純になります。\nサポートベクターマシンを実装する際、カーネル関数の種類とパラメータを指定する必要があります。具体的にはRBFカーネルでは、カーネルの形状を決定する \\(\\gamma\\) です。モデルが学習を行う前に、事前に決めておく必要がある、モデルの振る舞いを決定するパラメータをハイパーパラメータと呼びます。"
  },
  {
    "objectID": "week05/0502_svm.html#サポートベクターマシンに与えるデータ",
    "href": "week05/0502_svm.html#サポートベクターマシンに与えるデータ",
    "title": "7  サポートベクターマシン",
    "section": "7.2 サポートベクターマシンに与えるデータ",
    "text": "7.2 サポートベクターマシンに与えるデータ\nサポートベクターマシンでは入力データが欠損値を含まない状態に加えて、標準化されていることを想定します。それはサポートベクターマシンが、データの特徴量のスケールに敏感であるためです。具体的にはベクトルの内積を計算する際に、各特徴量の値が大きいほど、その特徴量の影響が大きくなるためです。そのため、サポートベクターマシンを適用する前に、データの標準化を行う必要があります。\n標準化とは、変数の値を平均0、標準偏差1に変換することです。標準化により、変数の値のスケールを揃えることができます1。"
  },
  {
    "objectID": "week05/0502_svm.html#scikit-learnでのサポートベクターマシンの実装",
    "href": "week05/0502_svm.html#scikit-learnでのサポートベクターマシンの実装",
    "title": "7  サポートベクターマシン",
    "section": "7.3 Scikit-learnでのサポートベクターマシンの実装",
    "text": "7.3 Scikit-learnでのサポートベクターマシンの実装\nScikit-learnでは、SVCクラスを用いてサポートベクターマシンを実装することができます。SVCクラスは、さきに述べたように、線形分離可能なデータに対してはロジスティック回帰モデルと同様に機能します。また、RBFなどのカーネル関数を指定することで、非線形な境界線を描くことも可能です。\nここではロジスティック回帰モデルと同様に、ペンギンのデータセットを用いて、ペンギンの種名を分類するモデルをサポートベクターマシンで実装してみましょう。\n\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold, cross_validate, cross_val_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report\n\n\nimport seaborn as sns\n# データの読み込み\npenguins = sns.load_dataset(\"penguins\")\n\n\n7.3.1 前処理・特徴量エンジニアリング\n前処理・特徴量エンジニアリングの工程を関数化しておきます。\n\ndef penguins_svm_preprocess(data):\n  from sklearn.preprocessing import LabelEncoder, StandardScaler\n  \n  # 元のデータを保持するためデータをコピー\n  data_processed = data.copy()\n  \n  # 前処理: 欠損値を含む行を削除\n  data_processed.dropna(inplace=True)\n  \n  # 特徴量エンジニアリング: ラベルラベルエンコーディング\n  le = LabelEncoder()\n  for col in [\"species\", \"island\", \"sex\"]:\n        data_processed[col] = le.fit_transform(data_processed[col])\n        \n  # 特徴量エンジニアリング: 標準化\n  scaler = StandardScaler()\n  data_processed[\n        [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n    ] = scaler.fit_transform(\n        data_processed[[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]]\n    )\n    \n  return data_processed\n\n定義した関数にペンギンデータを適用します。\n\npenguins_mod = penguins_svm_preprocess(penguins)\n\npenguins_mod.head(n = 3)\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\n0\n2\n-0.896042\n0.780732\n-1.426752\n-0.568475\n1\n\n\n1\n0\n2\n-0.822788\n0.119584\n-1.069474\n-0.506286\n0\n\n\n2\n0\n2\n-0.676280\n0.424729\n-0.426373\n-1.190361\n0\n\n\n\n\n\n\n\n\npd.DataFrame({\n  \"mean\": penguins_mod.mean(),\n  \"standard_deviation\": penguins_mod.std(),\n})\n\n\n\n\n\n\n\n\nmean\nstandard_deviation\n\n\n\n\nspecies\n9.189189e-01\n0.889718\n\n\nisland\n6.516517e-01\n0.714715\n\n\nbill_length_mm\n3.840772e-16\n1.001505\n\n\nbill_depth_mm\n6.401286e-16\n1.001505\n\n\nflipper_length_mm\n2.133762e-16\n1.001505\n\n\nbody_mass_g\n-1.707010e-16\n1.001505\n\n\nsex\n5.045045e-01\n0.500732\n\n\n\n\n\n\n\n\n\n7.3.2 データ分割\n続いて、モデルで予測したい目的変数と、モデルの学習に用いる説明変数を分離します。\n\n# 説明変数と目的変数の分離\nX = penguins_mod.drop(columns=\"species\")\ny = penguins_mod[\"species\"]\n\n今回のモデルでは、単純な訓練データとテストデータの分割によるホールドアウト法ではなく、交差検証法を適用してモデルの評価を行いましょう。交差検証法では、データをいくつかのセット（ホールド）に分割し、それぞれのグループをテストデータとして用いることで、モデルの汎化性能を評価します。\n\\(k\\) 分割交差検証は、データを \\(k\\) 個のグループに分割し、\\(k\\) 回の学習と評価を行う方法です。\\(k\\) 回の学習のうち、\\(k-1\\) 個のグループを訓練データとして用い、残りの1つのグループをテストデータとして用います。このようにして得られた \\(k\\) 回の評価の平均値を、モデルの評価値とします。\n\n\n\nK分割交差検証法\n\n\nデータに偏りがある場合、単純にデータをランダムに分割すると、各グループのデータに偏りが生じてしまう可能性があります。交差検証法の過程においても、分割後の訓練データとテストデータの間でデータの偏りが生じてしまうと、モデルが過学習を起こす心配があります。そのため、各グループのデータの割合が同じになるように分割する層化抽出法を利用します。層化抽出法では、データを分割する際に、各クラスの割合が同じになるように分割します。\n今回のデータでは、目的変数のクラスの割合が同じになるようにデータを分割する必要があります。そのため、層化 \\(K\\) 分割交差検証を行うために、sklearn.model_selectionモジュールのStratifiedKFoldを用いたデータ分割を行います。\n\n# 交差検証法の例: 層化 k 分割交差検証法\n# n_splitで分割数（ホールド数）を指定\n# shuffleでデータをシャッフルするか指定（元のデータの並び順を保持する場合はFalse）\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=20230515)\n\n\n# 各ホールドにおけるテストデータの種名の割合を確認\nfor train_index, test_index in skf.split(X, y):\n    print(y.iloc[test_index].value_counts() / len(y.iloc[test_index]))\n\nspecies\n0    0.447761\n2    0.358209\n1    0.194030\nName: count, dtype: float64\nspecies\n0    0.432836\n2    0.358209\n1    0.208955\nName: count, dtype: float64\nspecies\n0    0.432836\n2    0.358209\n1    0.208955\nName: count, dtype: float64\nspecies\n0    0.439394\n2    0.348485\n1    0.212121\nName: count, dtype: float64\nspecies\n0    0.439394\n2    0.363636\n1    0.196970\nName: count, dtype: float64\n\n\n各ホールドでのテストデータの種名の割合がほぼ同じになっていることが確認できました。\n\n\n7.3.3 モデルの学習と評価\nsklearn.svmのSVC()関数を用いて、サポートベクターマシンの分類モデルを構築します。今回は、カーネル関数にRBFカーネルを利用します。SVC()関数にはいくつかの引数があり、カーネル関数の種類やハイパーパラメータの指定が可能です。\nSVC()関数の引数・ハイパーパラメータは、以下の通りです。\n\nC… ソフトマージンのためのハイパーパラメータ。マージンの大きさを調整する。\nkernel… カーネル関数の種類。デフォルトで利用されるRBFカーネル(rbf)のほか、線形カーネル(linear)、多項式カーネル(poly)などがある。\ngamma… カーネル関数のパラメータ。RBFカーネルのみ使用。カーネルの幅を調整する。\n\n\n# サポートベクターマシンモデルの構築\n# カーネル関数はRBFカーネルを使用（デフォルト）\nsvm = SVC(kernel = 'rbf', gamma='scale')\n\n交差検証法を用いてモデルの評価を行うための関数としてcross_val_score()が利用できます。この関数は、モデルとデータを引数として受け取り、交差検証法による評価値を返します。評価指標にはデフォルトではモデルの精度（accuracy）が用いられますが、scoring引数を用いて、他の評価指標を用いることもできます。\n\nscores = cross_val_score(svm, X, y, cv=skf)\n# 各ホールドにおける正解率を平均\nprint(\"Average score: \", scores.mean())\n\nAverage score:  0.9939846223428311\n\n\n\n\nこの出力は、各ホールドにおける評価値の平均値を表しています。今回のモデルでは、平均で約0.994の正解率が得られました。\n\n\n\n# 再現率 (recall)を評価指標として用いる\nscores = cross_val_score(svm, X, y, cv=skf, scoring='recall_macro')\nprint(\"Average score: \", scores.mean())\n\nAverage score:  0.9901098901098901\n\n\ncross_validate()関数を用いると、複数の評価指標を同時に用いることができます。この関数はcross_val_score()関数と同じく、モデルとデータを引数として与えると、交差検証法による評価値を返却します。一方でscoring引数に複数の評価指標を指定でき、同時に複数の評価指標を比較可能です。加えて、return_train_score引数をTrueにすることで、テストデータのほかに訓練データに対する評価値も得ることができます。\n\n# 複数の評価指標（正解率、再現率）を用いる\ncross_validate(svm, X, y, cv=skf, scoring=['accuracy', 'recall_macro'], return_train_score=True)\n\n{'fit_time': array([0.00319529, 0.00324345, 0.00317788, 0.00320292, 0.00318241]),\n 'score_time': array([0.00564337, 0.00407696, 0.00403714, 0.00404596, 0.00404024]),\n 'test_accuracy': array([0.98507463, 1.        , 1.        , 0.98484848, 1.        ]),\n 'train_accuracy': array([0.9924812 , 0.9924812 , 0.9924812 , 0.99250936, 0.99625468]),\n 'test_recall_macro': array([0.97435897, 1.        , 1.        , 0.97619048, 1.        ]),\n 'train_recall_macro': array([0.98787879, 0.98765432, 0.98765432, 0.98765432, 0.99393939])}"
  },
  {
    "objectID": "week05/0502_svm.html#footnotes",
    "href": "week05/0502_svm.html#footnotes",
    "title": "7  サポートベクターマシン",
    "section": "",
    "text": "線形回帰モデルでの特徴量エンジニアリングを参考↩︎"
  },
  {
    "objectID": "week05/0503_k-means.html#クラスタリング",
    "href": "week05/0503_k-means.html#クラスタリング",
    "title": "8  k平均法によるクラスタリング",
    "section": "8.1 クラスタリング",
    "text": "8.1 クラスタリング\nクラスタリングは、類似するデータを類似度や距離などの指標を用いていくつかのグループに分割する手法です。クラスタリングでは、分けられたグループのことをクラスタまたはカテゴリと呼びます。\nクラスタリングは顧客を購買履歴などから類似する顧客に分類する顧客セグメンテーション、類似の内容を含む文書を分類するテキストマイニングや、類似の画像を分類する画像認識など、様々な分野で利用されています。また、正常なデータと異常なデータを分類する異常検知にも利用されます。\nクラスタリングはデータを自動的に分類する\n各クラスタの名前づけは人間が行う\nクラスタリングは機械学習の分野では、データがどのグループ（例えば動物の種名や分類群）に属するかという情報を与えず、それ以外の情報を用いて学習することから教師なし学習の一つとして扱われます。クラスタリングの手法はいくつかありますが、ここではk平均法を紹介します。"
  },
  {
    "objectID": "week05/0503_k-means.html#k平均法-k-means法",
    "href": "week05/0503_k-means.html#k平均法-k-means法",
    "title": "8  k平均法によるクラスタリング",
    "section": "8.2 k平均法 (k-means法)",
    "text": "8.2 k平均法 (k-means法)\nk平均法は、データを任意のk個のクラスタに分割する手法です。k平均法は、以下の手順でクラスタリングを行います。\n\nデータをk個のクラスタにランダムに分割する\n各クラスタの重心を計算する\n各データを最も近い重心を持つクラスタに再分割する\n2, 3を繰り返す\nクラスタの再分割が行われなくなったら終了\n\nk平均法を用いてペンギンデータをクラスタリングしてみましょう。\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n\n# データの読み込み\npenguins = sns.load_dataset(\"penguins\")\n\n# データの確認\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n\n\n\n\n\nデータ上にはspecies列が存在するので、ペンギンの身体の部位の大きさや体重、生息地の情報から\n\n# データの前処理\n# 欠損値を含む行の削除\npenguins = penguins.dropna()\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\npenguins[\"species\"] = le.fit_transform(penguins[\"species\"])\n\n# データの分割\nX = penguins.drop(columns=[\"species\", \"island\", \"sex\"])  # speciesを含め、数値を含まない列を削除\ny = penguins[\"species\"]\n\n# データの標準化\nscaler = StandardScaler()\nX_std = scaler.fit_transform(X)\n\n\n# k平均法の実行\nkmeans = KMeans(n_clusters=3, random_state=0)\nkmeans.fit(X_std)\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\nKMeans(n_clusters=3, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeansKMeans(n_clusters=3, random_state=0)\n\n\nkの数はいくつにするのがよい？\nエルボー法は古い？\n\n8.2.1 結果の確認と比較\nデータがどのように分類されたか、元のデータと比較して確認してみましょう。\n\n# クラスタリング結果の確認\n# クラスタリング結果のラベル\nlabels = kmeans.labels_\nlabels\n\narray([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0,\n       2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2,\n       2, 0, 2, 0, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2,\n       2, 0, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 0, 2, 0,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0,\n       2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1], dtype=int32)\n\n\n\n(labels==np.array(y)).sum() / len(labels)\n\n0.06606606606606606\n\n\n\n# Xにラベルを追加\nX[\"cluster\"] = np.where(labels == 1, 2, np.where(labels == 2, 1, labels))\n\n# クラスタリング結果のプロット\n# bill_length vs bill_depth\nsns.scatterplot(data=X, x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"cluster\")\nplt.show()\n\nsns.scatterplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\")\nplt.show()\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n\n\n\n\n\n\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n\n\n\n\n\n\n\n不一致のデータを確認してみましょう。\n\n# 不一致のデータを確認\n# sns.scatterplotに追加\nsns.scatterplot(data=X[labels == np.array(y)], x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"cluster\")\n# 不一致のデータを追加\nsns.scatterplot(data=X[labels != np.array(y)], x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"cluster\")\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n&lt;Axes: xlabel='bill_length_mm', ylabel='bill_depth_mm'&gt;"
  },
  {
    "objectID": "week06/index.html#この章の参考資料url",
    "href": "week06/index.html#この章の参考資料url",
    "title": "機械学習モデルの解釈・説明性",
    "section": "この章の参考資料・URL",
    "text": "この章の参考資料・URL\n\n(森下光之助 2021)\n(高柳慎一, 長田怜士, と ホクソエム 2023)\n\n\n\n\n\n森下光之助. 2021. 機械学習を解釈する技術 = Techniques for Interpreting Machine learning : 予測力と説明力を両立する実践テクニック. 技術評論社. http://id.ndl.go.jp/bib/031576666.\n\n\n高柳慎一, 長田怜士, と ホクソエム. 2023. 評価指標入門 = Introduction to Evaluation Metrics : データサイエンスとビジネスをつなぐ架け橋. 技術評論社. http://id.ndl.go.jp/bib/032637452."
  },
  {
    "objectID": "week06/0601_interpretable_model.html#重回帰モデル",
    "href": "week06/0601_interpretable_model.html#重回帰モデル",
    "title": "9  回帰モデルの解釈",
    "section": "9.1 重回帰モデル",
    "text": "9.1 重回帰モデル\n単回帰モデルでは入力変数が1つだけでしたが、現実の問題では、入力変数が1つだけではなく、複数ある場合が多いです。例えば、ペンギンの体重を予測する場合、翼の長さだけでなく、体の長さや体の深さなどの情報も、予測したいモデルの入力として使うことができます。このように、入力変数が1つだけではなく、複数ある場合の回帰モデルを重回帰モデルと呼びます。重回帰モデルは、次の式で表されます。\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_k x_k + \\epsilon\n\\]\nここで、\\(y\\) は目的変数、\\(x_1, x_2, \\ldots, x_k\\) は説明変数、\\(\\beta_0, \\beta_1, \\ldots, \\beta_k\\) はパラメータ、\\(\\epsilon\\) は誤差を表します。\n\n\ndef coefficient_table(model, X_vars):\n    # モデルの係数を取得\n    coefs = model.coef_\n\n    # 特徴量の名前を取得\n    feature_names = X_vars\n\n    # データフレームにまとめる\n    df_coef = pd.DataFrame({\n        'Feature': feature_names,\n        'Coefficient': coefs\n    })\n    df_coef['Feature'] = df_coef['Feature'].str.split('__').str[-1]\n    df_coef[\"abs_coef\"] = np.abs(df_coef[\"Coefficient\"])\n    df_coef = df_coef.sort_values(\"abs_coef\", ascending=False).drop(columns=\"abs_coef\")\n    return df_coef\n\nペンギンデータの体重以外の変数を説明変数として、線形回帰モデルを学習させます。\n\n# カテゴリ変数についてワンホットエンコーディングを指定\ncolumn_trans = make_column_transformer((OneHotEncoder(), ['species', 'island', 'sex']), remainder='passthrough')\n\n# パイプラインを作成\n# 前処理・特徴量エンジニアリングとモデル（線形回帰モデル）を組み合わせる\npipe = make_pipeline(column_trans, lr)\n\n\n# モデルの学習\n# 体重以外の変数を説明変数として、線形回帰モデルを学習させる\npipe.fit(penguins.drop(columns=\"body_mass_g\"), penguins[\"body_mass_g\"])\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('onehotencoder',\n                                                  OneHotEncoder(),\n                                                  ['species', 'island',\n                                                   'sex'])])),\n                ('linearregression', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('onehotencoder',\n                                                  OneHotEncoder(),\n                                                  ['species', 'island',\n                                                   'sex'])])),\n                ('linearregression', LinearRegression())])columntransformer: ColumnTransformerColumnTransformer(remainder='passthrough',\n                  transformers=[('onehotencoder', OneHotEncoder(),\n                                 ['species', 'island', 'sex'])])onehotencoder['species', 'island', 'sex']OneHotEncoderOneHotEncoder()remainder['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm']passthroughpassthroughLinearRegressionLinearRegression()\n\n\n重回帰モデルでも係数を元にモデルの予測値がどのように計算されたかを確認することができます。具体的には予測に使われたデータに対して係数をかけて足し合わせることで予測値を計算します。\n\n# 1行目のデータを取得\nx = penguins.drop(columns=\"body_mass_g\").iloc[[0]]\nx\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.10000\n18.70000\n181.00000\nMale\n\n\n\n\n\n\n\n\n# 予測値を計算\npipe.predict(x)\n\narray([3753.16389596])\n\n\n\ntransformed_data = pipe.named_steps[\"columntransformer\"].transform(penguins.drop(columns=\"body_mass_g\"))\n# データフレームに変換\ndf_transformed = pd.DataFrame(transformed_data, columns=pipe.named_steps['columntransformer'].get_feature_names_out())\n\n# 1行目のデータの予測値を計算\npipe.named_steps['linearregression'].intercept_ + np.dot(df_transformed.iloc[[0]], pipe.named_steps['linearregression'].coef_)\n\narray([3753.16389596])\n\n\n単回帰モデル同様、切片と係数を利用してモデルの予測値を得ることができました。\n重回帰モデルでの係数の値は、説明変数の重要度を表します。係数の絶対値が大きいほど、その説明変数が目的変数の値の予測に対して重要であることを表します。係数の値が正のときには、説明変数が増加すると目的変数の値も増加することを表します。一方、係数の値が負のときには、説明変数が増加すると目的変数の値は減少することを表します。\nこのことを踏まえてペンギンの体重を予測する重回帰モデルの係数を確認すると、次のことがわかります。\n\n# 係数を確認\ndf_coef = coefficient_table(\n  pipe.named_steps['linearregression'],\n  pipe.named_steps['columntransformer'].get_feature_names_out())\ndf_coef\n\n\n\n\n\n\n\n\nFeature\nCoefficient\n\n\n\n\n2\nspecies_Gentoo\n745.27639\n\n\n1\nspecies_Chinstrap\n-502.79134\n\n\n0\nspecies_Adelie\n-242.48505\n\n\n6\nsex_Female\n-193.61213\n\n\n7\nsex_Male\n193.61213\n\n\n9\nbill_depth_mm\n67.57543\n\n\n5\nisland_Torgersen\n-27.67473\n\n\n3\nisland_Biscoe\n20.38889\n\n\n8\nbill_length_mm\n18.18932\n\n\n10\nflipper_length_mm\n16.23851\n\n\n4\nisland_Dream\n7.28584\n\n\n\n\n\n\n\nまず、係数の絶対値の大きなペンギンの種類の違いが体重に影響を与えてます。ジェンツーペンギンでは係数が正ですが、他の2種は係数が負の値です。これはジェンツーペンギン、ヒゲペンギン、アデリーペンギンの順に体重が大きく、ジェンツーペンギンはアデリーペンギンよりも体重が約1000g大きいことがわかります。この傾向はグラフからも読み取ることができます。\n\n# 箱ひげ図を描画\nsns.boxplot(data=penguins, x=\"species\", y=\"body_mass_g\")\nplt.title(\"体重の箱ひげ図\")\nplt.suptitle(\"箱の中心の線が中央値を示す\")\nplt.show()\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n\n\n\n\n\n\n\nまた、種類に続いて性別の違いも体重に与える影響も大きくなっています。係数の値は、性別が雄の場合は雌の場合よりも体重が約500g大きいことを意味します。\n\n# オスとメスそれぞれの体重の平均値を計算\n# ペンギンの雄の体重の平均値\npenguins.groupby(\"sex\")[\"body_mass_g\"].mean()\n\nsex\nFemale   3862.27273\nMale     4545.68452\nName: body_mass_g, dtype: float64\n\n\nその他、数値変数の中ではくちばしの太さ bill_depth_mm の係数が大きくなっています。bill_depth_mmの係数は67.5となっています。これはくちばしの太さが1mm大きくなると体重が67.5g増加することを意味します。ただし、この値は他の変数が固定されている前提で計算された値です。\nbill_length_mm、flipper_lenth_mmについては、係数の絶対値が小さくなっています。そのため、これらの変数が体重に与える影響は、くちばしの太さよりも小さいということでしょうか。こうした変数間での係数の絶対値を比較することはできません。なぜなら、係数の絶対値は説明変数の値の範囲に依存するためです。これは、説明変数の値の範囲が広い変数では、その値が一単位変化したときの目的変数の変化量が相対的に小さくなるために係数が小さくなってしまうことがあるためです。これは特に異なる単位の変数を同時に利用する場合に問題となります。\n具体例を見てみましょう。bill_length_mm、bill_depth_mm、flipper_length_mmの値の範囲を確認すると、flipper_length_mmの値の範囲が最も大きいことがわかります。このことからflipper_length_mmの値の範囲が広いために、その値が一単位変化したときの目的変数の変化量が相対的に小さくなることを意味します。\n\n# 変数の範囲を確認\n# くちばしの長さ\nfor col in [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\"]:\n    print(col, penguins[col].min(), \"から\", penguins[col].max())\n\nbill_length_mm 32.1 から 59.6\nbill_depth_mm 13.1 から 21.5\nflipper_length_mm 172.0 から 231.0\n\n\n数値変数の値の範囲を揃えるために、数値変数を標準化してからモデルを学習させることを考えます。これにより、係数の大きさを比較できるようになります。\n\n9.1.1 標準化後の係数の解釈\n数値変数に対して、平均0、標準偏差1となるような標準化を行います。これにより、数値変数の値の範囲を揃えることができ、変数間での比較を行えるようになります。\n\n# 特定の列にOneHotEncoder()を適用\none_hot_cols = ['species', 'island', 'sex']\n# 特定の列にStandardScaler()を適用\nscale_cols = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm']\n\n# make_column_transformer()を使用してトランスフォーマを作成\ncolumn_trans = make_column_transformer(\n    (OneHotEncoder(), one_hot_cols),\n    (StandardScaler(), scale_cols),\n    remainder='passthrough'\n)\n\n# パイプラインを作成\n#pipe = Pipeline([\n#    ('preprocessor', column_trans),\n#    ('model', lr)\n#])\n\npipe = make_pipeline(column_trans, lr)\n\n\n# モデルの学習\npipe.fit(penguins.drop(columns=\"body_mass_g\"), penguins[\"body_mass_g\"])\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('onehotencoder',\n                                                  OneHotEncoder(),\n                                                  ['species', 'island', 'sex']),\n                                                 ('standardscaler',\n                                                  StandardScaler(),\n                                                  ['bill_length_mm',\n                                                   'bill_depth_mm',\n                                                   'flipper_length_mm'])])),\n                ('linearregression', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('onehotencoder',\n                                                  OneHotEncoder(),\n                                                  ['species', 'island', 'sex']),\n                                                 ('standardscaler',\n                                                  StandardScaler(),\n                                                  ['bill_length_mm',\n                                                   'bill_depth_mm',\n                                                   'flipper_length_mm'])])),\n                ('linearregression', LinearRegression())])columntransformer: ColumnTransformerColumnTransformer(remainder='passthrough',\n                  transformers=[('onehotencoder', OneHotEncoder(),\n                                 ['species', 'island', 'sex']),\n                                ('standardscaler', StandardScaler(),\n                                 ['bill_length_mm', 'bill_depth_mm',\n                                  'flipper_length_mm'])])onehotencoder['species', 'island', 'sex']OneHotEncoderOneHotEncoder()standardscaler['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm']StandardScalerStandardScaler()remainder[]passthroughpassthroughLinearRegressionLinearRegression()\n\n\n\n# 特徴量と係数の対応を確認\ndf_coef_std = coefficient_table(\n  pipe.named_steps['linearregression'],\n  pipe.named_steps['columntransformer'].get_feature_names_out())\n\ndf_coef_std[\"abs_coef\"] = np.abs(df_coef_std[\"Coefficient\"])\n\n# 元が数値変数だった特徴量の係数を取得\ndf_coef_std.query('Feature in @scale_cols').sort_values(\"abs_coef\", ascending=False).drop(columns=\"abs_coef\")\n\n\n\n\n\n\n\n\nFeature\nCoefficient\n\n\n\n\n10\nflipper_length_mm\n221.29551\n\n\n9\nbill_depth_mm\n134.46109\n\n\n8\nbill_length_mm\n103.11298\n\n\n\n\n\n\n\n係数の値が大きいのはflipper_length_mm、bill_depth_mm、bill_length_mmの順番です。このことは3種の変数の中で体重に影響を与えるのはflipper_length_mmが最も大きいということです。\n散布図や相関係数からも、flipper_length_mmが体重に影響を与えることがわかります。\n\n# 3つの図を並べて描画\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n# 3つの数値変数と体重の散布図を描画\n# 図ごとに色を変える\nfor ax, col, color in zip(axes, scale_cols, [\"#284b67\", \"#1a2127\", \"#fc5998\"]):\n    ax.scatter(penguins[col], penguins[\"body_mass_g\"], color=color, alpha=0.7)\n    ax.set_title(col)\n    ax.set_xlabel(col)\n    ax.set_ylabel(\"body_mass_g\")\n    ax.set_title(f\"相関係数: {penguins[col].corr(penguins['body_mass_g']):.3f}\")\nplt.show()\n\n\n\n\n\n\n\n\n最後に、重回帰モデルでの決定係数を計算しましょう。重回帰モデルの決定係数は、単回帰モデルの決定係数と同様に、0から1の値をとり、1に近いほど目的変数を説明できていることを意味します。\n\n# 決定係数を計算\npipe.score(penguins.drop(columns=\"body_mass_g\"), penguins[\"body_mass_g\"])\n\n0.8745268098179141\n\n\nこの値は説明変数の標準化を行ったあとでのモデルの決定係数ですが、実は説明変数の標準化を行わない場合でも同じ値になります。これは、重回帰モデルの決定係数は、説明変数の標準化を行っても行わなくても同じ値になるためです。つまり、モデルの精度には標準化の有無は影響を与えません。"
  },
  {
    "objectID": "week06/0602_patial_dependence.html",
    "href": "week06/0602_patial_dependence.html",
    "title": "10  PDP",
    "section": "",
    "text": "線形回帰モデルや木構造を利用したモデル（決定木やランダムフォレストなど）は一般的にその結果の解釈が容易な機械学習モデルです。一方、モデルがより複雑になると、モデルが導き出した値がどのようにして得られたのか、その予測を説明することが難しくなります。例えば、非線形回帰モデルやニューラルネットワーク、勾配ブースティングモデルなどのモデルは、その予測を説明することが難しいため、ブラックボックスモデルと呼ばれています。\nモデルの精度の観点からは、ブラックボックスモデルの方が優れていることが多いです。しかし、モデルが予測したものの理由を説明することが困難であるために、モデルに対する信頼性が低下する可能性があります。つまり、一般的にはモデルの精度と解釈性はトレードオフの関係にあります。モデルの精度を維持しながら、モデルの解釈性を向上させることは、機械学習の研究や開発において重要な課題です。\n\n11 Partial Dependence Plots　（PDP）\npartial dependence plotを用いると特徴量とモデルが予測する目的変数との関係性を可視化することができます。入力の値の変化に対して、モデルがどのように反応するかを表すことができます。線形回帰モデルでは常に線形の関係性が得られますが、非線形回帰モデルでは非線形の関係性が得られることがあります。\n勾配ブースティングを例に、モデルの解釈性を向上させる方法を紹介します。勾配ブースティングは、複数の決定木を組み合わせて予測を行うアンサンブル学習の一種です。\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.inspection import PartialDependenceDisplay\n\n\n# ペンギンデータの読み込みと欠損値の削除\npenguins = sns.load_dataset(\"penguins\")\npenguins.dropna(inplace=True)\n\n\n# One-hotエンコーディングを適用する特徴量\ncategorical_features = ['species', 'island', 'sex']\n\n# データの前処理とモデル構築のパイプライン\npreprocessor = make_column_transformer(\n    (OneHotEncoder(), categorical_features),\n    remainder='passthrough'\n)\n\nmodel = make_pipeline(\n    preprocessor,\n    GradientBoostingRegressor()\n)\n\n\n# データの準備\nX = penguins.drop(columns=\"body_mass_g\")\ny = penguins[\"body_mass_g\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=20230522, stratify=X[\"species\"])\n\n\n# モデルの学習\nmodel.fit(X_train, y_train)\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('onehotencoder',\n                                                  OneHotEncoder(),\n                                                  ['species', 'island',\n                                                   'sex'])])),\n                ('gradientboostingregressor', GradientBoostingRegressor())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('onehotencoder',\n                                                  OneHotEncoder(),\n                                                  ['species', 'island',\n                                                   'sex'])])),\n                ('gradientboostingregressor', GradientBoostingRegressor())])columntransformer: ColumnTransformerColumnTransformer(remainder='passthrough',\n                  transformers=[('onehotencoder', OneHotEncoder(),\n                                 ['species', 'island', 'sex'])])onehotencoder['species', 'island', 'sex']OneHotEncoderOneHotEncoder()remainder['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm']passthroughpassthroughGradientBoostingRegressorGradientBoostingRegressor()\n\n\n\n# plot_partial_dependence\ndisplay = PartialDependenceDisplay.from_estimator(\n    model,\n    X_train,\n    features=['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm']\n)\n\n\n\n\n\n\n\n\n\n# plot\ndisplay.plot()\n\n&lt;sklearn.inspection._plot.partial_dependence.PartialDependenceDisplay at 0x7f65484f7a00&gt;\n\n\n\n\n\n\n\n\n\npartial dependence plotのx軸は、特徴量の値の範囲を表しています。x軸の下方に示されるラインはデータの分布を表しています。y軸は、その特徴量の値がモデルの予測に与える影響を表しています。この図から、例えば、flipper_length_mmが200のとき、モデルの予測は約4000gになります。\nペンギンの体重は体の各部位と関係があると考えられ、モデルでもbill_depth_mmやflipper_length_mmの増加が体重の増加に寄与していることを確認できます。一方、partial dependence plotからは、bill_length_mmが体重に寄与していないことを示しています。しかしこの結果は直感的な予測とは異なります。モデルがこのような解釈を行った理由については、学習データの不足やモデルの設計によるものと考えられます。"
  },
  {
    "objectID": "week07/index.html",
    "href": "week07/index.html",
    "title": "演習1",
    "section": "",
    "text": "電気使用実績を予測するモデルの構築\nAIを活用したツール\nVisual Studio Code"
  },
  {
    "objectID": "week07/07_exercise.html#データの概要",
    "href": "week07/07_exercise.html#データの概要",
    "title": "11  電気使用実績を予測するモデルの構築",
    "section": "11.1 データの概要",
    "text": "11.1 データの概要\n2016年から2018年までの夏期（7月から9月の3ヶ月）における四国エリアの使用状況に加えて、気象庁（観測所: 高松）による気象データを収録しています。\n\ndatetime: 日本標準時での時刻。1時間ごとの記録が行われている。\nusage_GW : 使用状況の実績。単位はGW(ギガワット)。1GBは1000MW。元データでは万kWで記録。\natmosphere_land_hPa: 現地気圧(hPa)\nprecipitation_mm: 降水量（mm）\ntemperature_dC: 気温（摂氏）\nhumidity_pct: 湿度（%）\n\n\ndf_train = pd.read_csv(p / \"week07/input/train.csv\", parse_dates=False)\ndf_test = pd.read_csv(p / \"week07/input/test.csv\", parse_dates=False)\n\n\ndf_train.head(n=5)\n\n\n\n\n\n\n\n\n\n\n\n\n\ndatetime\nusage_GW\natmosphere_land_hPa\nprecipitation_mm\ntemperature_dC\nhumidity_pct\n\n\n\n\n2016-06-30 15:00:00\n2.53\n1011.3\n0\n23.1\n96\n\n\n2016-06-30 16:00:00\n2.52\n1011.1\n0\n22.6\n95\n\n\n2016-06-30 17:00:00\n2.66\n1011.0\n0\n22.1\n95\n\n\n2016-06-30 18:00:00\n2.86\n1011.7\n0\n21.8\n96\n\n\n2016-06-30 19:00:00\n2.96\n1011.7\n0\n22.0\n95\n\n\n2016-06-30 20:00:00\n2.90\n1011.9\n0\n22.0\n93"
  },
  {
    "objectID": "week07/07_exercise.html#データの可視化",
    "href": "week07/07_exercise.html#データの可視化",
    "title": "11  電気使用実績を予測するモデルの構築",
    "section": "11.2 データの可視化",
    "text": "11.2 データの可視化\n\n# 時系列でのusage_GWの変化を確認\ndf_train[\"datetime\"] = pd.to_datetime(df_train[\"datetime\"])\n# タイムゾーンを日本時間に変換\ndf_train[\"datetime\"] = df_train[\"datetime\"].dt.tz_convert(\"Asia/Tokyo\")\n\ndf_train[\"year\"] = df_train[\"datetime\"].dt.year\n\ng = sns.FacetGrid(df_train, col=\"year\", sharex=False, col_wrap=1)\n# 折れ線グラフを描画\ng.map(sns.lineplot, \"datetime\", \"usage_GW\")\nplt.show()\n\nこれ以外にも、データへの理解を深めるための可視化を行ってみましょう。"
  },
  {
    "objectID": "week07/07_exercise.html#評価指標",
    "href": "week07/07_exercise.html#評価指標",
    "title": "11  電気使用実績を予測するモデルの構築",
    "section": "11.3 評価指標",
    "text": "11.3 評価指標\nモデルの精度を評価するための指標として、二乗平均平方根誤差（RMSE）を用います。\n参考) 線形回帰モデルによるペンギンの体重の予測"
  },
  {
    "objectID": "week07/07_exercise.html#ベースラインのモデル",
    "href": "week07/07_exercise.html#ベースラインのモデル",
    "title": "11  電気使用実績を予測するモデルの構築",
    "section": "11.4 ベースラインのモデル",
    "text": "11.4 ベースラインのモデル\n教師あり機械学習モデルは一般的に次の手順で構築されます。\n参考）第四回スライド\n\n\n\n教師あり学習の流れ\n\n\n\nデータ分割\nデータの前処理・特徴量エンジニアリング\nモデルの学習\nモデルの性能評価\n\nここでは機械学習モデルの構築例として、電気使用実績を予測するために気温と降水量を用いた重回帰モデルによるモデルを作成します。このモデルをベースラインとして、他のモデルと比較してみましょう。\n\n11.4.1 データ分割\n\n# 欠損値のある行を削除\ndf_train_baseline = df_train.dropna()\n\n# 説明変数と目的変数に分割\nX = df_train_baseline[[\"temperature_dC\", \"precipitation_mm\"]]\ny = df_train_baseline[\"usage_GW\"]\n\n# 訓練データとテストデータに分割\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=20230529\n)\n\n\n\n11.4.2 前処理・特徴量エンジニアリング\nここでは数値変数の標準化（平均0、分散1への変換）を行います。\nScikit-learnを用いた前処理とモデルの管理はPipelineクラスを用いると便利です。\n\n# 前処理とモデルのパイプラインを作成\n# 1/2 前処理を適用する変数と処理内容を指定\nnumeric_features = [\"temperature_dC\", \"precipitation_mm\"]\n# パイプラインの作成。stepsには(名前, インスタンス)のタプルを指定する\n# standardScalerは平均0、分散1に変換するインスタンス\nnumeric_transformer = Pipeline(steps=[(\"scaler\", StandardScaler())])\n\n# 2/2 前処理として、数値変数の標準化を指定\npreprocessor = numeric_transformer\n\n\n\n11.4.3 モデルの学習\n\nmodel = Pipeline(\n    steps=[(\"preprocessor\", preprocessor), (\"regressor\", LinearRegression())]\n)\n\nmodel.fit(X_train, y_train)\n\n\n\n11.4.4 モデルの評価\n\n# テストデータの予測値を計算\ny_pred = model.predict(X_test)\n# RMSEを計算\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(f\"RMSE: {rmse:.3f}\")"
  },
  {
    "objectID": "week07/07_exercise.html#モデルの改善例",
    "href": "week07/07_exercise.html#モデルの改善例",
    "title": "11  電気使用実績を予測するモデルの構築",
    "section": "11.5 モデルの改善例",
    "text": "11.5 モデルの改善例\n\n変数の追加\n交差検証法\nモデルの変更\n\n正則化\nランダムフォレスト\nXGBoost\nLightGBM\n\nハイパーパラメータの調整\n特徴量エンジニアリング… 特に電気使用に影響を与える日付や時間の情報を追加すると良いかもしれません。"
  },
  {
    "objectID": "week08/index.html#この章の参考資料url",
    "href": "week08/index.html#この章の参考資料url",
    "title": "深層学習の基礎",
    "section": "この章の参考資料・URL",
    "text": "この章の参考資料・URL\n\n(手塚太郎 2018)\n(今泉允聡 2021)\n(瀧雅人 2017)\n(斎藤康毅 2016)\n(岡谷貴之 2022)\n(柳井啓司, 中鹿亘, と 稲葉通将 2022)\n\n\n\n\n\n今泉允聡. 2021. 深層学習の原理に迫る : 数学の挑戦. 岩波科学ライブラリー ; 303. 岩波書店. http://id.ndl.go.jp/bib/031339770.\n\n\n岡谷貴之. 2022. 深層学習 = Deep Learning. 改訂第2版 版. 機械学習プロフェッショナルシリーズ. 講談社. http://id.ndl.go.jp/bib/031901202.\n\n\n手塚太郎. 2018. しくみがわかる深層学習 = An Introduction to Deep Learning. 朝倉書店. http://id.ndl.go.jp/bib/029043769.\n\n\n斎藤康毅. 2016. ゼロから作るDeep Learning : Pythonで学ぶディープラーニングの理論と実装. オライリー・ジャパン. http://id.ndl.go.jp/bib/027597005.\n\n\n柳井啓司, 中鹿亘, と 稲葉通将. 2022. 深層学習. IT Text. オーム社. http://id.ndl.go.jp/bib/032479806.\n\n\n瀧雅人. 2017. これならわかる深層学習入門 = Introduction to Deep Learning. 機械学習スタートアップシリーズ. 講談社. http://id.ndl.go.jp/bib/028568441."
  },
  {
    "objectID": "week08/0801_neural_networks.html#深層学習",
    "href": "week08/0801_neural_networks.html#深層学習",
    "title": "12  ニューラルネットワーク",
    "section": "12.1 深層学習",
    "text": "12.1 深層学習\n深層学習は人工ニューロンと呼ばれる、人の脳で行われている情報処理を模倣した情報処理モデルを多層に重ねたニューラルネットワークを用いて、データから重要な特徴量を抽出し、その特徴量を用いて分類や回帰を行う機械学習の手法です。\n深層学習は、画像認識や音声認識、自然言語処理などの分野で高い精度を出しており、近年では様々な分野で活用されています。人工知能の中でも、特に注目されている技術です。\n\n12.1.1 機械学習との違い\n深層学習は機械学習の一種ですが、機械学習とはどのような違いがあるのでしょうか。機械学習と深層学習の特徴・その違いを整理すると、以下のようになります。\n\n12.1.1.1 機械学習\n\nデータからパターンを見つけ出し、そのパターンを基に新しいデータに対する予測を行う。多くの場合、人間が手動で重要な情報（特徴量）を設定する必要がある。\nモデルの訓練時間は一般的に深層学習に比べて短い\n\n\n\n12.1.1.2 深層学習\n\n機械学習の一手法。\n特徴量の抽出を人間が行うのではなく機械が行う。そのため手動での特徴量設定が不要となる。\n多層の人工ニューロンを使用する。各層のニューロンは、前の層のニューロンからの入力を受け取り、その入力に対して重み付けを行う。各ニューロンの出力は、そのニューロンへの入力と重み付けの積の総和を「活性化関数」に入力した値となる。→ 柔軟な表現を可能にする\n機械学習と比べ、モデルの訓練には多くのデータと長い時間が必要となる。"
  },
  {
    "objectID": "week08/0801_neural_networks.html#神経細胞のネットワーク",
    "href": "week08/0801_neural_networks.html#神経細胞のネットワーク",
    "title": "12  ニューラルネットワーク",
    "section": "12.2 神経細胞のネットワーク",
    "text": "12.2 神経細胞のネットワーク\n深層学習で利用される人工ニューロンは、人間の脳の神経細胞を模倣したものです。人間の脳は、神経細胞と呼ばれる細胞が結びつき、神経細胞同士で電気信号を伝えるネットワークを形成しており、このネットワークをニューラルネットワークと呼びます。\n神経細胞のネットワークは、以下のような構造をしています。\n\n神経細胞体\n樹状突起\n軸索\n\n軸索末端と次のニューロンの樹状突起とが接触し、情報の伝達が行われます。この接触部位はシナプスと呼ばれます。また、電気信号の伝達を「発火」と呼びます。\n発火は、樹状突起からの入力が一定の値を超えたときに発生します。この一定の値を閾値いきち、しきいちと呼びます。閾値を超えたときに発生する電気信号の強さは一定であり、発火の有無のみが伝達されます。"
  },
  {
    "objectID": "week08/0801_neural_networks.html#人工ニューロン",
    "href": "week08/0801_neural_networks.html#人工ニューロン",
    "title": "12  ニューラルネットワーク",
    "section": "12.3 人工ニューロン",
    "text": "12.3 人工ニューロン\n人工ニューロンは、ニューラルネットワークを構成する基本単位としての、モデル化された神経細胞です。1つ以上の入力を受け取り、それらの入力に対して重み付けを行い、その重み付けの総和を活性化関数に入力した値を出力する、という構造をしています。\n\n入力\n重み\n活性化関数\n出力\n\n\n12.3.1 パーセプトロン\n人工ニューロンの一種に「パーセプトロン」と呼ばれるものがあります。パーセプトロンの挙動を理解することで、人工ニューロンの基本的な動作の理解に役立ちます。\nパーセプトロンは、構造や学習の違いによって、単純パーセプトロンと多層パーセプトロンに分けられます。まずは単純パーセプトロンについて説明します。\n\n12.3.1.1 単純パーセプトロン\n単純パーセプトロンは、入力層と出力層の2つの層から構成されています。入力層は、入力を受け取る層です。出力層では、入力層からの入力と重みの積の総和が閾値を超えた場合に1を出力し、超えなかった場合に0を出力する、という動作をします。このような動作を行う関数を「ステップ関数」と呼びます。パーセプトロンでは、ステップ関数のように、入力に対して出力を決定する関数を活性化関数と呼びます。\nステップ関数を活性化関数として用いた単純パーセプトロンは以下の式で表現されます。 \\[\ny = \\begin{cases}\n0 & (w_1x_1 + w_2x_2 \\leq \\theta) \\\\\n1 & (w_1x_1 + w_2x_2 &gt; \\theta)\n\\end{cases}\n\\]\nここで \\(x1\\) 、\\(x2\\) は入力、 \\(w1\\) 、\\(w2\\) は各入力に対する重み（weight）、 \\(\\theta\\) は閾値です。ここでは入力の数が2つの場合を考えていますが、入力の数は任意の値にすることができます。また、入力の重要度を重みによって調整することができます。そのため重みは入力ごとに用意されています。 \\(\\theta\\) は、重みと入力の積の総和が閾値を超えた場合に1を出力するかどうかを決定するための値です。すなわち、 \\(\\theta\\) が大きいほど、重みと入力の積の総和が大きくないと1を出力しなくなります。\n1か0かという二値の出力を用いて、以下の論理演算を行うことができます。このような論理演算を行うパーセプトロンを「論理回路」と呼びます。\n\nAND: 2つの入力が1の場合に1を出力し、それ以外の場合に0を出力する。\nOR: 2つの入力のうち1つ以上が1の場合に1を出力し、それ以外の場合に0を出力する。\nNAND: 2つの入力が1の場合に0を出力し、それ以外の場合に1を出力する。ANDの逆の動作をする。\n\n\n\n\n\\(x_1\\)\n\\(x_2\\)\nAND\nOR\nNAND\n\n\n\n\n0\n0\n0\n0\n1\n\n\n1\n0\n0\n1\n1\n\n\n0\n1\n0\n1\n1\n\n\n1\n1\n1\n1\n0\n\n\n\nこれらの論理演算は、パーセプトロンの重みと閾値を適切に設定することで実現できます。例えば、ANDの場合は、 \\(w_1\\) 、 \\(w_2\\) 、 \\(\\theta\\) をそれぞれ0.5、0.5、0.7とすると、上の表のような出力が得られます。\n\\[\ny = \\begin{cases}\n0 & (0.5x_1 + 0.5x_2 \\leq 0.7) \\\\\n1 & (0.5x_1 + 0.5x_2 &gt; 0.7)\n\\end{cases}\n\\]\n\nx1 = 0\nx2 = 0\n(0.5 * x1 + 0.5 * x2 &lt;= 0.7) & (0.5 * x1 + 0.5 * x2 &gt; 0.7)\n\nFalse\n\n\nPythonで上記のパーセプトロンを実装すると以下のようになります。\n\ndef perceptron(x1, x2, theta = 0.7):\n    w1, w2 = 0.5, 0.5\n    tmp = w1 * x1 + w2 * x2\n    if tmp &lt;= theta:\n        return 0\n    elif tmp &gt; theta:\n        return 1\n\nprint(perceptron(0, 0)) # 0\nprint(perceptron(1, 0)) # 0\nprint(perceptron(0, 1)) # 0\nprint(perceptron(1, 1)) # 1\n\n0\n0\n0\n1\n\n\nPythonで実装したパーセプトロンは、「入力と重みの積の総和が閾値を超えた場合に1を出力し、超えなかった場合に0を出力する」、という動作をしています。閾値は関数の引数として与えていましたが、この値を変えることで、パーセプトロンの出力を変えることができます。これにより、パーセプトロンをORパーセプトロンとして機能させることができます。\n\n# ORのパーセプトロン\nprint(perceptron(0, 0, theta = 0.2)) # 0\nprint(perceptron(1, 0, theta = 0.2)) # 1\nprint(perceptron(0, 1, theta = 0.2)) # 1\nprint(perceptron(1, 1, theta = 0.2)) # 1\n\n0\n1\n1\n1\n\n\nNANDのパーセプトロンは、ANDのパーセプトロンの重みと閾値を反転させることで実現できます。\n\n# NANDのパーセプトロン\ndef NAND_perceptron(x1, x2, theta = -0.7):\n    # 重みと閾値を反転\n    w1, w2 = -0.5, -0.5\n    tmp = w1 * x1 + w2 * x2\n    if tmp &lt;= theta:\n        return 0\n    elif tmp &gt; theta:\n        return 1\n\nprint(NAND_perceptron(0, 0)) # 1\nprint(NAND_perceptron(1, 0)) # 1\nprint(NAND_perceptron(0, 1)) # 1\nprint(NAND_perceptron(1, 1)) # 0\n\n1\n1\n1\n0\n\n\nこれまでのパーセプトロンの例では、閾値を関数の引数として与えていましたが、閾値をバイアスとして扱い、重みと入力の積の総和に含めることができます。\n\\[\ny = \\begin{cases}\n0 & (b + w_1x_1 + w_2x_2 \\leq 0) \\\\\n1 & (b + w_1x_1 + w_2x_2 &gt; 0)\n\\end{cases}\n\\]\nここで \\(b\\) はバイアスです。この場合、閾値はバイアスの重みとなります。バイアスは、入力が0の場合に出力する値を決めるパラメータとして機能します。すなわちバイアスが大きいほど、入力が0の場合に出力する値は大きくなります。\n\n# バイアスを導入したANDのパーセプトロン\ndef AND_perceptron(x1, x2, b = -0.7):\n    w1, w2 = 0.5, 0.5\n    # b はバイアス\n    tmp = b + w1 * x1 + w2 * x2\n    if tmp &lt;= 0:\n        return 0\n    elif tmp &gt; 0:\n        return 1\n\nprint(AND_perceptron(0, 0)) # 0\nprint(AND_perceptron(1, 0)) # 0\nprint(AND_perceptron(0, 1)) # 0\nprint(AND_perceptron(1, 1)) # 1\n\n\nprint(\"バイアスの値を小さくする\")\n# バイアスの値が小さいと発火しやすくなる\nprint(AND_perceptron(0, 0, b = -0.2)) # 0\nprint(AND_perceptron(1, 0, b = -0.2)) # 1\nprint(AND_perceptron(0, 1, b = -0.2)) # 1\nprint(AND_perceptron(1, 1, b = -0.2)) # 1\n\n# バイアスの値が大きいと発火しにくくなる\nprint(\"バイアスの値を大きくする\")\nprint(AND_perceptron(0, 0, b = -20.0))\nprint(AND_perceptron(1, 1, b = -20.0))\n\n0\n0\n0\n1\nバイアスの値を小さくする\n0\n1\n1\n1\nバイアスの値を大きくする\n0\n0\n\n\n同様に、ORやNANDのパーセプトロンにもバイアスを導入することができます。\nパーセプトロンを用いることで、ANDやOR、NANDの論理回路を表現することができました。一方で、2つの入力が異なる場合に1を出力し、同じ場合に0を出力する論理回路であるXORは単純パーセプトロンでは表現できません。\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(y\\)\n\n\n\n\n0\n0\n0\n\n\n1\n0\n1\n\n\n0\n1\n1\n\n\n1\n1\n0\n\n\n\n単純パーセプトロンがXORを表現できない理由は、単純パーセプトロンは線形分離可能な問題しか表現できないからです。線形分離可能な問題とは、2つのクラスを直線で分離できる問題のことです。XORは、2つのクラスを直線で分離するものではないため、単純パーセプトロンでは表現できない、ということになります。\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n# XORの入出力と線形分離不可能なことを確認する\n# XORの入力\nX = np.array([[0, 0], [1, 0], [0, 1], [1, 1]])\n# XORの出力\nY = np.array([0, 1, 1, 0])\n\n# データをプロットする\nplt.scatter(X[:, 0], X[:, 1], c = Y)\n# 直線の追加... これでは分離できない\nplt.plot([0, 1], [1, 0])\nplt.show()\n\n\n\n\n\n\n\n\n一方で、多層パーセプトロンを用いることで、XORを表現することができます。\n\n\n\n12.3.1.2 多層パーセプトロン\n多層パーセプトロンは、複数のパーセプトロンを組み合わせることで表現することができます。これにより、単純パーセプトロンでは表現できない非線形な問題を表現することができます。\nバイアスを導入したANDパーセプトロンとORパーセプトロンを組み合わせることで、XORを表現することができます。これらの単純パーセプトロンを組み合わせて、XORを表現する多層パーセプトロンを作成してみましょう。\n\n# ANDパーセプトロン\ndef AND_perceptron(x1, x2, b = -0.7):\n    w1, w2 = 0.5, 0.5\n    # b はバイアス\n    tmp = b + w1 * x1 + w2 * x2\n    if tmp &lt;= 0:\n        return 0\n    elif tmp &gt; 0:\n        return 1\n\n\n# ORパーセプトロン\ndef OR_perceptron(x1, x2, b = -0.2):\n    w1, w2 = 0.5, 0.5\n    # b はバイアス\n    tmp = b + w1 * x1 + w2 * x2\n    if tmp &lt;= 0:\n        return 0\n    elif tmp &gt; 0:\n        return 1\n\n\n# NANDパーセプトロン\ndef NAND_perceptron(x1, x2, b = 0.7):\n    w1, w2 = -0.5, -0.5\n    # b はバイアス\n    tmp = b + w1 * x1 + w2 * x2\n    if tmp &lt;= 0:\n        return 0\n    elif tmp &gt; 0:\n        return 1\n\n\n# XORパーセプトロン\n# XORはAND, OR, NANDを組み合わせて表現できる\ndef XOR_perceptron(x1, x2):\n    # 第0層(入力層)... 2つの入力を受け取る最初の層\n    s1 = NAND_perceptron(x1, x2)\n    # 第1層(中間層)... NANDパーセプトロンの出力を入力として受け取る\n    s2 = OR_perceptron(x1, x2)\n    # 第2層(出力層)... ORパーセプトロンとNANDパーセプトロンの出力を入力として受け取り、ANDパーセプトロンの出力を返す\n    y = AND_perceptron(s1, s2)\n    return y\n\n# XORパーセプトロンの出力\nfor x1, x2 in X:\n    print(x1, x2, XOR_perceptron(x1, x2))\n\n0 0 0\n1 0 1\n0 1 1\n1 1 0\n\n\n上記の多重パーセプトロンは3種の単純パーセプトロンを組み合わせて表現しています。このように、多層パーセプトロンは複数のパーセプトロンを組み合わせて表現することができます。これにより、多層パーセプトロンは非線形な問題を表現することができます。言い換えると、層を重ねることで、柔軟な表現が可能になったということです。"
  },
  {
    "objectID": "week08/0801_neural_networks.html#ニューラルネットワーク",
    "href": "week08/0801_neural_networks.html#ニューラルネットワーク",
    "title": "12  ニューラルネットワーク",
    "section": "12.4 ニューラルネットワーク",
    "text": "12.4 ニューラルネットワーク\n多層パーセプトロンをベースに、ニューラルネットワークの概念を紹介します。ニューラルネットワークは多層パーセプトロンと同じく、複数のパーセプトロンを組み合わせて表現することができます。しかし、ニューラルネットワークでは、パーセプトロンの重みとバイアスを自動で調整することができます。これらのパラメータを調整することを「学習」と呼んでいます。\n\n12.4.1 ニューラルネットワークの構造\nニューラルネットワークは、入力層、中間層、出力層の3つの層で構成されます。入力層は、外部からの入力を受け取る層です。中間層は、入力層からの入力を受け取り、出力層への入力を行う層です。出力層は、中間層からの入力を受け取り、外部への出力を行う層です。これは、多層パーセプトロンと同じ構造です。多層パーセプトロンと異なる点は、中間層が複数存在することです。このように、中間層が複数存在するニューラルネットワークは「層が深い」と表現され、ディープニューラルネットワークと呼ばれる所以になっています。\nしかし、層の数を増やすだけでは、ニューラルネットワークの表現力は向上しません。ニューラルネットワークの表現力を向上させるには、活性化関数を工夫する必要があります。活性化関数には、ステップ関数以外にも、シグモイド関数やReLU関数などがあります。これらについて詳しく見ていきましょう。\n\n\n12.4.2 活性化関数\n「活性化」とは入力信号の総和がどのように発火するか（出力が1か0か）を決定することを指します。活性化関数はその名の通り、入力信号の総和がどのように活性化するかを決定する関数です。活性化関数には、ステップ関数、シグモイド関数、ReLU関数などがあります。\n\nステップ関数: 入力信号の総和が閾値を超えたら発火する関数\nシグモイド関数: 入力信号の総和を0から1の間（連続的な実数）に収める関数\n正規化線形関数（ReLU関数）: 入力信号の総和が0を超えたら発火する関数。出力は0から無限大の間の値を取る。\nソフトマックス関数: 入力信号の各要素を0から1の間の連続的な実数に変換し、その出力の総和が1になるように正規化する。これにより、出力は確率分布として解釈することが可能になる。\n\n入力と出力の関係をグラフで表すと、以下のようになります。\n\n# ステップ関数\ndef step_function(x):\n    return np.array(x &gt; 0, dtype=np.int)\n\n# シグモイド関数\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# ReLU関数\ndef relu(x):\n    return np.maximum(0, x)\n\n\n# 入力値\nx = np.arange(-5.0, 5.0, 0.1)\n\n# ステップ関数の出力\ny_step = step_function(x)\n\n# シグモイド関数の出力\ny_sigmoid = sigmoid(x)\n\n# ReLU関数の出力\ny_relu = relu(x)\n\n# グラフの描画\nplt.plot(x, y_step, label='step')\nplt.plot(x, y_sigmoid, label='sigmoid')\nplt.plot(x, y_relu, label='ReLU')\nplt.ylim(-0.1, 1.1)\nplt.legend()\nplt.show()\n\n/tmp/ipykernel_2594/993146812.py:3: DeprecationWarning:\n\n`np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n\n\n\n\n\n\n\n\n\n\n活性化関数の主な目的は、モデルに非線形性を導入し、より複雑な関数を近似する能力を提供することです。ニューラルネットワークでは、活性化関数として上記のシグモイド関数やReLU関数を用いることが多いです。ステップ関数は、ニューラルネットワークではあまり用いられません。\nニューラルネットワークにおいて、各中間層のニューロンは、その入力に対して重みを適用し、バイアスを追加した後、活性化関数を適用します。活性化関数は出力層でも使用されることがありますが、その選択は解くべき問題に依存します。たとえば、二項分類問題ではシグモイド関数が、多クラス分類問題ではソフトマックス関数が出力層の活性化関数としてよく使用されます。\n\n\n12.4.3 ニューラルネットワークの実装\nニューラルネットワークの実装は、多層パーセプトロンの実装とほぼ同じです。違いは、活性化関数をシグモイド関数やReLU関数に変更することです。また、中間層が複数存在することも違いの一つです。\n\n# シグモイド関数\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# 恒等関数... 入力をそのまま出力する関数\ndef identity_function(x):\n    return x\n\n# 3層ニューラルネットワーク\n# 重みとバイアスを初期化する\ndef init_network():\n    network = {}\n    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n    network['b1'] = np.array([0.1, 0.2, 0.3])\n    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n    network['b2'] = np.array([0.1, 0.2])\n    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])\n    network['b3'] = np.array([0.1, 0.2])\n\n    return network\n\n#　入力信号を出力へ変換する処理\ndef forward(network, x):\n    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n\n    a1 = np.dot(x, W1) + b1\n    z1 = sigmoid(a1)\n    a2 = np.dot(z1, W2) + b2\n    z2 = sigmoid(a2)\n    a3 = np.dot(z2, W3) + b3\n    # 出力層の活性化関数は、恒等関数を用いる\n    y = identity_function(a3)\n\n    return y\n\nnetwork = init_network()\nx = np.array([1.0, 0.5])\ny = forward(network, x)\nprint(y) # [0.31682708 0.69627909]\n\n[0.31682708 0.69627909]\n\n\n扱う問題が回帰か分類かによって、出力層の活性化関数は変更します。回帰問題では、恒等関数を、分類問題では、ソフトマックス関数を用います。ソフトマックス関数は、出力層のニューロンの数だけの出力を持ち、その出力の総和が1になるように正規化します。そのため、ソフトマックス関数の出力を確率として解釈することができます。"
  },
  {
    "objectID": "week08/0801_neural_networks.html#学習の進め方を決めるハイパーパラメータ",
    "href": "week08/0801_neural_networks.html#学習の進め方を決めるハイパーパラメータ",
    "title": "12  ニューラルネットワーク",
    "section": "12.5 学習の進め方を決めるハイパーパラメータ",
    "text": "12.5 学習の進め方を決めるハイパーパラメータ\nニューラルネットワークの学習では、ハイパーパラメータと呼ばれるパラメータが存在します。これは重みやバイアスのように、学習の過程で調整されるパラメータではなく、あらかじめ設定が必要なパラメータです。ハイパーパラメータには、エポック数、バッチサイズ、学習率などがあります。\nハイパーパラメータは以下に示すように、ニューラルネットワークの学習の進め方を決定する要因となります。そのため、ハイパーパラメータの値を適切に設定することは、ニューラルネットワークの学習を成功させるために重要です。\n\n12.5.1 エポック\nニューラルネットワークの学習では、訓練データを複数回繰り返し用いて学習を行います。このとき、訓練データをすべて用いて1回学習を行うことを1エポックと呼びます。例えば1万件の訓練データに対してバッチサイズを100とした場合、すべての訓練データを用いて学習するには100回の繰り返しが必要です。このとき、100回の繰り返しが1エポックとなります。\nモデルが訓練データに対して学習した回数をエポック数と呼びます。エポック数が大きいほど、モデルは訓練データに対してより多く学習することになります。エポックの数は、学習の反復回数を制御するためのハイパーパラメータとなります。そのためエポック数が小さいと、学習が十分に進まず、モデルの性能が低くなります。一方、エポック数が大きすぎると過学習しやすくなります。\n\n\n12.5.2 バッチサイズ\nニューラルネットワークの学習、すなわちパラメータの更新は通常、訓練データ全体を一度に処理するのではなく、小さな「ミニバッチ」に分割して行われます。ミニバッチは訓練データセットからランダムに選ばれた一部のデータの集まりで、ミニバッチの大きさをバッチサイズと呼びます。このようなミニバッチを用いた学習を「ミニバッチ学習」と呼びます。\nバッチサイズを大きく取る場合、1つのミニバッチを使った学習は、訓練データ全体を使った学習に近くなります。そのため、バッチサイズを大きく取ると、ミニバッチ学習の効率が良くなります。一方で学習時間が長くなり、メモリの使用量も増えます。\n\n\n12.5.3 学習率\n学習率とは、パラメータの更新量を決定するハイパーパラメータです。通常、0より大きく1以下の実数であり、その値は実験や経験に基づいて決定されます。\n学習率が大きい場合は、大きくパラメータを更新することができます。学習率が大きい場合は、最適なパラメータを早く見つけることができますが、最適なパラメータの探索が不安定になり、最適なパラメータを見過ごしてしまう可能性があります。このことを「発散」と呼びます。一方、学習率が小さすぎると、パラメータの更新が非常にゆっくりとしか進まず、適切なパラメータを見つけるのに必要な計算時間が非常に長くなるか、または最適な解に全く到達しない可能性があります。\n\n\n12.5.4 イテレーション\nイテレーションとは、パラメータの更新を行う回数を表します。イテレーションは、エポック数、バッチサイズ、学習率から計算することができます。たとえば、エポック数が10、バッチサイズが100、学習率が0.01の場合、イテレーション数は10,000となります。"
  },
  {
    "objectID": "week08/0801_neural_networks.html#最適化問題",
    "href": "week08/0801_neural_networks.html#最適化問題",
    "title": "12  ニューラルネットワーク",
    "section": "12.6 最適化問題",
    "text": "12.6 最適化問題\n最適化問題は、ある関数の最小値または最大値を見つける問題です。この関数は通常「目的関数」または「損失関数」と呼ばれます。最適化問題では、目的関数の最小値または最大値を求めるために、パラメータを調整します。このパラメータの調整を「最適化」と呼びます。また具体的な最適化の手法を「最適化アルゴリズム」と呼びます。\nたとえば、線形回帰モデルでは、最適化アルゴリズムとして最小二乗法を用いて、モデルのパラメータ（傾きと切片）を調整します。最小二乗法は損失関数として平均二乗誤差を用います。これはモデルの予測値と真の値との間の誤差を表します。この損失関数を最小化するようにモデルのパラメータを調整することで、モデルの予測値と真の値との誤差を小さくすることができます。\n最小二乗法は線形回帰問題の解法として広く知られていますが、線形代数の手法を用いた一度の計算が必要となり、パラメータの数が増えると計算量が膨大になり現実的でなくなる可能性があります。そのためニューラルネットワークでは一般的に、勾配降下法という最適化アルゴリズムを用いて、パラメータを調整します。\n\n12.6.1 勾配降下法によるパラメータの学習\n勾配とは、ある地点における関数の傾きのことです。勾配が正の場合は、関数の値が増加していることを意味し、勾配が負の場合は、関数の値が減少していることを意味します。勾配が0の場合は、関数の値が極値に達していることを意味します。つまり、勾配が0となるように、勾配を小さくする方向にパラメータを調整することで、損失関数の値を小さくすることができます。これを繰り返すことで、損失関数の値を小さくすることができます。このような手法を勾配降下法と呼びます。この方法は損失関数の勾配（つまり、パラメータを微小量変化させたときの損失関数の変化量）を計算し、その勾配が示す方向にパラメータを少しずつ更新していくものです。\n\n\n12.6.2 確率的勾配降下法\n勾配降下法では全ての訓練データを用いて、勾配を求めます。そのため、学習の順番（訓練データの与えられ方）によっては、最適なパラメータを得ることができない場合があります。また、訓練データが膨大な場合は、全ての訓練データを用いて勾配を求めることは現実的ではありません。そのため、訓練データの中からランダムに選んだ一部のデータ（ミニバッチと呼ばれる）を用いて、勾配を求める方法があります。この方法を確率的勾配降下法と呼びます。\n確率的勾配降下法では、一部のデータを用いて勾配計算を行うため、計算コストを削減することができます。また、ランダムに選ばれたデータを用いて勾配計算を行うため、最適なパラメータを得ることができない可能性があります。しかし、ランダムに選ばれたデータを用いて勾配計算を行うことで、局所的な最適解（局所解）に陥ることを防ぐことができます。また、確率的勾配降下法では複数のミニバッチを利用しますが、これらのミニバッチを並列して計算することで、計算コストを削減することができます。これにはGPUを用いることで、高速に並列計算を行うことが期待されます。\n勾配降下法は山下り（あるいは山登り）に例えられます。山の斜面を下るとき、最も急な斜面を下る方向に進むと、最も早く山を下ることができます。現在地から最も急な斜面を下る方向に進むことができるだけ進み、再び最も急な斜面を下る方向に進みます。これを繰り返すことで、最も急な斜面を下る方向に進むことができます。確率的勾配降下法では、進む方向にランダム性があるため、最も急な斜面を下る方向に進むことができない可能性があります。そのため、ジグザグとした動きで山を下ることになります。\n\n\n12.6.3 誤差逆伝播法\nニューラルネットワークでの重みパラメータに対する損失関数の勾配を効率的に求める方法を誤差逆伝播法（バックプロパゲーション）と呼びます。誤差逆伝播法により、パラメータ数が非常に多いモデルでも、各パラメータに対する損失関数の勾配を効率的に計算することが可能となります。\n誤差逆伝播法は次のような手順で行われます。まず、現在の重みとバイアスを元に入力データから出力を行い、損失関数の勾配を求める過程を「順伝播」と表現します。誤差逆伝播法はその名の通り、出力層から入力層へ向かって逆向きに「誤差」を伝播します。つまり、出力層から入力層へ向かって、各層の重みとバイアスの勾配を偏微分を用いて求めることができます。ここで得られた勾配を用いて、重みとバイアスのパラメータを更新します。具体的には、パラメータをその勾配方向に小さなステップ移動させて、損失関数の値を少しずつ減らしていきます。\n\n\n12.6.4 勾配消失問題\n誤差逆伝播法では、出力層から入力層へ向かって誤差を伝播させます。そのため、誤差逆伝播法では、出力層に近い層の重みパラメータの勾配が正しく計算されることが期待されます。一方、出力層から遠い層の重みパラメータの勾配は、誤差逆伝播法では正しく計算されない可能性があります。具体的には勾配が非常に小さくなり、0になってしまうことがあります。そのため、重みパラメータが更新されず、学習が進まなくなることがあります。これを勾配消失問題と呼びます。\n勾配消失問題は特に、活性化関数にシグモイド関数を用いる場合に発生します。シグモイド関数は、層を重ねるごとに勾配が小さくなり、0に近づいていきます。そのため、層を重ねることで、勾配が消失してしまうことがあります。そこで、層を重ねても勾配が消失しにくい活性化関数として、ReLU関数やその派生であるLeaky ReLU関数が用いられます。また、重みの初期値を適切に設定する、学習率を小さくすることで、勾配消失問題を回避することができます。"
  },
  {
    "objectID": "week08/0801_neural_networks.html#ディープニューラルネットワークのアーキテクチャ",
    "href": "week08/0801_neural_networks.html#ディープニューラルネットワークのアーキテクチャ",
    "title": "12  ニューラルネットワーク",
    "section": "12.7 ディープニューラルネットワークのアーキテクチャ",
    "text": "12.7 ディープニューラルネットワークのアーキテクチャ\nディープニューラルネットワークは、多層パーセプトロンをベースに、様々な構造を持つことができます。これらの構造を持つことで、ディープニューラルネットワークは、画像認識や自然言語処理などの分野で高い精度を出すことができます。ここでは、ディープニューラルネットワークの構造について紹介します。\n\n12.7.1 全結合層\n全結合層は、ニューラルネットワークの中で最も基本的な層です。全結合層は、前の層のすべてのニューロンが次の層のすべてのニューロンと結合しています。そのため、全結合層は、前の層のすべてのニューロンからの入力を受け取り、次の層のすべてのニューロンへ出力します。全結合層は、入力層と出力層の間に複数の中間層を持つことができます。\n\n\n12.7.2 畳み込みニューラルネットワーク\n畳み込みニューラルネットワーク（Convolutional Neural Network: CNN）は、画像認識の分野で高い精度を出すニューラルネットワークの一種です。画像の特徴を抽出する畳み込み層とプーリング層から構成されます。\n\n畳み込み層: 画像の特徴を抽出する層。入力データに対してフィルター演算を行い、その出力を次の層へ出力する。\nプーリング層: 畳み込み層の出力を圧縮する層。入力データの一部に対して、最大値や平均値を計算し、その出力を次の層へ出力する。\n\nフィルター演算あるいは畳み込み演算とは、入力データの一部に対してフィルターと呼ばれる小さな行列を適用し、フィルターと入力データの対応する要素の積和を計算する演算です。フィルターは、入力データよりも小さく、入力データの一部に対して適用されます。そのため、フィルター演算の出力は、入力データの一部に対応します。フィルター演算の出力を「特徴マップ」と呼びます。畳み込み層では、複数のフィルターを用いることができます。このとき、畳み込み層の出力は、複数の特徴マップから構成されます。\n畳み込みニューラルネットワークを用いた画像認識では、畳み込み層とプーリング層を複数組み合わせて、画像の特徴を抽出します。その後、全結合層を用いて、抽出した特徴を元に画像の分類を行います。\n\n\n\n\n\n\nヒント\n\n\n\nTensorflow公式によるCNNの実装: https://www.tensorflow.org/tutorials/images/cnn?hl=ja\n\n\n\n\n12.7.3 再帰型ニューラルネットワーク\nCNNは入力データの長さ（次元）がすべて統一されていなくてはいけません。そのため、音声や文章といった、隣り合う要素の関連性が強いデータを扱うのに不向きです。これらのデータの特徴は、そのデータの中に、重要な情報がどこにあるかが明確に定まっていないことです。また、与えられるデータの長さも一定ではありません。実際、文章のデータは単語数によって長さが異なります。このような可変長のデータは系列データと呼ばれます。\n再帰型ニューラルネットワーク（recurrent neural network: RNN）は、系列的なデータを扱うことができるニューラルネットワークの一種です。RNNは、系列的なデータの中に、重要な情報がどこにあるかを学習することができます。\nRNNは、長・短期記憶、ゲート付き再帰ユニットといった様々な構造を持つことができます。これらの構造を持つことで、RNNは時系列データの中に、重要な情報がどこにあるかを自動的に学習することができます。\n\n長・短期記憶　(LSTM):\nゲート付き再帰ユニット　(GRU):"
  },
  {
    "objectID": "week08/0802_mnist.html#mnist",
    "href": "week08/0802_mnist.html#mnist",
    "title": "13  ニューラルネットワークによる手書き文字の分類",
    "section": "13.1 MNIST",
    "text": "13.1 MNIST\n手書きによる文字（数字）の画像データセット\n\n28 \\(\\times\\) 28ピクセルのグレースケール画像\n0から9までの数字の画像\n60,000枚の訓練用画像と10,000枚のテスト用画像"
  },
  {
    "objectID": "week08/0802_mnist.html#tensorflow",
    "href": "week08/0802_mnist.html#tensorflow",
    "title": "13  ニューラルネットワークによる手書き文字の分類",
    "section": "13.2 TensorFlow",
    "text": "13.2 TensorFlow\nTensorFlowは、Googleが開発したオープンソースの機械学習ライブラリです。TensorFlow独自の低レベルAPIを使って、ニューラルネットワークを自由に設計可能なのが特徴です。\nしかし、ニューラルネットワークの構造の設計は、初心者にとってはハードルが高いです。そこで、TensorFlowの高レベルAPIであるKerasを使うことで、TensorFlowの機能を簡単に利用することができます。KerasとはTensorFlowの他にTheano、CNTKなどの上で動作する深層学習フレームワークの一種です。TensorFlow上でKerasの高レベルAPIを利用するには、具体的にはtf.kerasを用います。\n\n13.2.1 ライブラリのインポート\n\nimport tensorflow as tf\n\n\n\n13.2.2 データの読み込み\ndatasetsモジュールのmnist.load_data()関数を使ってMNISTデータセットを読み込みます。\n\nmnist = tf.keras.datasets.mnist\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n\n\n13.2.3 データの確認\nX_trainは訓練用の画像データ、y_trainは学習用のラベルデータとなっています。shepe属性を使ってデータの形状を確認しましょう。\n\n# 訓練データの件数と画像サイズの確認\nprint(x_train.shape)\n\n# テストデータの件数と画像サイズの確認\nprint(x_test.shape)\n\n\n# 訓練データの件数とラベルの確認\nprint(y_train.shape)\n\n画像データは28 \\(\\times\\) 28ピクセルのグレースケール画像です。画像データは0から255までの整数値で表されています。訓練データに格納された画像データを表示してみましょう。\n\nx_train[0]\n\n「画像」として表示するためには、matplotlibモジュールのimshow()関数を使います。\n\nimport matplotlib.pyplot as plt\n\nplt.imshow(x_train[0], cmap='gray')\nplt.show()\n\nこの数字が何を表しているのかは、y_trainに格納されているラベルを確認することでわかります。\n\ny_train[0]\n\nグレースケールで表された画像データは、0から255までの整数値で表されています。ニューラルネットワークの入力として扱うためには、各ピクセルの値を0から1の範囲にスケーリングして正規化しておくと都合が良いです。画像データを正規化するには、255で割るだけです。\n\nx_train = x_train / 255.0\nx_test = x_test / 255.0\n\n\n# 正規化した画像データの確認\nx_train[0]\n\nラベルをワンホットエンコーディングしておきます。これにより、出力層の10個のノードのうち、正解ラベルに対応するノードの値が1、それ以外のノードの値が0となります。\nラベルをワンホットエンコーディングを行うことで、クラス分類問題の出力層において、各クラスの確率を表現しやすくなります。また、損失関数や評価指標の計算にも利用されます。\n\n# ワンホットエンコーディング\ny_train = tf.keras.utils.to_categorical(y_train, 10)\ny_test = tf.keras.utils.to_categorical(y_test, 10)\n\n\n# 最初の画像は5であるため、6番目のノードの値が1となっている（ラベルは0からはじまる）\ny_train[0]\n\n\n\n13.2.4 モデルの構築\nそれでは、ニューラルネットワークのモデルを構築していきましょう。今回は、入力層に784個のノード（28 \\(\\times\\) 28ピクセルを入力）、出力層に10個のノードを持つニューラルネットワークを構築します。入力層には、28 \\(\\times\\) 28ピクセルの画像データを1次元のベクトルに変換して入力します。出力層の10個のノードは、それぞれ0から9までの数字を表します。出力層の10個のノードのうち、最も値が大きいノードがニューラルネットワークの予測結果となります。\n\n28 \\(\\times\\) 28の画像データを1次元のベクトルに変換\n10個のノードを持つ出力層に入力\n活性化関数にソフトマックス関数を指定することで、出力を確率として解釈できるように変換する。\n\n\nmodel = tf.keras.models.Sequential([\n  # 入力層には784個(28*28の画像サイズに対応)のノードを持つ\n  # 28*28の画像データを1次元のベクトルに変換して入力\n  # 1次元のベクトルに変化するため、入力層にはFlatten()を指定する\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\n  # 全結合層には10個のノードを持つ\n  # 活性化関数にはソフトマックス関数を指定する\n  # これにより、出力を確率として解釈できるようになる\n  tf.keras.layers.Dense(10, activation='softmax')\n])\n\nモデルのアーキテクチャを確認してみましょう。モデルが扱う層の数や、各層のノード数、パラメータ数などを確認することができます。\n\nmodel.summary()\n\n\n\n13.2.5 ハイパーパラメータの設定\nハイパーパラメータとは、モデルの学習において、人が設定するパラメータのことです。ハイパーパラメータの設定によって、モデルの学習結果が大きく変わることがあります。ハイパーパラメータの設定には、経験則に基づいたものが多くあります。ハイパーパラメータの設定には、試行錯誤が必要です。\n\noptimizer：最適化アルゴリズム。学習率を調整するアルゴリズムを指定します。確率的勾配降下法（SGD）を指定する場合は、optimizer='sgd'と指定します。確率的勾配降下法のほかにも、自動的に学習率を調整するAdam、Adagrad、RMSpropなどのアルゴリズムを指定することができます。\nloss：損失関数\nmetrics：評価指標\nbatch_size：バッチサイズ\nepochs：エポック数\nlearning_rate： 学習率\n\nモデルの構築が完了したら、compile()メソッドを使ってモデルをコンパイルします。compile()メソッドの引数には、最適化アルゴリズム、損失関数、評価指標を指定します。\n\nmodel.compile(\n    # 学習率を指定\n    optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),\n    # 損失関数の指定\n    loss='categorical_crossentropy',\n    # 評価指標の指定\n    metrics=['accuracy']\n)\n\n5エポックで学習を行います。これは、訓練データを5回繰り返し学習させることを意味します。\n\nmodel.fit(x_train, y_train, epochs=5)\n\nテストデータを使って、モデルの評価を行います。ここで計算される損失関数はcomplie()メソッドで指定した損失関数（クロスエントロピー損失関数）です。\n\nmodel.evaluate(x_test, y_test, verbose=2)\n\npredict()メソッドを使って、テストデータの予測結果を確認します。この予測結果は、各ラベルに対する確率を表しています。もっとも確率の高いラベルが予測結果となります。\n\n# テストデータの最初の画像の予測結果\nmodel.predict(x_test[:1])\n\n# 最も確率の高いラベル\nimport numpy as np\nprint(np.argmax(model.predict(x_test[:1]), axis=-1))\n\n\n# テストデータの最初の画像の正解ラベル\ny_test[:1]"
  },
  {
    "objectID": "week09/index.html#この章の参考資料url",
    "href": "week09/index.html#この章の参考資料url",
    "title": "実社会での応用",
    "section": "この章の参考資料・URL",
    "text": "この章の参考資料・URL\n\n(AI白書編集委員会 2022)\n(Tunstall, Leandro von Werra, と 中山光樹 2022)\n(岡野原大輔 2022)\n(岡﨑直観 ほか 2022)\n(柳井啓司, 中鹿亘, と 稲葉通将 2022)\n(M.クーケルバーク と 直江清隆 2020)\n\n\n\n\n\nAI白書編集委員会. 2022. AI白書2022. KADOKAWA.\n\n\nM.クーケルバーク, と 直江清隆. 2020. AIの倫理学. 丸善出版. http://id.ndl.go.jp/bib/031185559.\n\n\nTunstall, Lewis, Thomas Wolf Leandro von Werra, と 中山光樹. 2022. 機械学習エンジニアのためのTransformers : 最先端の自然言語処理ライブラリによるモデル開発. オライリー・ジャパン. http://id.ndl.go.jp/bib/032288109.\n\n\n岡野原大輔. 2022. AI技術の最前線 : これからのAIを読み解く先端技術73. 日経BP. http://id.ndl.go.jp/bib/032288114.\n\n\n岡﨑直観, 荒瀬由紀, 鈴木潤, 鶴岡慶雅, と 宮尾祐介. 2022. 自然言語処理の基礎. IT Text. オーム社. http://id.ndl.go.jp/bib/032310940.\n\n\n柳井啓司, 中鹿亘, と 稲葉通将. 2022. 深層学習. IT Text. オーム社. http://id.ndl.go.jp/bib/032479806."
  },
  {
    "objectID": "week09/09_application.html#深層学習が得意とする分野",
    "href": "week09/09_application.html#深層学習が得意とする分野",
    "title": "14  実社会での応用",
    "section": "14.1 深層学習が得意とする分野",
    "text": "14.1 深層学習が得意とする分野\n深層学習が得意とする分野は、従来の機械学習手法が得意とする分野とは異なります。深層学習は、高次元かつ複雑なデータに対しても効果的に学習できることが特徴です。これには、画像や動画、音声、自然言語など、実社会で頻繁に出現するデータが含まれます。深層学習の能力を活用することで、画像認識、音声認識、自然言語処理、機械翻訳など、多岐にわたる応用が可能となり、現代社会において不可欠な技術となっています。\n深層学習が複雑なデータの表現を学習できる理由の一つとして、深層学習がデータの特徴を自動的に抽出する能力があります。例えば、画像のデータを扱う場合、従来の機械学習手法では、画像の輝度やエッジなどの情報を特徴量として抽出する必要がありました。しかし、どのような特徴量を抽出すれば良いかは、データの分布によって異なります。データの分布に依存性がある場合、特徴量の抽出には専門家の知識が必要となることが多かったです。\n一方、深層学習では、データの特徴を自動的に抽出することが可能です。深層学習モデルは、複数の隠れ層から構成され、隠れ層のニューロンがデータの特徴を学習します。階層的な特徴表現を獲得することができるため、より複雑なデータの表現や異なるレベルの抽象化が可能となります。特徴抽出は、モデル自体が学習の一部として行われるため、事前の特徴量エンジニアリングが不要となります。\n深層学習では、例えば、画像データでは各ピクセルが特徴として扱われ、画像全体の情報を抽出します。また、動画データではフレームごとの情報や時間的なパターンを学習し、音声データでは音響特徴を解析します。さらに、自然言語処理では単語や文の意味を表現し、文脈を理解するためのモデル構築にも利用されます。\n深層学習が得意とする分野は、以下の通りです。\n\n画像認識: 画像の分類、物体検出、セグメンテーション\n音声認識: 音声の認識、音声の合成\n自然言語処理: 単語の分散表現、文の生成、文脈の理解\n\nこれらの分野で扱うデータはいずれも、高次元かつ複雑なデータです。このようなデータは、データが得られる前からある程度、どのようなデータ構造になるかが分かる構造化データに対して、非構造化データとして呼ばれます。非構造化データは、データの特徴を抽出することが難しく、従来の機械学習手法では、データの特徴を表現することが困難でした。しかし、深層学習では、データの特徴を自動的に抽出することが可能となり、非構造化データに対しても効果的に学習できるようになったことが、現在のAI社会を支えています。\n\n14.1.1 画像認識\n画像認識は、画像の特徴を抽出し、その特徴を元に画像の分類や物体検出、セグメンテーションなどのタスクを行います。\n\n画像の分類: 画像に写っているものが何かを判別する\n物体検出: 画像に写っている物体の位置とクラス（またはラベル）を同時に推定する\nセグメンテーション: 画像の各ピクセルに対して、そのピクセルが属するクラスを推定する\n\n画像認識は社会の様々な場面で利用されています。例えば、画像の分類では、画像検索や画像の自動整理、画像の自動キャプション生成などに利用されます。また、セグメンテーションでは、医療画像の解析や自動運転などに利用されています。\n\n\n14.1.2 音声認識\n音声認識は、音声の特徴を抽出し、その特徴を元に音声の認識や音声の合成などのタスクを行います。自動読み上げや音声検索などに利用されています。\n\n\n14.1.3 自然言語処理\n自然言語処理は、自然言語、すなわち人間が日常的に使う言語をコンピュータで処理する技術です。自然言語処理は、単語や文の意味を表現し、文脈を理解するためのモデル構築に利用されます。自然言語処理は、機械翻訳や文章要約、チャットボットなどに利用されています。"
  },
  {
    "objectID": "week09/09_application.html#生成系ai",
    "href": "week09/09_application.html#生成系ai",
    "title": "14  実社会での応用",
    "section": "14.2 生成系AI",
    "text": "14.2 生成系AI\n新しいデータや情報を作り出す能力を持ったAIのことを指します。\nこれは一般的には、学習データからパターンやルールを抽出し、それを基にして新しい内容を生成します。\n学習には膨大な量のデータが必要となり、大規模言語モデルと呼ばれる\n生成系AIは、様々な領域で応用されています。例えば、画像生成では、特定の人物の写真を生成したり、絵画やイラストを生成したりします。また、自然言語処理では、新しい文章や物語を生成するために用いられます。さらには、音楽や映像などの芸術的なコンテンツの生成、3Dモデルの生成、医療データの合成など、多岐にわたる領域での応用が研究されています。\nテキストや写真、動画、コード、データ、3D画像などの出力を生成・作成を可能とします。\n2022年は生成系AIの年\n\n14.2.1 生成系AIを支える技術\n自然言語処理や画像認識などの分野で使われる生成系AIにおいて重要な技術を紹介します。\n\n注意機構\nTransformer\n強化学習\n大規模言語モデル\n\n事前学習\nファインチューニング\n\n\n\n14.2.1.1 注意機構\n注意機構（attention）は、入力として与えられたデータの一部に注目し、その注目度合いに基づいて重みを割り当てます。これにより、より重要な情報に高い重みが与えられ、モデルは注目すべき部分により焦点を当てることを可能にします。重要な情報とは、例えば、画像の中で対象物が写っている領域や、文章の中で主語や述語などの単語を指します。注意機構は特に系列データを扱う際に有用です。系列データとは、時系列データや自然言語など、順序を持ったデータのことを指します。画像におけるピクセルや音声における波形なども、時系列データの一種とみなすことができます。\n注意機構は、モデルが入力の一部に重点を置くことで、長い文や複雑なデータにおいてもより効果的な処理が可能となります。このように、注意機構は情報の重要性を明示的に扱い、モデルの性能向上に寄与する重要な手法の一つです。\n\n\n14.2.1.2 Transformer\n注意機構を利用したモデルにTransformerと呼ばれるものがあります。Transformerは、自然言語処理の分野で登場しましたが、それ以上に、生成系AIの分野に大きな影響を与えたモデルです。このモデルは、それまでのLSTMやGRUといったリカレントニューラルネットワークとは異なり、データを系列ではなく集合として扱うことで並列化を可能にしました。\nTransformerモデルの主要な要素は以下の通りです。\n\n自己注意メカニズム(Self-Attention)\n位置エンコーディング(Positional Encoding)\nエンコーダとデコーダのアーキテクチャ\n\nTransformerモデルは非常に強力であり、そのアーキテクチャは多くの最新のNLPモデル（GPT-3、BERT、PaLMなど）の基盤となっています。\n一部の生成系AIは、強化学習を用いて学習を行います。これは、AIが試行錯誤を通じて最適な行動を学び、報酬（行動により得られる収益）を最大化するように設計された学習の枠組みです。生成系AIにおける強化学習の一つの例として、AIが生成したテキストの品質を評価し、そのフィードバックを用いてAIのパフォーマンスを改善するという方法があります。具体的には、AIが生成したテキストに不適切な表現が含まれていないかを人間が評価し、その評価をAIにフィードバックします。AIは、フィードバックを受け取ることで、不適切な表現を含まないより良いテキストを生成するように学習します。\n\n\n\n14.2.2 マルチモーダルAI\nマルチモーダルAIは、複数のモーダル（入力データの種類）を扱うAIのことを指します。例えば、テキストをもとに画像を生成するAIや、画像の情報を文章化するAIなどがあります。\n従来のAIシステムでは、主に単一のモーダルに基づいて処理や判断が行われていました。一方で実社会の多様な問題を解決するには、複数のモーダルを扱う必要があります。"
  },
  {
    "objectID": "week09/09_application.html#現在の社会が抱えるaiに対する課題",
    "href": "week09/09_application.html#現在の社会が抱えるaiに対する課題",
    "title": "14  実社会での応用",
    "section": "14.3 現在の社会が抱えるAIに対する課題",
    "text": "14.3 現在の社会が抱えるAIに対する課題\nAIの注目度が高まる一方で、AIに対する課題も多くあります。それはAIが社会に普及し始めて間もないことが原因の一つです。AIが社会へ及ぼす影響、特に負の影響について十分な議論や法整備が行われていないのも現状です。上手に使いこなせれば便利な一方、使い方を誤れば大きな被害をもたらす可能性があるため、AIの社会への普及には、慎重な議論が必要です。\n\n14.3.1 AIの社会への普及に伴う課題\nAIの社会への普及に伴う課題として、以下のようなものがあります。\n\n仕事の自動化による失業\n個人情報の漏洩\n差別や偏見の増大\n著作権の侵害\nフェイクニュースの拡散"
  },
  {
    "objectID": "week10/index.html#この章の参考資料url",
    "href": "week10/index.html#この章の参考資料url",
    "title": "深層生成モデル",
    "section": "この章の参考資料・URL",
    "text": "この章の参考資料・URL\n\n(手塚太郎 2018)\n(David Foster and 松田晃一 and 小沼千絵 2020)\n(岡谷貴之 2022)\n(柳井啓司, 中鹿亘, と 稲葉通将 2022)\n(岡野原大輔 2022)\n\n\n\n\n\nDavid Foster and 松田晃一 and 小沼千絵. 2020. 生成Deep Learning : 絵を描き、物語や音楽を作り、ゲームをプレイする. オライリー・ジャパン. http://id.ndl.go.jp/bib/030645407.\n\n\n岡谷貴之. 2022. 深層学習 = Deep Learning. 改訂第2版 版. 機械学習プロフェッショナルシリーズ. 講談社. http://id.ndl.go.jp/bib/031901202.\n\n\n岡野原大輔. 2022. ディープラーニングを支える技術. Tech×Books plus 2. 技術評論社. http://id.ndl.go.jp/bib/032086743.\n\n\n手塚太郎. 2018. しくみがわかる深層学習 = An Introduction to Deep Learning. 朝倉書店. http://id.ndl.go.jp/bib/029043769.\n\n\n柳井啓司, 中鹿亘, と 稲葉通将. 2022. 深層学習. IT Text. オーム社. http://id.ndl.go.jp/bib/032479806."
  },
  {
    "objectID": "week10/10_generative_model.html#識別モデルと生成モデル",
    "href": "week10/10_generative_model.html#識別モデルと生成モデル",
    "title": "15  深層生成モデル",
    "section": "15.1 識別モデルと生成モデル",
    "text": "15.1 識別モデルと生成モデル\n機械学習の手法の分類方法として、教師あり学習と教師なし学習を紹介してきました。一方、出力の種類に着目すると、識別モデルと生成モデルに分類することもできます。識別モデルは、ロジスティック回帰やサポートベクターマシンのように機械学習の中でも最もよく使われるモデルの一つです。\n識別モデルの多くは、教師あり学習の一種として扱われます。具体的には、訓練データに正解ラベルが付いている学習を行うことで、入力データから正解ラベルを予測することができます。入力データ \\(x\\) が与えられたときに、そのデータがクラス \\(y\\) に属する確率 \\(p(y|x)\\) を求めるモデルが識別モデルです。\n\\(p(y|x)\\) のような関係を条件付き確率と呼びます。条件付き確率は、ある事象が起こったときに、別の事象が起こる確率を表すものです。ペンギンの種を表現する特徴量が与えられたときに、そのペンギンが特定のペンギンの種である確率を求めることができます。\n一方、生成モデルは、与えられたデータから新しいデータを生成するモデルです。生成モデルは、既存のデータの特徴やパターンを学習し、それを元に新たなデータを生成する能力を持ちます。少し難しい言い方をすれば、生成モデルは入力データを与えたときに、そのデータがどのような分布から生成されたのかを推定する確率モデルです。これらは密度推定問題を解くためのモデルです。\n入力 \\(x\\) が観測される確率 \\(p(x)\\) を求めることを密度推定と呼びます。生成モデルは、密度推定を行うことで、入力データの分布を学習します。\n確率モデルから新たなデータを生成することは、機械学習の分野ではサンプリングと呼ばれます。生成モデルは、サンプリングを行うことで、新たなデータを生成することができます。\n生成モデルを用いると、例えば、入力データが犬の画像だった場合、生成モデルはその犬の画像と似たような犬の画像を生成できます。似たような、というのは、生成モデルが完璧に同じ画像を生成することはほぼないためです。生成モデルは、入力データの分布を学習することで、その分布に基づいて新たなデータを生成することができるのです。\n生成モデルの学習から新たなデータの生成までの過程は次のようになります。まず、訓練データから、そのデータの分布を表す確率分布を学習します。次にランダムノイズを入力として、学習した確率分布に基づく新たなデータを生成します。生成されたデータは、訓練データと同じ確率分布に従っているため、訓練データと同じようなデータが生成されることになります。\n生成モデルはその特性から、画像や文章を使った創作活動に適しています。生成モデルを用いることで、白黒の画像をカラー画像に変換することができます。また、ある作家の文章のスタイルを学習し、そのスタイルに基づいて新たな文章を生成することができます。CNNやRNNを用いた深層生成モデルの進歩は、AIによる芸術作品の創造、自然言語処理などの領域で革新的な成果を生み出しています。"
  },
  {
    "objectID": "week10/10_generative_model.html#敵対的生成ネットワーク",
    "href": "week10/10_generative_model.html#敵対的生成ネットワーク",
    "title": "15  深層生成モデル",
    "section": "15.2 敵対的生成ネットワーク",
    "text": "15.2 敵対的生成ネットワーク\n深層生成モデルの代表的な手法の一つは、敵対的生成ネットワーク（Generative Adversarial Networks: GAN）です。GANは、生成器と分類器の2つのネットワークで構成されています。生成器ではランダムノイズなどの入力から新しいデータを生成します。もう一方のネットワークである識別器は、生成されたデータと本物のデータを識別する役割を持ちます。このとき、扱うデータが画像であればCNN、文章であればRNNというように、データの種類に応じたネットワークを用います。この2つのネットワークを競わせながら学習を進めることで、高品質な新しいデータを生成することが可能となります。\nGANの学習手順は次のようになります。まず、生成器はランダムノイズなどの入力からサンプルを生成します。生成されたサンプルと本物のサンプルを識別器に入力し、それが本物かどうかを判別します。このとき、生成器は識別器を騙すように学習を進めます。一方、識別器は生成器が生成した画像を本物と判別できるように学習を進めます。学習は、識別器が本物と生成器によるサンプルを区別できなくなるまで繰り返されます。\nつまり、生成器と識別器は互いに競争する形で学習を進めます。生成器は本物のデータに近いサンプルを生成し、識別器を欺く能力を高めることを目指します。一方、識別器は生成器が生成した偽物と本物のデータを正しく識別できるように学習を行います。\n\n\n\n\n\n\nヒント\n\n\n\nTensorflow公式によるGANのデモ: https://www.tensorflow.org/tutorials/generative/dcgan?hl=ja"
  },
  {
    "objectID": "week10/10_generative_model.html#表現学習",
    "href": "week10/10_generative_model.html#表現学習",
    "title": "15  深層生成モデル",
    "section": "15.3 表現学習",
    "text": "15.3 表現学習\n表現学習"
  },
  {
    "objectID": "week10/10_generative_model.html#テンソル",
    "href": "week10/10_generative_model.html#テンソル",
    "title": "15  深層生成モデル",
    "section": "15.4 テンソル",
    "text": "15.4 テンソル\nカラー画像は4つの次元を持つテンソルで表現されます。3つの次元は画像の高さ、幅、色のチャンネル数を表します。4つ目の次元は画像の枚数を表します。"
  },
  {
    "objectID": "week10/10_generative_model.html#変分自己符号化器変分オートエンコーダ",
    "href": "week10/10_generative_model.html#変分自己符号化器変分オートエンコーダ",
    "title": "15  深層生成モデル",
    "section": "15.5 変分自己符号化器（変分オートエンコーダ）",
    "text": "15.5 変分自己符号化器（変分オートエンコーダ）"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "参考資料・URL",
    "section": "",
    "text": "AI白書編集委員会. 2022. Ai白書2022. KADOKAWA.\n\n\nAlice Zheng, Amanda Casari, and ホクソエム. 2019.\n機械学習のための特徴量エンジニアリング :\nその原理とPythonによる実践. Translated by ホクソエム.\nオライリー・ジャパン. http://id.ndl.go.jp/bib/029512290.\n\n\nDavid Foster and 松田晃一 and 小沼千絵. 2020. 生成Deep Learning :\n絵を描き、物語や音楽を作り、ゲームをプレイする.\nオライリー・ジャパン. http://id.ndl.go.jp/bib/030645407.\n\n\nGéron, Aurélien, 下田倫大, and 長尾高弘. 2020.\nScikit-Learn、keras、TensorFlowによる実践機械学習. Translated\nby 下田倫大 and 長尾高弘. 第2版 ed. オライリー・ジャパン. http://id.ndl.go.jp/bib/030701507.\n\n\nGrus, Joel, and 菊池彰. 2020. ゼロからはじめるデータサイエンス :\nPythonで学ぶ基本と実践. Translated by 菊池彰. 第2版 ed.\nオライリー・ジャパン. http://id.ndl.go.jp/bib/030372878.\n\n\nM.クーケルバーク, and 直江清隆. 2020. Aiの倫理学. 丸善出版. http://id.ndl.go.jp/bib/031185559.\n\n\nSweigart, Al, and 相川愛三. 2023. 退屈なことはPythonにやらせよう :\nノンプログラマーにもできる自動化処理プログラミング. Translated by\n相川愛三. 第2版 ed. オライリー・ジャパン.\n\n\nTunstall, Lewis, Thomas Wolf Leandro von Werra, and 中山光樹. 2022.\n機械学習エンジニアのためのTransformers :\n最先端の自然言語処理ライブラリによるモデル開発.\nオライリー・ジャパン. http://id.ndl.go.jp/bib/032288109.\n\n\n今泉允聡. 2021. 深層学習の原理に迫る : 数学の挑戦.\n岩波科学ライブラリー ; 303. 岩波書店. http://id.ndl.go.jp/bib/031339770.\n\n\n八谷大岳. 2020. ゼロからつくるPython機械学習プログラミング入門 =\nIntroduction to Machine Learning from Scratch with Python.\n機械学習スタートアップシリーズ. 講談社. http://id.ndl.go.jp/bib/030584765.\n\n\n吉田拓真, and 尾原颯. 2018. 現場で使える!NumPyデータ処理入門 :\n機械学習・データサイエンスで役立つ高速処理手法. 翔泳社. http://id.ndl.go.jp/bib/029316312.\n\n\n小林一郎. 2008. 人工知能の基礎. Computer Science Library ; 13.\nサイエンス社. http://id.ndl.go.jp/bib/000009788211.\n\n\n岡谷貴之. 2022. 深層学習 = Deep Learning. 改訂第2版 ed.\n機械学習プロフェッショナルシリーズ. 講談社. http://id.ndl.go.jp/bib/031901202.\n\n\n岡野原大輔. 2022a. Ai技術の最前線 :\nこれからのaiを読み解く先端技術73. 日経BP. http://id.ndl.go.jp/bib/032288114.\n\n\n———. 2022b. ディープラーニングを支える技術. Tech×books Plus 2.\n技術評論社. http://id.ndl.go.jp/bib/032086743.\n\n\n———. 2022c. ディープラーニングを支える技術 :\n「正解」を導くメカニズム〈技術基礎〉. Tech×books Plus. 技術評論社.\nhttp://id.ndl.go.jp/bib/031881422.\n\n\n岡﨑直観, 荒瀬由紀, 鈴木潤, 鶴岡慶雅, and 宮尾祐介. 2022.\n自然言語処理の基礎. IT Text. オーム社. http://id.ndl.go.jp/bib/032310940.\n\n\n手塚太郎. 2018. しくみがわかる深層学習 = an Introduction to Deep\nLearning. 朝倉書店. http://id.ndl.go.jp/bib/029043769.\n\n\n斎藤康毅. 2016. ゼロから作るDeep Learning :\nPythonで学ぶディープラーニングの理論と実装. オライリー・ジャパン.\nhttp://id.ndl.go.jp/bib/027597005.\n\n\n有賀友紀, and 大橋俊介. 2021.\nRとPythonで学ぶ実践的データサイエンス&機械学習. 増補改訂版.\n技術評論社. http://id.ndl.go.jp/bib/031401828.\n\n\n柳井啓司, 中鹿亘, and 稲葉通将. 2022. 深層学習. IT Text.\nオーム社. http://id.ndl.go.jp/bib/032479806.\n\n\n森下光之助. 2021. 機械学習を解釈する技術 = Techniques for\nInterpreting Machine Learning :\n予測力と説明力を両立する実践テクニック. 技術評論社. http://id.ndl.go.jp/bib/031576666.\n\n\n椎名洋, 姫野哲人, 保科架風, and 清水昌平. 2019.\nデータサイエンスのための数学 = Mathematics for Data Science.\nEdited by 清水昌平. データサイエンス入門シリーズ. 講談社. http://id.ndl.go.jp/bib/029903862.\n\n\n池内孝啓, and 片柳薫子. 2020.\nPythonユーザのためのJupyter〈実践〉入門. 改訂版. 技術評論社. http://id.ndl.go.jp/bib/030570561.\n\n\n瀧雅人. 2017. これならわかる深層学習入門 = Introduction to Deep\nLearning. 機械学習スタートアップシリーズ. 講談社. http://id.ndl.go.jp/bib/028568441.\n\n\n石川聡彦. 2018. 人工知能プログラミングのための数学がわかる本 =\nMATHEMATICS FOR AI PROGRAMMING. KADOKAWA. http://id.ndl.go.jp/bib/028819866.\n\n\n竹内一郎, and 烏山昌幸. 2015. サポートベクトルマシン = Support\nVector Machine. 機械学習プロフェッショナルシリーズ. 講談社. http://id.ndl.go.jp/bib/026619227.\n\n\n谷口忠大. 2020. イラストで学ぶ人工知能概論 = an Illustrated Guide to\nArtificial Intelligence. 改訂第2版 ed. 講談社. http://id.ndl.go.jp/bib/030810109.\n\n\n赤穂昭太郎, 今泉允聡, 内田誠一, 清智也, 高野渉, 辻真吾, 原尚幸, et al.\n2023. 応用基礎としてのデータサイエンス : AI×データ活用の実践.\nEdited by 北川源四郎 and 竹村彰通. データサイエンス入門シリーズ. 講談社.\nhttp://id.ndl.go.jp/bib/032637501.\n\n\n門脇大輔, 阪田隆司, 保坂桂佑, and 平松雄司. 2019.\nKaggleで勝つデータ分析の技術. 技術評論社. http://id.ndl.go.jp/bib/029976550.\n\n\n高柳慎一, 長田怜士, and ホクソエム. 2023. 評価指標入門 =\nIntroduction to Evaluation Metrics :\nデータサイエンスとビジネスをつなぐ架け橋. 技術評論社. http://id.ndl.go.jp/bib/032637452."
  },
  {
    "objectID": "appendix/jupyternotebook.html#ローカル実行",
    "href": "appendix/jupyternotebook.html#ローカル実行",
    "title": "付録 A — Jupyter Notebook",
    "section": "A.1 ローカル実行",
    "text": "A.1 ローカル実行\n自身のコンピュータ上にPython環境を整備して実行する方法です。リモート実行環境とは異なり、自身のコンピュータ上で動作できるため、気軽に実行できる反面、環境構築に知識と手間がかかります。\n\nA.1.1 Windows\n\nanaconda\nVSCode + WSL + Docker\n\n\n\nA.1.2 macOS\n\nVSCode + Docker"
  },
  {
    "objectID": "appendix/jupyternotebook.html#リモート実行環境",
    "href": "appendix/jupyternotebook.html#リモート実行環境",
    "title": "付録 A — Jupyter Notebook",
    "section": "A.2 リモート実行環境",
    "text": "A.2 リモート実行環境\nJupyter Notebookを実行するための環境を提供するサービスを利用する方法です。Jupyter環境を提供するサービスは多数ありますが、ここでは代表的なものを紹介します。\n\nA.2.1 Google Colaboratory (Colab)\nColabはGoogleが提供するJupyter Notebookのリモート実行環境です。Googleアカウントがあれば無料で利用できます。制限付きではありますが、GPUを利用することもできます。また、有料のプランを契約することで、高性能なGPU、TPUを利用することも可能です。\n\n\nA.2.2 SageMaker Studio\nAmazon\n\n\nA.2.3 Kaggle\n\n\nA.2.4 Binder"
  },
  {
    "objectID": "appendix/ai_tools.html#github-copilot",
    "href": "appendix/ai_tools.html#github-copilot",
    "title": "付録 B — AIを活用したツール",
    "section": "B.1 GitHub Copilot",
    "text": "B.1 GitHub Copilot\nGitHub CopilotはGitHubが提供する、AIを活用したプログラミング支援のツールです。利用にはGitHubのアカウントが必要で、試用期間が終わると有料になります。ただし、GitHubへの教育機関の登録を行った場合には無料で利用できます。\nGitHub Copilotはいくつかのテキストエディタ、開発環境のアプリケーション上で動作します。Visual Studio Codeの拡張機能も提供されており、この資料の一部もGitHub Copilotの協力によって文章およびプログラムコードの記述が行われています。"
  },
  {
    "objectID": "appendix/ai_tools.html#chatgpt",
    "href": "appendix/ai_tools.html#chatgpt",
    "title": "付録 B — AIを活用したツール",
    "section": "B.2 ChatGPT",
    "text": "B.2 ChatGPT\n2022年11月にOpenAIによって公開されたChatGPTは、ごくごく自然な「対話」によって文章生成からプログラミング、独学まで、さまざまな用途に応用可能なAIツール（チャットボット）です。\n利用には、メールアドレスによるアカウントの登録が必要です。メールアドレスの代わりに、Google、Microsoft、Appleのアカウントを利用することもできます。ただし、生年月日や電話番号といった個人情報の登録も必要になります。\n大規模言語モデルを用いたチャットボットには、他にGoogleによるBardやMicrosoftのBingなどがあります。"
  },
  {
    "objectID": "appendix/ai_tools.html#aiツール利用時の注意点",
    "href": "appendix/ai_tools.html#aiツール利用時の注意点",
    "title": "付録 B — AIを活用したツール",
    "section": "B.3 AIツール利用時の注意点",
    "text": "B.3 AIツール利用時の注意点\nAIツールを利用する際には、特に以下の点に注意してください。\n\nAIによって生成された文章・プログラムは、必ずしも正しいとは限りません。間違った情報、古い情報、不適切な情報が含まれている可能性があります。そのため、AIの生成物を確認、修正する必要があります。\n個人情報や機密情報はAIツールに送信しないでください。AIツールは、利用者の入力した情報を学習することがあります。そのため、個人情報や機密情報がAIツールを介して漏洩する可能性があります。\n大学での講義、課題、卒業研究等では、AIツールを利用しないことが前提となっている場合があります。また、担当する教員によっては、AIツールの利用を禁止している場合があります。AIツールの利用を不正とみなす教員もいます。そのため、AIツールの利用については、必ず担当する教員に相談してください。\n\n徳島大学では生成AIを活用するための基本方針について公表しています。この方針は生成AIを取り巻く社会状況や学内の意見等を踏まえて見直しが行われる予定です。また、学生向けに注意喚起を行っています。生成AIを利用する前に必ず目を通しておき、実際の利用時には注意点を忘れずに適切な活用を心がけましょう。"
  },
  {
    "objectID": "appendix/vscode.html#インストール",
    "href": "appendix/vscode.html#インストール",
    "title": "付録 C — Visual Studio Code",
    "section": "C.1 インストール",
    "text": "C.1 インストール\nVS Codeのインストールは、公式サイトからダウンロードしてインストールします。インストール方法はOSによって異なります。ここでは、Windowsの場合のインストール方法を説明します。\n\n公式サイトにアクセスして、Download for Windowsをクリックします。\nダウンロードしたインストーラを実行します。\nインストールが完了したら、LaunchをクリックしてVS Codeを起動します。"
  },
  {
    "objectID": "appendix/vscode.html#基本的な使い方",
    "href": "appendix/vscode.html#基本的な使い方",
    "title": "付録 C — Visual Studio Code",
    "section": "C.2 基本的な使い方",
    "text": "C.2 基本的な使い方\nVS Codeの基本的な使い方について説明します。VS Codeは次のような画面になっています。\n\n\n\nVS Codeの画面\n\n\nVS Codeの画面は、以下のように分割されています。\n\nタイトルバー\nメニューバー\nアクティビティバー\nサイドバー\nエディター\nステータスバー\n\nこれらの部分について説明します。いずれの部分も、表示・非表示を切り替えることができます。表示・非表示を切り替えるには、表示メニューから、表示・非表示を切り替えたい部分を選択します。また、レイアウトも自分の好みに合わせて変更することができます。\n\nC.2.1 タイトルバー\nタイトルバーには、VS Codeのバージョンや拡張機能の更新情報などが表示されます。また、タイトルバーの右側には、最大化、最小化、閉じるボタンがあります。\n\n\nC.2.2 メニューバー\nメニューバーには、ファイル、編集、表示、移動、実行、ターミナル、ヘルプなどのメニューがあります。メニューをクリックすると、そのメニューに属するコマンドが表示されます。例えば新規ファイルを作成したい場合は、ファイルメニューの新規ファイルをクリックします。\nVS Codeでは、メニューの代わりにコマンドパレットを利用することもできます。コマンドパレットは、表示メニューのコマンドパレットをクリックすることで表示されます。コマンドパレットには、ファイルの作成や拡張機能のインストール、設定の変更などの豊富なコマンドがあります。コマンドパレットを利用すると、メニューを開くことなくコマンドを実行することが可能となります。\nVS Code上でターミナルを利用することも可能です。これにより、VS Code上でコマンドを実行したり、Pythonの対話型インタプリタを起動したりすることができます。ターミナルを利用するには、表示メニューのターミナルをクリックします。新しいターミナルはエディター部分の下に表示されます。\n\n\nC.2.3 アクティビティバー\nアクティビティバーには、エクスプローラー、検索、Git、拡張機能などのアイコンがあります。アクティビティバーのアイコンをクリックすると、その機能がサイドバーに表示されます。\n\n\nC.2.4 サイドバー\nサイドバーはアクティビティバーで選択した機能に応じて表示される領域です。エクスプローラーを選択した場合は、ファイルやフォルダの構造を表示します。サイドバーにはプライマリサイドバーとセカンダリサイドバーの2種類があり、デフォルトではセカンダリサイドバーは非表示です。なおサイドバーの幅は、エディタ部分との境界をマウスでドラッグすることで調整可能です。\n\n\nC.2.5 エディター\nエディターは、ファイルの内容を表示する領域です。複数のファイルをタブによって個別に表示できます。また、エディターの分割も可能です。エディターの分割は、表示メニューのエディターの分割から行うことができます。\n\n\nC.2.6 ステータスバー\n画面下部のステータスバーには各種の情報が表示されます。編集中のファイルの文字コードや改行コード、行数、列数の他、拡張機能による追加情報が表示されます。"
  },
  {
    "objectID": "appendix/vscode.html#拡張機能",
    "href": "appendix/vscode.html#拡張機能",
    "title": "付録 C — Visual Studio Code",
    "section": "C.3 拡張機能",
    "text": "C.3 拡張機能\nVS Codeはユーザーが必要に応じて拡張機能を追加することで、その使い勝手や機能を拡張できるようになっています。拡張機能は、アクティビティーバーの拡張機能アイコンをクリックすると表示される画面から、任意のものを選んでインストールできます。拡張機能パネルは、表示メニューの拡張機能を選択することでも表示することができます。\n拡張機能の種類は非常に多く、プログラミング言語のサポートやプロジェクト管理のためのものや、VS Codeのテーマを変更するものなどがあります。拡張機能は、インストールタブからインストールすることができます。インストールした拡張機能は、インストール済みタブから確認することができます。また、有効タブから拡張機能を有効化・無効化することができます。\n\n\n\n\n\n\nおすすめの拡張機能\n\n\n\nデータサイエンス、機械学習、プログラミング全般に役立つ、おすすめの拡張機能を紹介します。\n\nJapanese Language Pack for Visual Studio Code\nJupyter\nPython\nR\nQuarto\nDocker\nGitHub Copilot"
  },
  {
    "objectID": "appendix/vscode.html#ワークスペース",
    "href": "appendix/vscode.html#ワークスペース",
    "title": "付録 C — Visual Studio Code",
    "section": "C.4 ワークスペース",
    "text": "C.4 ワークスペース\nVS Codeでは、ワークスペースという概念を利用します。ワークスペースとは、フォルダやファイルの集合を指します。ワークスペースを作成することで、複数のフォルダやファイルを一つのプロジェクトとして管理することができます。ワークスペースは、ファイルメニューのワークスペースを開くから開くことができます。また、ファイルメニューのワークスペースを保存からワークスペースを保存することができます。"
  },
  {
    "objectID": "appendix/vscode.html#設定",
    "href": "appendix/vscode.html#設定",
    "title": "付録 C — Visual Studio Code",
    "section": "C.5 設定",
    "text": "C.5 設定\nユーザー設定とワークスペース設定の2種類"
  },
  {
    "objectID": "appendix/vscode.html#キーボードショートカット",
    "href": "appendix/vscode.html#キーボードショートカット",
    "title": "付録 C — Visual Studio Code",
    "section": "C.6 キーボードショートカット",
    "text": "C.6 キーボードショートカット\n豊富なキーボードショートカット。jsonファイルでカスタマイズ可能。"
  },
  {
    "objectID": "appendix/vscode.html#vs-codeを利用したjupyter実行環境の構築",
    "href": "appendix/vscode.html#vs-codeを利用したjupyter実行環境の構築",
    "title": "付録 C — Visual Studio Code",
    "section": "C.7 VS Codeを利用したJupyter実行環境の構築",
    "text": "C.7 VS Codeを利用したJupyter実行環境の構築\nWindows向け。\n\n理系大学生は研究でもJupyterNotebook(Anaconda)ではなくVScodeを使おう+Python環境構築\n理系大学生のためのPython環境のススメ"
  },
  {
    "objectID": "appendix/docker.html#dockerとは",
    "href": "appendix/docker.html#dockerとは",
    "title": "付録 D — Docker",
    "section": "D.1 Dockerとは",
    "text": "D.1 Dockerとは\n\nD.1.1 コンテナ型仮想化"
  },
  {
    "objectID": "appendix/docker.html#dockerのインストール",
    "href": "appendix/docker.html#dockerのインストール",
    "title": "付録 D — Docker",
    "section": "D.2 Dockerのインストール",
    "text": "D.2 Dockerのインストール\n\nD.2.1 Windows\n\n\nD.2.2 macOS"
  }
]