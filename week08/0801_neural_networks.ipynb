{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: ニューラルネットワーク\n",
        "execute:\n",
        "  keep-ipynb: true\n",
        "---"
      ],
      "id": "8bd8909b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 深層学習\n",
        "\n",
        "深層学習は人工ニューロンと呼ばれる、人の脳で行われている情報処理を模倣した情報処理モデルを多層に重ねたニューラルネットワークを用いて、データから重要な特徴量を抽出し、その特徴量を用いて分類や回帰を行う機械学習の手法です。\n",
        "\n",
        "深層学習は、画像認識や音声認識、自然言語処理などの分野で高い精度を出しており、近年では様々な分野で活用されています。人工知能の中でも、特に注目されている技術です。\n",
        "\n",
        "### 機械学習との違い\n",
        "\n",
        "深層学習は機械学習の一種ですが、機械学習とはどのような違いがあるのでしょうか。機械学習と深層学習の特徴・その違いを整理すると、以下のようになります。\n",
        "\n",
        "#### 機械学習\n",
        "\n",
        "- データからパターンを見つけ出し、そのパターンを基に新しいデータに対する予測を行う。多くの場合、人間が手動で重要な情報（特徴量）を設定する必要がある。\n",
        "- モデルの訓練時間は一般的に深層学習に比べて短い\n",
        "\n",
        "#### 深層学習\n",
        "\n",
        "- 機械学習の一手法。\n",
        "- 特徴量の抽出を人間が行うのではなく機械が行う。そのため手動での特徴量設定が不要となる。\n",
        "- 多層の人工ニューロンを使用する。各層のニューロンは、前の層のニューロンからの入力を受け取り、その入力に対して重み付けを行う。各ニューロンの出力は、そのニューロンへの入力と重み付けの積の総和を「活性化関数」に入力した値となる。→ **柔軟な表現を可能にする**\n",
        "- 機械学習と比べ、モデルの訓練には多くのデータと長い時間が必要となる。\n",
        "\n",
        "## 神経細胞のネットワーク\n",
        "\n",
        "深層学習で利用される人工ニューロンは、人間の脳の神経細胞を模倣したものです。人間の脳は、神経細胞と呼ばれる細胞が結びつき、神経細胞同士で電気信号を伝えるネットワークを形成しており、このネットワークを**ニューラルネットワーク**と呼びます。\n",
        "\n",
        "神経細胞のネットワークは、以下のような構造をしています。\n",
        "\n",
        "- 神経細胞体\n",
        "- 樹状突起\n",
        "- 軸索\n",
        "\n",
        "軸索末端と次のニューロンの樹状突起とが接触し、情報の伝達が行われます。この接触部位はシナプスと呼ばれます。また、電気信号の伝達を「発火」と呼びます。\n",
        "\n",
        "発火は、樹状突起からの入力が一定の値を超えたときに発生します。この一定の値を<ruby>閾値<rt>いきち、しきいち</rt></ruby>と呼びます。閾値を超えたときに発生する電気信号の強さは一定であり、発火の有無のみが伝達されます。\n",
        "\n",
        "## 人工ニューロン\n",
        "\n",
        "人工ニューロンは、ニューラルネットワークを構成する基本単位としての、モデル化された神経細胞です。1つ以上の入力を受け取り、それらの入力に対して重み付けを行い、その重み付けの総和を活性化関数に入力した値を出力する、という構造をしています。\n",
        "\n",
        "- 入力\n",
        "- 重み\n",
        "- 活性化関数\n",
        "- 出力\n",
        "\n",
        "### パーセプトロン\n",
        "\n",
        "人工ニューロンの一種に「パーセプトロン」と呼ばれるものがあります。パーセプトロンの挙動を理解することで、人工ニューロンの基本的な動作の理解に役立ちます。\n",
        "\n",
        "パーセプトロンは、構造や学習の違いによって、単純パーセプトロンと多層パーセプトロンに分けられます。まずは単純パーセプトロンについて説明します。\n",
        "\n",
        "#### 単純パーセプトロン\n",
        "\n",
        "単純パーセプトロンは、入力層と出力層の2つの層から構成されています。入力層は、入力を受け取る層です。出力層では、入力層からの入力と重みの積の総和が閾値を超えた場合に1を出力し、超えなかった場合に0を出力する、という動作をします。このような動作を行う関数を「ステップ関数」と呼びます。パーセプトロンでは、ステップ関数のように、入力に対して出力を決定する関数を活性化関数と呼びます。\n",
        "\n",
        "ステップ関数を活性化関数として用いた単純パーセプトロンは以下の式で表現されます。\n",
        "$$\n",
        "y = \\begin{cases}\n",
        "0 & (w_1x_1 + w_2x_2 \\leq \\theta) \\\\\n",
        "1 & (w_1x_1 + w_2x_2 > \\theta)\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "ここで $x1$ 、$x2$ は入力、 $w1$ 、$w2$ は各入力に対する重み（weight）、 $\\theta$ は閾値です。ここでは入力の数が2つの場合を考えていますが、入力の数は任意の値にすることができます。また、入力の重要度を重みによって調整することができます。そのため重みは入力ごとに用意されています。 $\\theta$ は、重みと入力の積の総和が閾値を超えた場合に1を出力するかどうかを決定するための値です。すなわち、 $\\theta$ が大きいほど、重みと入力の積の総和が大きくないと1を出力しなくなります。\n",
        "\n",
        "1か0かという二値の出力を用いて、以下の論理演算を行うことができます。このような論理演算を行うパーセプトロンを「論理回路」と呼びます。\n",
        "\n",
        "- AND: 2つの入力が1の場合に1を出力し、それ以外の場合に0を出力する。\n",
        "- OR: 2つの入力のうち1つ以上が1の場合に1を出力し、それ以外の場合に0を出力する。\n",
        "- NAND: 2つの入力が1の場合に0を出力し、それ以外の場合に1を出力する。ANDの逆の動作をする。\n",
        "\n",
        "| $x_1$ | $x_2$ | AND | OR | NAND |\n",
        "| --- | --- | --- | --- | --- |\n",
        "| 0 | 0 | 0 | 0 | 1 |\n",
        "| 1 | 0 | 0 | 1 | 1 |\n",
        "| 0 | 1 | 0 | 1 | 1 |\n",
        "| 1 | 1 | 1 | 1 | 0 |\n",
        "\n",
        "これらの論理演算は、パーセプトロンの重みと閾値を適切に設定することで実現できます。例えば、ANDの場合は、 $w_1$ 、 $w_2$ 、 $\\theta$ をそれぞれ0.5、0.5、0.7とすると、上の表のような出力が得られます。\n",
        "\n",
        "$$\n",
        "y = \\begin{cases}\n",
        "0 & (0.5x_1 + 0.5x_2 \\leq 0.7) \\\\\n",
        "1 & (0.5x_1 + 0.5x_2 > 0.7)\n",
        "\\end{cases}\n",
        "$$\n"
      ],
      "id": "eb4b1891"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x1 = 0\n",
        "x2 = 0\n",
        "(0.5 * x1 + 0.5 * x2 <= 0.7) & (0.5 * x1 + 0.5 * x2 > 0.7)"
      ],
      "id": "21042097",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pythonで上記のパーセプトロンを実装すると以下のようになります。\n"
      ],
      "id": "5e2b089b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def perceptron(x1, x2, theta = 0.7):\n",
        "    w1, w2 = 0.5, 0.5\n",
        "    tmp = w1 * x1 + w2 * x2\n",
        "    if tmp <= theta:\n",
        "        return 0\n",
        "    elif tmp > theta:\n",
        "        return 1\n",
        "\n",
        "print(perceptron(0, 0)) # 0\n",
        "print(perceptron(1, 0)) # 0\n",
        "print(perceptron(0, 1)) # 0\n",
        "print(perceptron(1, 1)) # 1"
      ],
      "id": "ff95b403",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pythonで実装したパーセプトロンは、「入力と重みの積の総和が閾値を超えた場合に1を出力し、超えなかった場合に0を出力する」、という動作をしています。閾値は関数の引数として与えていましたが、この値を変えることで、パーセプトロンの出力を変えることができます。これにより、パーセプトロンをORパーセプトロンとして機能させることができます。\n"
      ],
      "id": "45a0f4cb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ORのパーセプトロン\n",
        "print(perceptron(0, 0, theta = 0.2)) # 0\n",
        "print(perceptron(1, 0, theta = 0.2)) # 1\n",
        "print(perceptron(0, 1, theta = 0.2)) # 1\n",
        "print(perceptron(1, 1, theta = 0.2)) # 1"
      ],
      "id": "52ad4819",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NANDのパーセプトロンは、ANDのパーセプトロンの重みと閾値を反転させることで実現できます。\n"
      ],
      "id": "a49daa9a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# NANDのパーセプトロン\n",
        "def NAND_perceptron(x1, x2, theta = -0.7):\n",
        "    # 重みと閾値を反転\n",
        "    w1, w2 = -0.5, -0.5\n",
        "    tmp = w1 * x1 + w2 * x2\n",
        "    if tmp <= theta:\n",
        "        return 0\n",
        "    elif tmp > theta:\n",
        "        return 1\n",
        "\n",
        "print(NAND_perceptron(0, 0)) # 1\n",
        "print(NAND_perceptron(1, 0)) # 1\n",
        "print(NAND_perceptron(0, 1)) # 1\n",
        "print(NAND_perceptron(1, 1)) # 0"
      ],
      "id": "cd7d52e9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "これまでのパーセプトロンの例では、閾値を関数の引数として与えていましたが、閾値をバイアスとして扱い、重みと入力の積の総和に含めることができます。\n",
        "\n",
        "$$\n",
        "y = \\begin{cases}\n",
        "0 & (b + w_1x_1 + w_2x_2 \\leq 0) \\\\\n",
        "1 & (b + w_1x_1 + w_2x_2 > 0)\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "ここで $b$ はバイアスです。この場合、閾値はバイアスの重みとなります。バイアスは、入力が0の場合に出力する値を決めるパラメータとして機能します。すなわちバイアスが大きいほど、入力が0の場合に出力する値は大きくなります。\n"
      ],
      "id": "e2a1212f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# バイアスを導入したANDのパーセプトロン\n",
        "def AND_perceptron(x1, x2, b = -0.7):\n",
        "    w1, w2 = 0.5, 0.5\n",
        "    # b はバイアス\n",
        "    tmp = b + w1 * x1 + w2 * x2\n",
        "    if tmp <= 0:\n",
        "        return 0\n",
        "    elif tmp > 0:\n",
        "        return 1\n",
        "\n",
        "print(AND_perceptron(0, 0)) # 0\n",
        "print(AND_perceptron(1, 0)) # 0\n",
        "print(AND_perceptron(0, 1)) # 0\n",
        "print(AND_perceptron(1, 1)) # 1\n",
        "\n",
        "\n",
        "print(\"バイアスの値を小さくする\")\n",
        "# バイアスの値が小さいと発火しやすくなる\n",
        "print(AND_perceptron(0, 0, b = -0.2)) # 0\n",
        "print(AND_perceptron(1, 0, b = -0.2)) # 1\n",
        "print(AND_perceptron(0, 1, b = -0.2)) # 1\n",
        "print(AND_perceptron(1, 1, b = -0.2)) # 1\n",
        "\n",
        "# バイアスの値が大きいと発火しにくくなる\n",
        "print(\"バイアスの値を大きくする\")\n",
        "print(AND_perceptron(0, 0, b = -20.0))\n",
        "print(AND_perceptron(1, 1, b = -20.0))"
      ],
      "id": "1a52a675",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "同様に、ORやNANDのパーセプトロンにもバイアスを導入することができます。\n",
        "\n",
        "パーセプトロンを用いることで、ANDやOR、NANDの論理回路を表現することができました。一方で、2つの入力が異なる場合に1を出力し、同じ場合に0を出力する論理回路であるXORは単純パーセプトロンでは表現できません。\n",
        "\n",
        "| $x_1$ | $x_2$ | $y$ |\n",
        "| --- | --- | --- |\n",
        "| 0 | 0 | 0 |\n",
        "| 1 | 0 | 1 |\n",
        "| 0 | 1 | 1 |\n",
        "| 1 | 1 | 0 |\n",
        "\n",
        "単純パーセプトロンがXORを表現できない理由は、単純パーセプトロンは線形分離可能な問題しか表現できないからです。線形分離可能な問題とは、2つのクラスを直線で分離できる問題のことです。XORは、2つのクラスを直線で分離するものではないため、単純パーセプトロンでは表現できない、ということになります。\n"
      ],
      "id": "49fbb349"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# XORの入出力と線形分離不可能なことを確認する\n",
        "# XORの入力\n",
        "X = np.array([[0, 0], [1, 0], [0, 1], [1, 1]])\n",
        "# XORの出力\n",
        "Y = np.array([0, 1, 1, 0])\n",
        "\n",
        "# データをプロットする\n",
        "plt.scatter(X[:, 0], X[:, 1], c = Y)\n",
        "# 直線の追加... これでは分離できない\n",
        "plt.plot([0, 1], [1, 0])\n",
        "plt.show()"
      ],
      "id": "8d041a77",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "一方で、多層パーセプトロンを用いることで、XORを表現することができます。\n",
        "\n",
        "<!-- 単純パーセプトロンの学習は、入力と正解ラベルの組み合わせを与えることで行います。学習の結果、パーセプトロンの重みが調整され、正解ラベルを出力するようになります。 -->\n",
        "\n",
        "#### 多層パーセプトロン\n",
        "\n",
        "多層パーセプトロンは、複数のパーセプトロンを組み合わせることで表現することができます。これにより、単純パーセプトロンでは表現できない非線形な問題を表現することができます。\n",
        "\n",
        "バイアスを導入したANDパーセプトロンとORパーセプトロンを組み合わせることで、XORを表現することができます。これらの単純パーセプトロンを組み合わせて、XORを表現する多層パーセプトロンを作成してみましょう。\n"
      ],
      "id": "ac5e99f8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ANDパーセプトロン\n",
        "def AND_perceptron(x1, x2, b = -0.7):\n",
        "    w1, w2 = 0.5, 0.5\n",
        "    # b はバイアス\n",
        "    tmp = b + w1 * x1 + w2 * x2\n",
        "    if tmp <= 0:\n",
        "        return 0\n",
        "    elif tmp > 0:\n",
        "        return 1\n",
        "\n",
        "\n",
        "# ORパーセプトロン\n",
        "def OR_perceptron(x1, x2, b = -0.2):\n",
        "    w1, w2 = 0.5, 0.5\n",
        "    # b はバイアス\n",
        "    tmp = b + w1 * x1 + w2 * x2\n",
        "    if tmp <= 0:\n",
        "        return 0\n",
        "    elif tmp > 0:\n",
        "        return 1\n",
        "\n",
        "\n",
        "# NANDパーセプトロン\n",
        "def NAND_perceptron(x1, x2, b = 0.7):\n",
        "    w1, w2 = -0.5, -0.5\n",
        "    # b はバイアス\n",
        "    tmp = b + w1 * x1 + w2 * x2\n",
        "    if tmp <= 0:\n",
        "        return 0\n",
        "    elif tmp > 0:\n",
        "        return 1\n",
        "\n",
        "\n",
        "# XORパーセプトロン\n",
        "# XORはAND, OR, NANDを組み合わせて表現できる\n",
        "def XOR_perceptron(x1, x2):\n",
        "    # 第0層(入力層)... 2つの入力を受け取る最初の層\n",
        "    s1 = NAND_perceptron(x1, x2)\n",
        "    # 第1層(中間層)... NANDパーセプトロンの出力を入力として受け取る\n",
        "    s2 = OR_perceptron(x1, x2)\n",
        "    # 第2層(出力層)... ORパーセプトロンとNANDパーセプトロンの出力を入力として受け取り、ANDパーセプトロンの出力を返す\n",
        "    y = AND_perceptron(s1, s2)\n",
        "    return y\n",
        "\n",
        "# XORパーセプトロンの出力\n",
        "for x1, x2 in X:\n",
        "    print(x1, x2, XOR_perceptron(x1, x2))"
      ],
      "id": "ba2fe7d5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "上記の多重パーセプトロンは3種の単純パーセプトロンを組み合わせて表現しています。このように、多層パーセプトロンは複数のパーセプトロンを組み合わせて表現することができます。これにより、多層パーセプトロンは非線形な問題を表現することができます。言い換えると、層を重ねることで、柔軟な表現が可能になったということです。\n",
        "\n",
        "## ニューラルネットワーク\n",
        "\n",
        "多層パーセプトロンをベースに、ニューラルネットワークの概念を紹介します。ニューラルネットワークは多層パーセプトロンと同じく、複数のパーセプトロンを組み合わせて表現することができます。しかし、ニューラルネットワークでは、**パーセプトロンの重みとバイアスを自動で調整する**ことができます。これらのパラメータを調整することを「学習」と呼んでいます。\n",
        "\n",
        "### ニューラルネットワークの構造\n",
        "\n",
        "ニューラルネットワークは、入力層、中間層、出力層の3つの層で構成されます。入力層は、外部からの入力を受け取る層です。中間層は、入力層からの入力を受け取り、出力層への入力を行う層です。出力層は、中間層からの入力を受け取り、外部への出力を行う層です。これは、多層パーセプトロンと同じ構造です。多層パーセプトロンと異なる点は、中間層が複数存在することです。このように、中間層が複数存在するニューラルネットワークは「層が深い」と表現され、**ディープニューラルネットワーク**と呼ばれる所以になっています。\n",
        "\n",
        "しかし、層の数を増やすだけでは、ニューラルネットワークの表現力は向上しません。ニューラルネットワークの表現力を向上させるには、活性化関数を工夫する必要があります。活性化関数には、ステップ関数以外にも、シグモイド関数やReLU関数などがあります。これらについて詳しく見ていきましょう。\n",
        "\n",
        "### 活性化関数\n",
        "\n",
        "「活性化」とは入力信号の総和がどのように発火するか（出力が1か0か）を決定することを指します。活性化関数はその名の通り、入力信号の総和がどのように活性化するかを決定する関数です。活性化関数には、ステップ関数、シグモイド関数、ReLU関数などがあります。\n",
        "\n",
        "- ステップ関数: 入力信号の総和が閾値を超えたら発火する関数\n",
        "- シグモイド関数: 入力信号の総和を0から1の間（連続的な実数）に収める関数\n",
        "- 正規化線形関数（ReLU関数）: 入力信号の総和が0を超えたら発火する関数。出力は0から無限大の間の値を取る。\n",
        "- ソフトマックス関数: 入力信号の各要素を0から1の間の連続的な実数に変換し、その出力の総和が1になるように正規化する。これにより、出力は確率分布として解釈することが可能になる。\n",
        "\n",
        "入力と出力の関係をグラフで表すと、以下のようになります。\n"
      ],
      "id": "5bddaca2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ステップ関数\n",
        "def step_function(x):\n",
        "    return np.array(x > 0, dtype=np.int)\n",
        "\n",
        "# シグモイド関数\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# ReLU関数\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "# 入力値\n",
        "x = np.arange(-5.0, 5.0, 0.1)\n",
        "\n",
        "# ステップ関数の出力\n",
        "y_step = step_function(x)\n",
        "\n",
        "# シグモイド関数の出力\n",
        "y_sigmoid = sigmoid(x)\n",
        "\n",
        "# ReLU関数の出力\n",
        "y_relu = relu(x)\n",
        "\n",
        "# グラフの描画\n",
        "plt.plot(x, y_step, label='step')\n",
        "plt.plot(x, y_sigmoid, label='sigmoid')\n",
        "plt.plot(x, y_relu, label='ReLU')\n",
        "plt.ylim(-0.1, 1.1)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "05d6285a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "活性化関数の主な目的は、モデルに非線形性を導入し、より複雑な関数を近似する能力を提供することです。ニューラルネットワークでは、活性化関数として上記のシグモイド関数やReLU関数を用いることが多いです。ステップ関数は、ニューラルネットワークではあまり用いられません。\n",
        "\n",
        "ニューラルネットワークにおいて、各中間層のニューロンは、その入力に対して重みを適用し、バイアスを追加した後、活性化関数を適用します。活性化関数は出力層でも使用されることがありますが、その選択は解くべき問題に依存します。たとえば、二項分類問題ではシグモイド関数が、多クラス分類問題ではソフトマックス関数が出力層の活性化関数としてよく使用されます。\n",
        "\n",
        "### ニューラルネットワークの実装\n",
        "\n",
        "ニューラルネットワークの実装は、多層パーセプトロンの実装とほぼ同じです。違いは、活性化関数をシグモイド関数やReLU関数に変更することです。また、中間層が複数存在することも違いの一つです。\n"
      ],
      "id": "e00cf9c4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# シグモイド関数\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# 恒等関数... 入力をそのまま出力する関数\n",
        "def identity_function(x):\n",
        "    return x\n",
        "\n",
        "# 3層ニューラルネットワーク\n",
        "# 重みとバイアスを初期化する\n",
        "def init_network():\n",
        "    network = {}\n",
        "    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
        "    network['b1'] = np.array([0.1, 0.2, 0.3])\n",
        "    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
        "    network['b2'] = np.array([0.1, 0.2])\n",
        "    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])\n",
        "    network['b3'] = np.array([0.1, 0.2])\n",
        "\n",
        "    return network\n",
        "\n",
        "#　入力信号を出力へ変換する処理\n",
        "def forward(network, x):\n",
        "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
        "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
        "\n",
        "    a1 = np.dot(x, W1) + b1\n",
        "    z1 = sigmoid(a1)\n",
        "    a2 = np.dot(z1, W2) + b2\n",
        "    z2 = sigmoid(a2)\n",
        "    a3 = np.dot(z2, W3) + b3\n",
        "    # 出力層の活性化関数は、恒等関数を用いる\n",
        "    y = identity_function(a3)\n",
        "\n",
        "    return y\n",
        "\n",
        "network = init_network()\n",
        "x = np.array([1.0, 0.5])\n",
        "y = forward(network, x)\n",
        "print(y) # [0.31682708 0.69627909]"
      ],
      "id": "a6288ec7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "扱う問題が回帰か分類かによって、出力層の活性化関数は変更します。回帰問題では、恒等関数を、分類問題では、ソフトマックス関数を用います。ソフトマックス関数は、出力層のニューロンの数だけの出力を持ち、その出力の総和が1になるように正規化します。そのため、ソフトマックス関数の出力を確率として解釈することができます。\n",
        "\n",
        "## 損失関数\n",
        "\n",
        "ニューラルネットワークの学習において、最適な重み、バイアスのパラメータを探索するために、損失関数を用います。損失関数は、ニューラルネットワークの出力と正解ラベルの誤差を表します。損失関数には、二乗和誤差や交差エントロピー誤差などがあります。\n",
        "\n",
        "損失関数は言い換えるとモデルの予測値と正解値がどの程度近いかを示す指標です。この損失関数の値が小さいほど、モデルの予測値と正解値が近いことを意味します。そのため、損失関数の値が小さくなるように、重みやバイアスのパラメータを調整することがニューラルネットワークの学習の目的となります。\n",
        "\n",
        "それでは、どのようにしてパラメータを調整すればよいのでしょうか。パラメータを調整するためには、損失関数の勾配を求める必要があります。そのために勾配降下法という手法を用います。\n",
        "\n",
        "\n",
        "## 勾配降下法によるパラメータの学習\n",
        "\n",
        "勾配とは、ある地点における関数の傾きのことです。勾配が正の場合は、関数の値が増加していることを意味し、勾配が負の場合は、関数の値が減少していることを意味します。勾配が0の場合は、関数の値が極値に達していることを意味します。つまり、勾配が0となるように、勾配を小さくする方向にパラメータを調整することで、損失関数の値を小さくすることができます。これを繰り返すことで、損失関数の値を小さくすることができます。このような手法を勾配降下法と呼びます。\n",
        "\n",
        "損失関数の勾配を求めることで、損失関数の値が増加する方向を知ることができます。そのため、損失関数の値が減少する方向にパラメータを調整することができます。このように、損失関数の勾配を求めることで、パラメータを調整することができます。この勾配を求める方法を誤差逆伝播法と呼びます。\n",
        "\n",
        "### 確率的勾配降下法\n",
        "\n",
        "勾配降下法は、全ての訓練データを用いて、勾配を求めます。しかし、訓練データが膨大な場合は、全ての訓練データを用いて勾配を求めることは現実的ではありません。そのため、訓練データの中からランダムに選んだ一部のデータ（ミニバッチと呼ばれる）を用いて、勾配を求める方法があります。この方法を確率的勾配降下法と呼びます。\n",
        "\n",
        "### 誤差逆伝播法\n",
        "\n",
        "ニューラルネットワークでの重みパラメータに対する損失関数の勾配を効率的に求める方法を誤差逆伝播法と呼びます。誤差逆伝播法は、出力層から入力層の方向に、各層の重みパラメータに対する損失関数の勾配を求めます。これにより効率的に勾配を求めることができます。\n",
        "\n",
        "<!-- ### 勾配消失問題 -->\n",
        "\n",
        "## 畳み込みニューラルネットワーク\n",
        "\n",
        "畳み込みニューラルネットワーク（Convolutional Neural Network: CNN）は、画像認識や音声認識などの分野で高い精度を出すニューラルネットワークの一種です。\n",
        "畳み込み層とプーリング層から構成されます。畳み込み層は、画像の特徴を抽出する層です。プーリング層は、畳み込み層の出力を圧縮する層です。\n",
        "\n",
        "### 畳み込み演算\n",
        "\n",
        "## 再帰型ニューラルネットワーク\n",
        "\n",
        "再帰型ニューラルネットワーク（recurrent neural network: RNN）は、系列的なデータを扱うことができるニューラルネットワークの一種です。例えば、文章や音声などのデータは、系列的なデータとして扱うことができます。そのため、RNNは機械翻訳や音声認識などの分野で高い精度を出すことができます。\n",
        "\n",
        "RNNは、長・短期記憶、ゲート付き再帰ユニットといった様々な構造を持つことができます。これらの構造を持つことで、RNNは時系列データの中に、重要な情報がどこにあるかを自動的に学習することができます。\n",
        "\n",
        "### 長・短期記憶　(LSTM)\n",
        "\n",
        "### ゲート付き再帰ユニット　(GRU)"
      ],
      "id": "c29c5dcf"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "jupytext": {
      "formats": "ipynb,qmd",
      "text_representation": {
        "extension": ".qmd",
        "format_name": "quarto",
        "format_version": "1.0",
        "jupytext_version": "1.14.5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}